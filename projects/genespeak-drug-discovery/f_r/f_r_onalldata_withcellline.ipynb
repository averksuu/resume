{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a12c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# f_r FULL TRAINING SCRIPT (fixed)\n",
    "# - âœ… filter token_id < 3 everywhere (so no \"genes\" 1,2)\n",
    "# - âœ… NO [TARGET] token in input\n",
    "# - âœ… input prefix: [CLS][DRUG][CELL] + gene tokens\n",
    "# - âœ… load pretrained gene embeddings (gene_embeddings.npy)\n",
    "# - âœ… load pretrained cell-line embeddings (cell_embeddings.npy) + mapping (cell2id.csv)\n",
    "# =========================================================\n",
    "\n",
    "import os, glob, math, random\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from itertools import islice\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.amp import autocast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "import scanpy as sc\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 0) PATHS / CONFIG\n",
    "# =========================================================\n",
    "GENE_META_PATH = \"/data/aiffel/data/Tahoe-100M/metadata/gene_metadata.parquet\"\n",
    "DRUG_META_PATH = \"/data/aiffel/data/Tahoe-100M/metadata/drug_metadata.parquet\"\n",
    "COUNTS_CSV     = \"/data/aiffel/babayakga/making_data/aiffel/babayakga/making_data/tahoe_counts_per_drug_cell_line.csv\"\n",
    "\n",
    "PARQUET_DIR    = \"/data/aiffel/data/Tahoe-100M/data\"\n",
    "DMSO_H5AD      = \"/data/aiffel/babayakga/outputs/dmso.h5ad\"\n",
    "\n",
    "# pretrained embeddings\n",
    "PRETRAINED_GENE_NPY = \"/data/aiffel/babayakga/pretraining/checkpoints_with_cell/gene_embeddings.npy\"\n",
    "\n",
    "CELL_CKPT_DIR  = \"/data/aiffel/babayakga/pretraining/checkpoints_with_cell\"\n",
    "CELL2ID_CSV    = os.path.join(CELL_CKPT_DIR, \"cell2id.csv\")\n",
    "CELL_EMB_NPY   = os.path.join(CELL_CKPT_DIR, \"cell_embeddings.npy\")\n",
    "\n",
    "# smiles embedding for drugs (must match drug_metadata row order)\n",
    "SMILES_EMB_PATH = \"/data/aiffel/babayakga/f_p module/f_r/drug_smiles_emb_all.pt\"\n",
    "\n",
    "# training\n",
    "CONTROL_DRUG = \"DMSO_TF\"\n",
    "SEED = 42\n",
    "\n",
    "MIN_GENE_TOKEN_ID = 3   # âœ… IMPORTANT: exclude 0/1/2 (not real genes)\n",
    "\n",
    "TOP_K      = 1000\n",
    "MAX_LEN    = 512        # gene tokens length (not counting prefix)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "TOTAL_EPOCHS     = 8\n",
    "WARMUP_EPOCHS    = 2\n",
    "lambda_rank_main = 0.2\n",
    "\n",
    "STEPS_PER_EPOCH  = 10000\n",
    "VAL_STEPS        = 900\n",
    "\n",
    "GRAD_CLIP = 1.0\n",
    "LR = 3e-4\n",
    "\n",
    "CKPT_DIR   = \"/data/aiffel/babayakga/checkpoints/f_r_withcellline\"\n",
    "SAVE_EVERY = 2\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1) VOCAB (special + ENSG) from gene_metadata\n",
    "# =========================================================\n",
    "def build_vocab_from_gene_metadata(gene_meta_path: str):\n",
    "    \"\"\"\n",
    "    vocab-space: special tokens + all ensembl_id\n",
    "    gene-space: Tahoe token_id (0..N_GENES-1)\n",
    "    \"\"\"\n",
    "    SPECIAL_TOKENS = [\"[PAD]\", \"[CLS]\", \"[DRUG]\", \"[TARGET]\", \"[CELL]\", \"[MASK]\"]\n",
    "\n",
    "    gene_md = pd.read_parquet(gene_meta_path).copy()\n",
    "    gene_md[\"ensembl_id\"] = gene_md[\"ensembl_id\"].astype(str)\n",
    "    gene_md[\"token_id\"]   = gene_md[\"token_id\"].astype(int)\n",
    "    gene_md = gene_md.sort_values(\"token_id\").reset_index(drop=True)\n",
    "\n",
    "    N_GENES = int(gene_md[\"token_id\"].max()) + 1\n",
    "\n",
    "    local_token_to_id = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
    "    for ensg in gene_md[\"ensembl_id\"].tolist():\n",
    "        if ensg not in local_token_to_id:\n",
    "            local_token_to_id[ensg] = len(local_token_to_id)\n",
    "\n",
    "    token_id_to_vocab_id = {\n",
    "        int(tid): int(local_token_to_id[str(ensg)])\n",
    "        for tid, ensg in zip(gene_md[\"token_id\"].values, gene_md[\"ensembl_id\"].values)\n",
    "    }\n",
    "\n",
    "    ensg_to_token_id = {\n",
    "        str(ensg): int(tid)\n",
    "        for ensg, tid in zip(gene_md[\"ensembl_id\"].values, gene_md[\"token_id\"].values)\n",
    "    }\n",
    "\n",
    "    PAD_ID  = local_token_to_id[\"[PAD]\"]\n",
    "    CLS_ID  = local_token_to_id[\"[CLS]\"]\n",
    "    DRUG_ID = local_token_to_id[\"[DRUG]\"]\n",
    "    CELL_ID = local_token_to_id[\"[CELL]\"]\n",
    "\n",
    "    return local_token_to_id, token_id_to_vocab_id, ensg_to_token_id, N_GENES, SPECIAL_TOKENS, PAD_ID, CLS_ID, DRUG_ID, CELL_ID\n",
    "\n",
    "\n",
    "local_token_to_id, token_id_to_vocab_id, ensg_to_token_id, N_GENES, SPECIAL_TOKENS, PAD_ID, CLS_ID, DRUGTOK_ID, CELLTOK_ID = \\\n",
    "    build_vocab_from_gene_metadata(GENE_META_PATH)\n",
    "\n",
    "VOCAB_SIZE = len(local_token_to_id)\n",
    "print(\"VOCAB_SIZE(vocab-space):\", VOCAB_SIZE)\n",
    "print(\"N_GENES(gene-space):\", N_GENES)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) LOAD cell2id mapping (MUST match pretrained cell embeddings)\n",
    "# =========================================================\n",
    "if not os.path.exists(CELL2ID_CSV):\n",
    "    raise FileNotFoundError(f\"cell2id.csv not found: {CELL2ID_CSV}\")\n",
    "if not os.path.exists(CELL_EMB_NPY):\n",
    "    raise FileNotFoundError(f\"cell_embeddings.npy not found: {CELL_EMB_NPY}\")\n",
    "\n",
    "cell2id_df = pd.read_csv(CELL2ID_CSV)\n",
    "cell2id_df[\"cell_line_id\"] = cell2id_df[\"cell_line_id\"].astype(str)\n",
    "cell_line2id = {c: int(i) for c, i in zip(cell2id_df[\"cell_line_id\"], cell2id_df[\"cell_id\"])}\n",
    "\n",
    "NUM_CELL_LINE = len(cell_line2id)\n",
    "W_cell = np.load(CELL_EMB_NPY)\n",
    "print(\"NUM_CELL_LINE(from cell2id.csv):\", NUM_CELL_LINE, \"| cell_emb rows:\", W_cell.shape[0])\n",
    "\n",
    "assert W_cell.shape[0] == NUM_CELL_LINE, f\"cell2id size {NUM_CELL_LINE} != cell_emb rows {W_cell.shape[0]}\"\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3) SPLIT PAIRS (drug, cell_line) from COUNTS_CSV\n",
    "# =========================================================\n",
    "DRUG_COL, CELL_COL, N_COL = \"drug\", \"cell_line_id\", \"n_cells\"\n",
    "\n",
    "counts = pd.read_csv(COUNTS_CSV)\n",
    "counts[DRUG_COL] = counts[DRUG_COL].astype(str)\n",
    "counts[CELL_COL] = counts[CELL_COL].astype(str)\n",
    "\n",
    "MIN_TRAIN = 1000\n",
    "MIN_EVAL  = 1000\n",
    "\n",
    "train_pool = counts[counts[N_COL] >= MIN_TRAIN].copy()\n",
    "eval_pool  = counts[counts[N_COL] >= MIN_EVAL].copy()\n",
    "\n",
    "pairs_df = train_pool[[DRUG_COL, CELL_COL]].drop_duplicates()\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    pairs_df,\n",
    "    test_size=0.1,\n",
    "    random_state=SEED,\n",
    "    stratify=pairs_df[DRUG_COL] if len(pairs_df) else None,\n",
    ")\n",
    "\n",
    "train_df = train_df[train_df[DRUG_COL] != CONTROL_DRUG]\n",
    "val_df   = val_df[val_df[DRUG_COL]   != CONTROL_DRUG]\n",
    "\n",
    "train_pairs = list(zip(train_df[DRUG_COL], train_df[CELL_COL]))\n",
    "val_pairs   = list(zip(val_df[DRUG_COL],   val_df[CELL_COL]))\n",
    "\n",
    "eval_pairs_df = eval_pool[[DRUG_COL, CELL_COL]].drop_duplicates()\n",
    "eval_pairs_df = eval_pairs_df[eval_pairs_df[DRUG_COL] != CONTROL_DRUG]\n",
    "eval_pairs = list(zip(eval_pairs_df[DRUG_COL], eval_pairs_df[CELL_COL]))\n",
    "\n",
    "print(\"train pairs:\", len(train_pairs))\n",
    "print(\"val pairs:\", len(val_pairs))\n",
    "print(f\"eval pairs (>={MIN_EVAL}):\", len(eval_pairs))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4) INDEX PARQUET row-groups for valid pairs\n",
    "# =========================================================\n",
    "PARQUET_FILES = sorted(glob.glob(os.path.join(PARQUET_DIR, \"**\", \"*.parquet\"), recursive=True))\n",
    "print(\"parquet files found:\", len(PARQUET_FILES))\n",
    "\n",
    "PARQUET_DRUG_COL = \"drug\"\n",
    "PARQUET_CELL_COL = \"cell_line_id\"\n",
    "\n",
    "def build_pair_to_locations(parquet_files, valid_pairs_set, drug_col, cell_col):\n",
    "    out = defaultdict(list)\n",
    "    for f in tqdm(parquet_files, desc=\"Index parquet row-groups\"):\n",
    "        try:\n",
    "            pf = pq.ParquetFile(f)\n",
    "        except Exception:\n",
    "            continue\n",
    "        for rg in range(pf.num_row_groups):\n",
    "            try:\n",
    "                tbl = pf.read_row_group(rg, columns=[drug_col, cell_col])\n",
    "                df = tbl.to_pandas()\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            pairs_here = set(zip(df[drug_col].astype(str), df[cell_col].astype(str)))\n",
    "            inter = pairs_here.intersection(valid_pairs_set)\n",
    "            for p in inter:\n",
    "                out[p].append((f, rg))\n",
    "    return dict(out)\n",
    "\n",
    "valid_pairs_set = set(train_pairs) | set(val_pairs)\n",
    "pair_to_locations = build_pair_to_locations(\n",
    "    parquet_files=PARQUET_FILES,\n",
    "    valid_pairs_set=valid_pairs_set,\n",
    "    drug_col=PARQUET_DRUG_COL,\n",
    "    cell_col=PARQUET_CELL_COL\n",
    ")\n",
    "print(\"indexed pairs:\", len(pair_to_locations))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5) DMSO baselines + topK variance genes (gene-space token_id)\n",
    "#    âœ… filter token_id < 3 here too\n",
    "# =========================================================\n",
    "def build_dmso_baselines_gene_space(dmso_h5ad_path: str, control_drug: str, N_GENES: int, ensg_to_token_id: dict,\n",
    "                                    drug_col=\"drug\", cell_col=\"cell_line_id\", dtype=np.float32):\n",
    "    adata = sc.read_h5ad(dmso_h5ad_path)\n",
    "    obs = adata.obs\n",
    "    X = adata.X.tocsr() if sparse.issparse(adata.X) else sparse.csr_matrix(adata.X)\n",
    "\n",
    "    m = (obs[drug_col].astype(str).values == control_drug)\n",
    "    idx_ctrl = np.where(m)[0]\n",
    "    if idx_ctrl.size == 0:\n",
    "        raise ValueError(f\"No control rows: {control_drug}\")\n",
    "\n",
    "    ensgs = adata.var_names.astype(str).tolist()\n",
    "    token_ids, cols = [], []\n",
    "    for j, ensg in enumerate(ensgs):\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        tid = int(tid)\n",
    "        if tid < MIN_GENE_TOKEN_ID:        # âœ… filter 0/1/2\n",
    "            continue\n",
    "        token_ids.append(tid)\n",
    "        cols.append(j)\n",
    "\n",
    "    token_ids = np.asarray(token_ids, dtype=np.int64)\n",
    "    cols      = np.asarray(cols, dtype=np.int64)\n",
    "\n",
    "    Xc = X[idx_ctrl][:, cols]\n",
    "    mean_global_sub = np.asarray(Xc.mean(axis=0)).ravel().astype(dtype)\n",
    "\n",
    "    baseline_global = np.zeros(N_GENES, dtype=dtype)\n",
    "    baseline_global[token_ids] = mean_global_sub\n",
    "\n",
    "    baseline_by_cl = {}\n",
    "    cls_all = obs[cell_col].astype(str).values\n",
    "    for cl in np.unique(cls_all):\n",
    "        cl_idx = np.where(m & (cls_all == cl))[0]\n",
    "        if cl_idx.size == 0:\n",
    "            continue\n",
    "        Xcl = X[cl_idx][:, cols]\n",
    "        mean_cl_sub = np.asarray(Xcl.mean(axis=0)).ravel().astype(dtype)\n",
    "        v = np.zeros(N_GENES, dtype=dtype)\n",
    "        v[token_ids] = mean_cl_sub\n",
    "        baseline_by_cl[str(cl)] = v\n",
    "\n",
    "    return baseline_global, baseline_by_cl\n",
    "\n",
    "\n",
    "def topk_by_variance_gene_space(dmso_h5ad_path: str, control_drug: str, N_GENES: int, ensg_to_token_id: dict,\n",
    "                               drug_col=\"drug\", top_k=1000):\n",
    "    adata = sc.read_h5ad(dmso_h5ad_path)\n",
    "    obs = adata.obs\n",
    "    X = adata.X.tocsr() if sparse.issparse(adata.X) else sparse.csr_matrix(adata.X)\n",
    "\n",
    "    m = (obs[drug_col].astype(str).values == control_drug)\n",
    "    idx = np.where(m)[0]\n",
    "    if idx.size == 0:\n",
    "        raise ValueError(f\"No control rows: {control_drug}\")\n",
    "\n",
    "    ensgs = adata.var_names.astype(str).tolist()\n",
    "    token_ids, cols = [], []\n",
    "    for j, ensg in enumerate(ensgs):\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        tid = int(tid)\n",
    "        if tid < MIN_GENE_TOKEN_ID:        # âœ… filter 0/1/2\n",
    "            continue\n",
    "        token_ids.append(tid)\n",
    "        cols.append(j)\n",
    "\n",
    "    token_ids = np.asarray(token_ids, dtype=np.int64)\n",
    "    cols      = np.asarray(cols, dtype=np.int64)\n",
    "\n",
    "    Xc = X[idx][:, cols]\n",
    "    ex  = np.asarray(Xc.mean(axis=0)).ravel()\n",
    "    ex2 = np.asarray(Xc.power(2).mean(axis=0)).ravel()\n",
    "    var = ex2 - ex**2\n",
    "\n",
    "    top_local = np.argsort(-var)[:top_k]\n",
    "    top_gene_token_ids = token_ids[top_local]\n",
    "    return top_gene_token_ids.astype(np.int64)\n",
    "\n",
    "\n",
    "baseline_global, baseline_by_cl = build_dmso_baselines_gene_space(\n",
    "    dmso_h5ad_path=DMSO_H5AD,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    N_GENES=N_GENES,\n",
    "    ensg_to_token_id=ensg_to_token_id,\n",
    ")\n",
    "\n",
    "sorted_gene_token_ids = topk_by_variance_gene_space(\n",
    "    dmso_h5ad_path=DMSO_H5AD,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    N_GENES=N_GENES,\n",
    "    ensg_to_token_id=ensg_to_token_id,\n",
    "    top_k=TOP_K,\n",
    ")\n",
    "\n",
    "assert (sorted_gene_token_ids >= MIN_GENE_TOKEN_ID).all(), \"TOP_K contains token_id < 3 !\"\n",
    "\n",
    "print(\"baseline_global:\", baseline_global.shape, \"baseline_by_cl:\", len(baseline_by_cl))\n",
    "print(\"sorted_gene_token_ids:\", sorted_gene_token_ids.shape, sorted_gene_token_ids[:10])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6) DRUG -> id + SMILES embeddings\n",
    "# =========================================================\n",
    "drug_meta_df = pd.read_parquet(DRUG_META_PATH).copy()\n",
    "drug_meta_df[\"drug\"] = drug_meta_df[\"drug\"].astype(str)\n",
    "drugs = drug_meta_df[\"drug\"].tolist()\n",
    "drug2id = {d: i for i, d in enumerate(drugs)}\n",
    "print(\"num drugs:\", len(drug2id))\n",
    "\n",
    "smiles_tensor = torch.load(SMILES_EMB_PATH, map_location=\"cpu\").to(torch.float32)\n",
    "assert smiles_tensor.shape[0] == len(drug_meta_df), \"SMILES rows != drug_metadata rows\"\n",
    "\n",
    "drug_to_smiles_emb = {d: smiles_tensor[i] for i, d in enumerate(drugs)}\n",
    "smiles_dim = int(smiles_tensor.shape[-1])\n",
    "print(\"smiles_dim:\", smiles_dim)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7) pair weights\n",
    "# =========================================================\n",
    "def make_pair_weights_from_counts(counts_df, pairs, drug_col=\"drug\", cell_col=\"cell_line_id\", n_col=\"n_cells\",\n",
    "                                  mode=\"inv_sqrt\", eps=1.0):\n",
    "    tmp = counts_df[[drug_col, cell_col, n_col]].copy()\n",
    "    tmp[drug_col] = tmp[drug_col].astype(str)\n",
    "    tmp[cell_col] = tmp[cell_col].astype(str)\n",
    "\n",
    "    pair2n = {(d, c): int(n) for d, c, n in tmp.values}\n",
    "\n",
    "    w = []\n",
    "    for p in pairs:\n",
    "        n = pair2n.get(p, 0)\n",
    "        if mode == \"inv\":\n",
    "            ww = 1.0 / (n + eps)\n",
    "        elif mode == \"inv_log\":\n",
    "            ww = 1.0 / np.log1p(n + eps)\n",
    "        else:\n",
    "            ww = 1.0 / np.sqrt(n + eps)\n",
    "        w.append(float(ww))\n",
    "\n",
    "    w = np.asarray(w, dtype=np.float64)\n",
    "    w = np.clip(w, 0.0, None)\n",
    "    w = w / (w.sum() + 1e-12)\n",
    "    pair2w = {p: float(wi) for p, wi in zip(pairs, w)}\n",
    "    return w, pair2w\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8) DATASET (Iterable, aligned)  âœ… filters token_id<3\n",
    "# =========================================================\n",
    "class FRSeqExpressionParquetDatasetAligned(IterableDataset):\n",
    "    \"\"\"\n",
    "    input_ids: [CLS][DRUG][CELL] + gene tokens (vocab-space ids)\n",
    "    values:    delta (val - baseline[cell or global]) aligned with tokens\n",
    "    y_topk:    true expression on TOP_K genes (gene-space token_id list)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pair_to_locations,\n",
    "        pairs,\n",
    "        token_id_to_vocab_id,\n",
    "        sorted_gene_token_ids,\n",
    "        baseline_global,\n",
    "        baseline_by_cellline,\n",
    "        cell_line2id,\n",
    "        drug2id,\n",
    "        drug_to_smiles_emb,\n",
    "        pair_weights=None,\n",
    "        seed=42,\n",
    "        max_gene_len=512,\n",
    "        top_k=1000,\n",
    "        batch_size=16,\n",
    "        pad_id=0,\n",
    "        cls_id=1,\n",
    "        drugtok_id=2,\n",
    "        celltok_id=4,\n",
    "        drug_col=\"drug\",\n",
    "        cell_col=\"cell_line_id\",\n",
    "        genes_col=\"genes\",\n",
    "        expr_col=\"expressions\",\n",
    "        cap_per_pair_in_rg=None,\n",
    "        max_tries=30,\n",
    "        shuffle=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pair_to_locations = pair_to_locations\n",
    "        self.pairs = list(pairs)\n",
    "\n",
    "        self.token_id_to_vocab_id = token_id_to_vocab_id\n",
    "        self.q = np.asarray(sorted_gene_token_ids, dtype=np.int64)\n",
    "\n",
    "        self.baseline_global = np.asarray(baseline_global, dtype=np.float32)\n",
    "        self.baseline_by_cellline = baseline_by_cellline or {}\n",
    "\n",
    "        self.cell_line2id = cell_line2id\n",
    "        self.drug2id = drug2id\n",
    "        self.drug_to_smiles_emb = drug_to_smiles_emb\n",
    "\n",
    "        self.max_gene_len = int(max_gene_len)\n",
    "        self.top_k = int(top_k)\n",
    "        self.batch_size = int(batch_size)\n",
    "\n",
    "        self.pad_id = int(pad_id)\n",
    "        self.cls_id = int(cls_id)\n",
    "        self.drugtok_id = int(drugtok_id)\n",
    "        self.celltok_id = int(celltok_id)\n",
    "\n",
    "        self.drug_col = drug_col\n",
    "        self.cell_col = cell_col\n",
    "        self.genes_col = genes_col\n",
    "        self.expr_col = expr_col\n",
    "\n",
    "        self.cap_per_pair_in_rg = cap_per_pair_in_rg\n",
    "        self.max_tries = int(max_tries)\n",
    "        self.shuffle = bool(shuffle)\n",
    "\n",
    "        self.num_prefix = 3  # âœ… [CLS][DRUG][CELL]\n",
    "        self.seq_len = self.num_prefix + self.max_gene_len\n",
    "\n",
    "        any_vec = next(iter(self.drug_to_smiles_emb.values()))\n",
    "        self.smiles_dim = int(any_vec.shape[-1])\n",
    "\n",
    "        # weights\n",
    "        self.seed = int(seed)\n",
    "        if pair_weights is None:\n",
    "            self.pair_weights = None\n",
    "        elif isinstance(pair_weights, dict):\n",
    "            w = np.asarray([pair_weights.get(p, 0.0) for p in self.pairs], dtype=np.float64)\n",
    "            w = np.clip(w, 0.0, None)\n",
    "            w = w / (w.sum() + 1e-12)\n",
    "            self.pair_weights = w\n",
    "        else:\n",
    "            w = np.asarray(pair_weights, dtype=np.float64)\n",
    "            assert len(w) == len(self.pairs), \"pair_weights length must match pairs length\"\n",
    "            w = np.clip(w, 0.0, None)\n",
    "            w = w / (w.sum() + 1e-12)\n",
    "            self.pair_weights = w\n",
    "\n",
    "    def _read_row_group_df(self, file_path, rg_id, columns):\n",
    "        pf = pq.ParquetFile(file_path)\n",
    "        return pf.read_row_group(rg_id, columns=columns).to_pandas()\n",
    "\n",
    "    def _prepare_sparse(self, genes, expr):\n",
    "        idx = np.asarray(genes, dtype=np.int64)\n",
    "        val = np.asarray(expr, dtype=np.float32)\n",
    "        if idx.size == 0:\n",
    "            return idx, val\n",
    "\n",
    "        # âœ… FILTER: remove non-gene tokens 0/1/2 (and anything <3)\n",
    "        keep = idx >= MIN_GENE_TOKEN_ID\n",
    "        idx = idx[keep]\n",
    "        val = val[keep]\n",
    "        if idx.size == 0:\n",
    "            return idx, val\n",
    "\n",
    "        order = np.argsort(idx)\n",
    "        return idx[order], val[order]\n",
    "\n",
    "    def _make_y_true_topk(self, idx_sorted, val_sorted):\n",
    "        q = self.q\n",
    "        if idx_sorted.size == 0:\n",
    "            return np.zeros(q.shape[0], dtype=np.float32)\n",
    "\n",
    "        pos = np.searchsorted(idx_sorted, q)\n",
    "        y = np.zeros(q.shape[0], dtype=np.float32)\n",
    "\n",
    "        in_bounds = (pos < idx_sorted.size)\n",
    "        match = np.zeros(q.shape[0], dtype=bool)\n",
    "        match[in_bounds] = (idx_sorted[pos[in_bounds]] == q[in_bounds])\n",
    "        ok = in_bounds & match\n",
    "        y[ok] = val_sorted[pos[ok]]\n",
    "        return y.astype(np.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        rng = np.random.default_rng(self.seed if worker_info is None else (self.seed + worker_info.id))\n",
    "\n",
    "        pairs = self.pairs\n",
    "        weights = self.pair_weights\n",
    "        cols = [self.drug_col, self.cell_col, self.genes_col, self.expr_col]\n",
    "\n",
    "        while True:\n",
    "            # weighted pair sampling\n",
    "            if weights is None:\n",
    "                drug_name, cell_line = pairs[rng.integers(0, len(pairs))]\n",
    "            else:\n",
    "                idxp = rng.choice(len(pairs), p=weights)\n",
    "                drug_name, cell_line = pairs[idxp]\n",
    "\n",
    "            locs = self.pair_to_locations.get((drug_name, cell_line), [])\n",
    "            if not locs:\n",
    "                continue\n",
    "\n",
    "            for _ in range(self.max_tries):\n",
    "                fpath, rg_id = locs[rng.integers(0, len(locs))]\n",
    "\n",
    "                try:\n",
    "                    df = self._read_row_group_df(fpath, rg_id, columns=cols)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                df = df[(df[self.drug_col].astype(str) == str(drug_name)) &\n",
    "                        (df[self.cell_col].astype(str) == str(cell_line))]\n",
    "                if len(df) < self.batch_size:\n",
    "                    continue\n",
    "\n",
    "                if self.cap_per_pair_in_rg is not None and len(df) > self.cap_per_pair_in_rg:\n",
    "                    df = df.sample(self.cap_per_pair_in_rg, replace=False, random_state=int(rng.integers(0, 1e9)))\n",
    "\n",
    "                df = df.sample(self.batch_size, replace=False, random_state=int(rng.integers(0, 1e9)))\n",
    "\n",
    "                # baseline (cell-specific if exists)\n",
    "                baseline = self.baseline_by_cellline.get(cell_line, self.baseline_global)\n",
    "\n",
    "                # cell id must exist in pretrained mapping\n",
    "                if cell_line not in self.cell_line2id:\n",
    "                    continue\n",
    "                cell_id = self.cell_line2id[cell_line]\n",
    "\n",
    "                drug_id = self.drug2id.get(drug_name, 0)\n",
    "\n",
    "                sm = self.drug_to_smiles_emb.get(drug_name, None)\n",
    "                if sm is None:\n",
    "                    smiles_emb = torch.zeros(self.smiles_dim, dtype=torch.float32)\n",
    "                else:\n",
    "                    smiles_emb = sm.detach().clone().to(torch.float32) if isinstance(sm, torch.Tensor) \\\n",
    "                        else torch.tensor(sm, dtype=torch.float32)\n",
    "\n",
    "                input_ids = np.full((self.batch_size, self.seq_len), self.pad_id, dtype=np.int64)\n",
    "                values    = np.zeros((self.batch_size, self.seq_len), dtype=np.float32)\n",
    "                mask      = np.zeros((self.batch_size, self.seq_len), dtype=np.int64)\n",
    "                y_topk    = np.zeros((self.batch_size, self.top_k), dtype=np.float32)\n",
    "\n",
    "                cell_batch   = np.full((self.batch_size,), cell_id, dtype=np.int64)\n",
    "                drug_batch   = np.full((self.batch_size,), drug_id, dtype=np.int64)\n",
    "                smiles_batch = np.stack([smiles_emb.numpy()] * self.batch_size, axis=0).astype(np.float32)\n",
    "\n",
    "                # prefix: [CLS][DRUG][CELL]\n",
    "                input_ids[:, 0] = self.cls_id\n",
    "                input_ids[:, 1] = self.drugtok_id\n",
    "                input_ids[:, 2] = self.celltok_id\n",
    "                mask[:, :self.num_prefix] = 1\n",
    "\n",
    "                for b, (_, r) in enumerate(df.iterrows()):\n",
    "                    idx, val = self._prepare_sparse(r[self.genes_col], r[self.expr_col])\n",
    "\n",
    "                    # y_true on TOP_K\n",
    "                    y_topk[b] = self._make_y_true_topk(idx, val)\n",
    "\n",
    "                    if idx.size == 0:\n",
    "                        continue\n",
    "\n",
    "                    # delta\n",
    "                    base_vals = baseline[idx]\n",
    "                    delta = (val - base_vals)\n",
    "\n",
    "                    # choose top genes by |delta|\n",
    "                    k = min(self.max_gene_len, idx.size)\n",
    "                    if k <= 0:\n",
    "                        continue\n",
    "\n",
    "                    if k == idx.size:\n",
    "                        top_pos = np.argsort(-np.abs(delta))\n",
    "                    else:\n",
    "                        top_pos = np.argpartition(-np.abs(delta), k - 1)[:k]\n",
    "                        top_pos = top_pos[np.argsort(-np.abs(delta[top_pos]))]\n",
    "\n",
    "                    sel_gene_token_ids = idx[top_pos]\n",
    "                    sel_delta = delta[top_pos]\n",
    "\n",
    "                    # token_id -> vocab_id (drop missing)\n",
    "                    sel_vocab_ids = np.asarray(\n",
    "                        [self.token_id_to_vocab_id.get(int(t), -1) for t in sel_gene_token_ids],\n",
    "                        dtype=np.int64\n",
    "                    )\n",
    "                    ok = sel_vocab_ids != -1\n",
    "                    sel_vocab_ids = sel_vocab_ids[ok]\n",
    "                    sel_delta = sel_delta[ok]\n",
    "\n",
    "                    L = min(self.max_gene_len, sel_vocab_ids.size)\n",
    "                    if L <= 0:\n",
    "                        continue\n",
    "\n",
    "                    start = self.num_prefix\n",
    "                    input_ids[b, start:start+L] = sel_vocab_ids[:L]\n",
    "                    values[b,    start:start+L] = sel_delta[:L]\n",
    "                    mask[b,      start:start+L] = 1\n",
    "\n",
    "                yield (\n",
    "                    torch.tensor(input_ids, dtype=torch.long),\n",
    "                    torch.tensor(values, dtype=torch.float32),\n",
    "                    torch.tensor(mask, dtype=torch.long),\n",
    "                    torch.tensor(y_topk, dtype=torch.float32),\n",
    "                    torch.tensor(cell_batch, dtype=torch.long),\n",
    "                    torch.tensor(drug_batch, dtype=torch.long),\n",
    "                    torch.tensor(smiles_batch, dtype=torch.float32),\n",
    "                )\n",
    "                break\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9) DataLoaders\n",
    "# =========================================================\n",
    "train_w, _ = make_pair_weights_from_counts(counts, train_pairs, mode=\"inv_sqrt\")\n",
    "\n",
    "train_ds = FRSeqExpressionParquetDatasetAligned(\n",
    "    pair_to_locations=pair_to_locations,\n",
    "    pairs=train_pairs,\n",
    "    pair_weights=train_w,\n",
    "    token_id_to_vocab_id=token_id_to_vocab_id,\n",
    "    sorted_gene_token_ids=sorted_gene_token_ids,\n",
    "    baseline_global=baseline_global,\n",
    "    baseline_by_cellline=baseline_by_cl,\n",
    "    cell_line2id=cell_line2id,\n",
    "    drug2id=drug2id,\n",
    "    drug_to_smiles_emb=drug_to_smiles_emb,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_gene_len=MAX_LEN,\n",
    "    top_k=TOP_K,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "val_ds = FRSeqExpressionParquetDatasetAligned(\n",
    "    pair_to_locations=pair_to_locations,\n",
    "    pairs=val_pairs,\n",
    "    pair_weights=None,\n",
    "    token_id_to_vocab_id=token_id_to_vocab_id,\n",
    "    sorted_gene_token_ids=sorted_gene_token_ids,\n",
    "    baseline_global=baseline_global,\n",
    "    baseline_by_cellline=baseline_by_cl,\n",
    "    cell_line2id=cell_line2id,\n",
    "    drug2id=drug2id,\n",
    "    drug_to_smiles_emb=drug_to_smiles_emb,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_gene_len=MAX_LEN,\n",
    "    top_k=TOP_K,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=None, num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=None, num_workers=0, pin_memory=True)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10) MODEL (Cell2Sentence-like Encoder for f_r)\n",
    "# =========================================================\n",
    "class Cell2SentenceEncoderFR(nn.Module):\n",
    "    \"\"\"\n",
    "    prefix: [CLS][DRUG][CELL] + gene tokens\n",
    "    - token_emb: vocab-space ids\n",
    "    - values: delta\n",
    "    - inject smiles into position 1 ([DRUG])\n",
    "    - inject cell_line embedding into position 2 ([CELL]) âœ… pretrained\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, n_heads, num_layers, max_len_with_prefix, smiles_dim, num_cell_lines, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n",
    "        self.value_proj = nn.Sequential(\n",
    "            nn.Linear(1, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "        )\n",
    "        self.pos_emb = nn.Embedding(max_len_with_prefix, d_model)\n",
    "\n",
    "        self.cell_line_emb = nn.Embedding(num_cell_lines, d_model)\n",
    "        self.smiles_proj = nn.Linear(smiles_dim, d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads,\n",
    "            dim_feedforward=4*d_model,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, input_ids, values, attention_mask, cell_line_id, smiles_emb):\n",
    "        B, L = input_ids.shape\n",
    "        device_ = input_ids.device\n",
    "\n",
    "        x = self.token_emb(input_ids) + self.value_proj(values.unsqueeze(-1))\n",
    "\n",
    "        pos = torch.arange(L, device=device_).unsqueeze(0).expand(B, L)\n",
    "        x = x + self.pos_emb(pos)\n",
    "\n",
    "        # inject drug / cell info\n",
    "        x[:, 1, :] = x[:, 1, :] + self.smiles_proj(smiles_emb.to(device=device_, dtype=torch.float32)).to(x.dtype)\n",
    "        x[:, 2, :] = x[:, 2, :] + self.cell_line_emb(cell_line_id.to(device=device_)).to(x.dtype)\n",
    "\n",
    "        key_padding_mask = (attention_mask == 0)\n",
    "        h = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        return h[:, 0, :]  # CLS\n",
    "\n",
    "\n",
    "class FRModelExpression(nn.Module):\n",
    "    def __init__(self, encoder, d_model, out_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = nn.Linear(d_model, out_dim)\n",
    "\n",
    "    def forward(self, input_ids, values, mask, cell_line_id, smiles_emb):\n",
    "        h = self.encoder(input_ids, values, mask, cell_line_id, smiles_emb)\n",
    "        return self.head(h)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 11) Load pretrained gene + cell embeddings\n",
    "# =========================================================\n",
    "def load_pretrained_token_emb_from_gene_metadata(token_emb: nn.Embedding, npy_path: str, gene_meta_path: str, local_token_to_id: dict, device):\n",
    "    W = np.load(npy_path)  # (N_genes, d_model)\n",
    "    Wt = torch.tensor(W, dtype=torch.float32, device=device)\n",
    "\n",
    "    gene_md = pd.read_parquet(gene_meta_path).copy()\n",
    "    gene_md[\"ensembl_id\"] = gene_md[\"ensembl_id\"].astype(str)\n",
    "    gene_md[\"token_id\"] = gene_md[\"token_id\"].astype(int)\n",
    "    gene_md = gene_md.sort_values(\"token_id\").reset_index(drop=True)\n",
    "\n",
    "    if Wt.shape[1] != token_emb.weight.shape[1]:\n",
    "        raise ValueError(f\"d mismatch: npy d={Wt.shape[1]} vs token_emb d={token_emb.weight.shape[1]}\")\n",
    "\n",
    "    # NOTE: original training saved gene_embeddings.npy as \"special removed\" (index 0 corresponds to gene_md row 0)\n",
    "    n = min(len(gene_md), Wt.shape[0])\n",
    "    loaded = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            ensg = gene_md.loc[i, \"ensembl_id\"]\n",
    "            vid = local_token_to_id.get(ensg, None)\n",
    "            if vid is None:\n",
    "                continue\n",
    "            token_emb.weight[vid].copy_(Wt[i])\n",
    "            loaded += 1\n",
    "    print(f\"âœ… Loaded pretrained gene token_emb: {loaded} genes\")\n",
    "\n",
    "\n",
    "def load_pretrained_cell_emb(cell_emb: nn.Embedding, cell_emb_npy: str, device):\n",
    "    W = np.load(cell_emb_npy)  # (num_cell_lines, d_model)\n",
    "    Wt = torch.tensor(W, dtype=torch.float32, device=device)\n",
    "\n",
    "    if Wt.shape != cell_emb.weight.shape:\n",
    "        raise ValueError(f\"cell_emb shape mismatch: npy={tuple(Wt.shape)} vs emb={tuple(cell_emb.weight.shape)}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cell_emb.weight.copy_(Wt)\n",
    "    print(f\"âœ… Loaded pretrained cell_line_emb: {tuple(Wt.shape)}\")\n",
    "\n",
    "\n",
    "\n",
    "def sanity_check_gene_emb_mapping(\n",
    "    gene_meta_path,\n",
    "    local_token_to_id,\n",
    "    token_emb: torch.nn.Embedding,\n",
    "    pretrained_gene_npy,\n",
    "    n_check=20,\n",
    "    seed=0,\n",
    "):\n",
    "    gene_md = pd.read_parquet(gene_meta_path).copy()\n",
    "    gene_md[\"ensembl_id\"] = gene_md[\"ensembl_id\"].astype(str)\n",
    "    gene_md[\"token_id\"]   = gene_md[\"token_id\"].astype(int)\n",
    "    gene_md = gene_md.sort_values(\"token_id\").reset_index(drop=True)\n",
    "\n",
    "    W = np.load(pretrained_gene_npy)  # (N_genes, d_model)\n",
    "    assert W.shape[1] == token_emb.weight.shape[1]\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idxs = rng.integers(0, min(len(gene_md), W.shape[0]), size=n_check)\n",
    "\n",
    "    max_abs = 0.0\n",
    "    bad = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in idxs:\n",
    "            ensg = gene_md.loc[i, \"ensembl_id\"]\n",
    "            vid = local_token_to_id.get(ensg, None)\n",
    "            if vid is None:\n",
    "                continue\n",
    "\n",
    "            a = token_emb.weight[vid].detach().cpu().numpy()\n",
    "            b = W[i]\n",
    "\n",
    "            diff = np.max(np.abs(a - b))\n",
    "            max_abs = max(max_abs, float(diff))\n",
    "            if diff > 1e-6:\n",
    "                bad += 1\n",
    "                print(\"Mismatch:\", \"i=\", i, \"ensg=\", ensg, \"vid=\", vid, \"max_abs_diff=\", diff)\n",
    "\n",
    "    print(f\"[sanity] checked={n_check}, bad={bad}, max_abs_diff={max_abs}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 12) Init model\n",
    "# =========================================================\n",
    "D_MODEL = 256\n",
    "assert W_cell.shape[1] == D_MODEL, f\"cell_emb dim {W_cell.shape[1]} != D_MODEL {D_MODEL}\"\n",
    "\n",
    "encoder = Cell2SentenceEncoderFR(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=8,\n",
    "    num_layers=4,\n",
    "    max_len_with_prefix=(3 + MAX_LEN),   # prefix 3 + gene_len\n",
    "    smiles_dim=smiles_dim,\n",
    "    num_cell_lines=NUM_CELL_LINE,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "# gene embeddings\n",
    "load_pretrained_token_emb_from_gene_metadata(\n",
    "    token_emb=encoder.token_emb,\n",
    "    npy_path=PRETRAINED_GENE_NPY,\n",
    "    gene_meta_path=GENE_META_PATH,\n",
    "    local_token_to_id=local_token_to_id,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# âœ… cell embeddings (pretrained)\n",
    "load_pretrained_cell_emb(\n",
    "    cell_emb=encoder.cell_line_emb,\n",
    "    cell_emb_npy=CELL_EMB_NPY,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "fr_model = FRModelExpression(encoder=encoder, d_model=D_MODEL, out_dim=TOP_K).to(device)\n",
    "optimizer = torch.optim.AdamW(fr_model.parameters(), lr=LR, weight_decay=0.01)\n",
    "scaler = GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "print(\"âœ… f_r model ready\")\n",
    "\n",
    "sanity_check_gene_emb_mapping(GENE_META_PATH, local_token_to_id, encoder.token_emb, PRETRAINED_GENE_NPY)\n",
    "\n",
    "\n",
    "gene_md = pd.read_parquet(GENE_META_PATH).copy()\n",
    "gene_md[\"token_id\"] = gene_md[\"token_id\"].astype(int)\n",
    "bad = gene_md.index.values != gene_md[\"token_id\"].values\n",
    "print(\"âŒ rows where index != token_id:\", bad.sum())\n",
    "\n",
    "if bad.any():\n",
    "    print(\"â— MISALIGNMENT: gene_md index â‰  token_id\")\n",
    "else:\n",
    "    print(\"âœ… OK: gene_md index == token_id\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb_table = encoder.token_emb.weight.detach().cpu().numpy()\n",
    "\n",
    "# Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ð¼ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ñ… gene token_id\n",
    "rng = np.random.default_rng(42)\n",
    "test_ids = rng.choice(gene_md[\"token_id\"].values, size=10, replace=False)\n",
    "\n",
    "for tid in test_ids:\n",
    "    ensg = gene_md.loc[gene_md[\"token_id\"] == tid, \"ensembl_id\"].values[0]\n",
    "    vocab_id = local_token_to_id[ensg]\n",
    "\n",
    "    diff = np.linalg.norm(emb_table[vocab_id] - W[tid])\n",
    "    print(f\"token_id={tid} | vocab_id={vocab_id} | diff={diff:.6f}\")\n",
    "\n",
    "# =========================================================\n",
    "# 13) Losses (MSE + ranking)\n",
    "# =========================================================\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "baseline_vec = torch.tensor(baseline_global[sorted_gene_token_ids], dtype=torch.float32, device=device)  # (TOP_K,)\n",
    "\n",
    "def expr_ranking_loss(y_pred, y_true, baseline_vec, top_pos=30, num_neg=80, margin=0.0):\n",
    "    device_ = y_pred.device\n",
    "    B, K = y_pred.shape\n",
    "\n",
    "    base = baseline_vec.view(1, K).expand(B, K).to(device=device_, dtype=y_pred.dtype)\n",
    "    dt = y_true - base\n",
    "    dp = y_pred - base\n",
    "\n",
    "    losses = []\n",
    "    for b in range(B):\n",
    "        order = torch.argsort(dt[b].abs(), descending=True)\n",
    "        P = min(top_pos, K)\n",
    "        pos_idx = order[:P]\n",
    "        neg_candidates = order[P:]\n",
    "        if neg_candidates.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        if neg_candidates.numel() > num_neg:\n",
    "            neg_idx = neg_candidates[torch.randperm(neg_candidates.numel(), device=device_)[:num_neg]]\n",
    "        else:\n",
    "            neg_idx = neg_candidates\n",
    "\n",
    "        pos_scores = dp[b, pos_idx]   # (P,)\n",
    "        neg_scores = dp[b, neg_idx]   # (N,)\n",
    "\n",
    "        diff = pos_scores.view(-1, 1) - neg_scores.view(1, -1)\n",
    "        loss_mat = F.relu(margin - diff)\n",
    "        losses.append(loss_mat.mean())\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=device_, dtype=y_pred.dtype)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 14) Checkpoint utils\n",
    "# =========================================================\n",
    "def save_fr_checkpoint(save_dir, fr_model, optimizer, scaler, epoch, metrics=None, extra=None, prefix=\"fr\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    ckpt = {\n",
    "        \"epoch\": int(epoch),\n",
    "        \"model_state\": fr_model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict() if scaler is not None else None,\n",
    "        \"metrics\": metrics or {},\n",
    "        \"extra\": extra or {},\n",
    "    }\n",
    "\n",
    "    path = os.path.join(save_dir, f\"{prefix}_epoch{epoch}_{ts}.pt\")\n",
    "    torch.save(ckpt, path)\n",
    "    print(f\"ðŸ’¾ saved checkpoint: {path}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 15) Eval helpers\n",
    "# =========================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_mse(fr_model, val_loader, steps, device):\n",
    "    fr_model.eval()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    for batch in islice(val_loader, steps):\n",
    "        input_ids, values, mask, y_true, cell_id, drug_id, smiles = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        values    = values.to(device)\n",
    "        mask      = mask.to(device)\n",
    "        y_true    = y_true.to(device)\n",
    "        cell_id   = cell_id.to(device)\n",
    "        smiles    = smiles.to(device)\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "            y_pred = fr_model(input_ids, values, mask, cell_id, smiles)\n",
    "            loss = mse_loss(y_pred, y_true)\n",
    "\n",
    "        bs = y_true.size(0)\n",
    "        total += loss.item() * bs\n",
    "        n += bs\n",
    "    return total / max(1, n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def baseline_mse(val_loader, steps, baseline_vec, device):\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    baseline_vec = baseline_vec.to(device)\n",
    "    for batch in islice(val_loader, steps):\n",
    "        _, _, _, y_true, _, _, _ = batch\n",
    "        y_true = y_true.to(device)\n",
    "        bs = y_true.size(0)\n",
    "        pred = baseline_vec.view(1, -1).expand(bs, -1)\n",
    "        loss = F.mse_loss(pred, y_true)\n",
    "        total += loss.item() * bs\n",
    "        n += bs\n",
    "    return total / max(1, n)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 16) TRAIN\n",
    "# =========================================================\n",
    "base_mse = baseline_mse(val_loader, steps=VAL_STEPS, baseline_vec=baseline_vec, device=device)\n",
    "print(f\"Baseline Valid MSE (DMSO) = {base_mse:.6f}\")\n",
    "\n",
    "print(\"ðŸš€ f_r training start\")\n",
    "\n",
    "for epoch in range(1, TOTAL_EPOCHS + 1):\n",
    "    lambda_rank = 0.0 if epoch <= WARMUP_EPOCHS else lambda_rank_main\n",
    "\n",
    "    fr_model.train()\n",
    "    run_mse = 0.0\n",
    "    run_rank = 0.0\n",
    "    run_total = 0.0\n",
    "    n = 0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        islice(train_loader, STEPS_PER_EPOCH),\n",
    "        total=STEPS_PER_EPOCH,\n",
    "        desc=f\"[Epoch {epoch}] Train\",\n",
    "        leave=True,\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        input_ids, values, mask, y_true, cell_id, drug_id, smiles = batch\n",
    "\n",
    "        input_ids = input_ids.to(device, non_blocking=True)\n",
    "        values    = values.to(device, non_blocking=True)\n",
    "        mask      = mask.to(device, non_blocking=True)\n",
    "        y_true    = y_true.to(device, non_blocking=True)\n",
    "        cell_id   = cell_id.to(device, non_blocking=True)\n",
    "        smiles    = smiles.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "            y_pred = fr_model(input_ids, values, mask, cell_id, smiles)\n",
    "            loss_m = mse_loss(y_pred, y_true)\n",
    "\n",
    "            if lambda_rank > 0:\n",
    "                loss_r = expr_ranking_loss(\n",
    "                    y_pred, y_true, baseline_vec,\n",
    "                    top_pos=30, num_neg=80, margin=0.0\n",
    "                )\n",
    "            else:\n",
    "                loss_r = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = loss_m + lambda_rank * loss_r\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            continue\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(fr_model.parameters(), GRAD_CLIP)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        bs = y_true.size(0)\n",
    "        run_mse   += loss_m.item() * bs\n",
    "        run_rank  += loss_r.item() * bs\n",
    "        run_total += loss.item() * bs\n",
    "        n += bs\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"mse\": f\"{loss_m.item():.4f}\",\n",
    "            \"rank\": f\"{loss_r.item():.4f}\",\n",
    "            \"Î»_rank\": float(lambda_rank),\n",
    "        })\n",
    "\n",
    "    train_mse   = run_mse   / max(1, n)\n",
    "    train_rank  = run_rank  / max(1, n)\n",
    "    train_total = run_total / max(1, n)\n",
    "\n",
    "    val_mse = eval_mse(fr_model, val_loader, steps=VAL_STEPS, device=device)\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch}] \"\n",
    "        f\"Train total={train_total:.6f}, mse={train_mse:.6f}, rank={train_rank:.6f} (Î»_rank={lambda_rank}) | \"\n",
    "        f\"Valid mse={val_mse:.6f} | Baseline(DMSO) mse={base_mse:.6f}\"\n",
    "    )\n",
    "\n",
    "    if (epoch % SAVE_EVERY == 0) or (epoch == TOTAL_EPOCHS):\n",
    "        save_fr_checkpoint(\n",
    "            save_dir=CKPT_DIR,\n",
    "            fr_model=fr_model,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "            epoch=epoch,\n",
    "            metrics={\n",
    "                \"train_total\": float(train_total),\n",
    "                \"train_mse\": float(train_mse),\n",
    "                \"train_rank\": float(train_rank),\n",
    "                \"val_mse\": float(val_mse),\n",
    "                \"baseline_mse\": float(base_mse),\n",
    "                \"lambda_rank\": float(lambda_rank),\n",
    "            },\n",
    "            extra={\n",
    "                \"TOP_K\": int(baseline_vec.numel()),\n",
    "                \"STEPS_PER_EPOCH\": int(STEPS_PER_EPOCH),\n",
    "                \"VAL_STEPS\": int(VAL_STEPS),\n",
    "                \"WARMUP_EPOCHS\": int(WARMUP_EPOCHS),\n",
    "                \"lambda_rank_main\": float(lambda_rank_main),\n",
    "                \"baseline_vec\": baseline_vec.detach().float().cpu(),\n",
    "                \"CELL2ID_CSV\": CELL2ID_CSV,\n",
    "                \"CELL_EMB_NPY\": CELL_EMB_NPY,\n",
    "                \"sorted_gene_token_ids\": sorted_gene_token_ids.astype(np.int64)\n",
    "            },\n",
    "            prefix=\"fr\",\n",
    "        )\n",
    "\n",
    "print(\"âœ… DONE\")\n",
    "\n",
    "gene_md = pd.read_parquet(GENE_META_PATH)[[\"token_id\",\"ensembl_id\"]].copy()\n",
    "tid2ensg = dict(zip(gene_md[\"token_id\"].astype(int), gene_md[\"ensembl_id\"].astype(str)))\n",
    "topk_ensg = np.array([tid2ensg[int(t)] for t in sorted_gene_token_ids], dtype=object)\n",
    "\n",
    "np.save(os.path.join(CKPT_DIR, f\"topk_ensg_k{TOP_K}.npy\"), topk_ensg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feda284",
   "metadata": {},
   "source": [
    "## fast option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31316287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# f_r FULL TRAINING SCRIPT (SAFE-FAST DATALOADER)\n",
    "# - âœ… token_id < 3 filtered everywhere\n",
    "# - âœ… NO [TARGET] token in input\n",
    "# - âœ… input prefix: [CLS][DRUG][CELL] + gene tokens\n",
    "# - âœ… load pretrained gene embeddings (gene_embeddings.npy)\n",
    "# - âœ… load pretrained cell-line embeddings (cell_embeddings.npy) + mapping (cell2id.csv)\n",
    "#\n",
    "# SAFE-FAST changes (Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑƒÑÐºÐ¾Ñ€Ð¸Ñ‚ÑŒ Ð¸ Ð½Ðµ ÑƒÐ±Ð¸Ñ‚ÑŒ CPU / kernel):\n",
    "# - âœ… Ñ‡Ñ‚ÐµÐ½Ð¸Ðµ row_group Ð‘Ð•Ð— pandas (pyarrow + numpy mask)\n",
    "# - âœ… Ð½ÐµÐ±Ð¾Ð»ÑŒÑˆÐ¾Ð¹ num_workers=2 + prefetch_factor=1 + persistent_workers=True\n",
    "# - âœ… LRU-ÐºÑÑˆ ParquetFile Ð²Ð½ÑƒÑ‚Ñ€Ð¸ worker\n",
    "# - âœ… cap_per_pair_in_rg=256 (ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ CPU Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸)\n",
    "# - âœ… (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾) Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð² BLAS/OMP\n",
    "# =========================================================\n",
    "\n",
    "import os, glob, math, random\n",
    "from collections import defaultdict, OrderedDict\n",
    "from datetime import datetime\n",
    "from itertools import islice\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.amp import autocast\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "import scanpy as sc\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4673acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB_SIZE(vocab-space): 62716\n",
      "N_GENES(gene-space): 62713\n",
      "NUM_CELL_LINE(from cell2id.csv): 50 | cell_emb rows: 50\n",
      "train pairs: 15118\n",
      "val pairs: 1680\n",
      "eval pairs (>=1000): 16798\n",
      "parquet files found: 3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Index parquet row-groups: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3388/3388 [07:54<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexed pairs: 16798\n",
      "baseline_global: (62713,) baseline_by_cl: 50\n",
      "sorted_gene_token_ids: (1000,) [39721 21437 21401 37295  4423  3916   455 17902  6378  4185]\n",
      "num drugs: 379\n",
      "smiles_dim: 768\n",
      "âœ… Loaded pretrained gene token_emb: 62710 genes\n",
      "âœ… Loaded pretrained cell_line_emb: (50, 256)\n",
      "âœ… f_r model ready\n",
      "[sanity] checked=20, bad=0, max_abs_diff=0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aiffel/.cache/tmp/ipykernel_3316815/2952599674.py:913: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(device.type == \"cuda\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Valid MSE (DMSO) = 20.909212\n",
      "ðŸš€ f_r training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [2:22:11<00:00,  1.17it/s, mse=6.0742, rank=0.0000, Î»_rank=0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train total=11.203143, mse=11.203143, rank=0.000000 (Î»_rank=0.0) | Valid mse=7.016501 | Baseline(DMSO) mse=20.909212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [2:14:59<00:00,  1.23it/s, mse=4.0167, rank=0.0000, Î»_rank=0]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train total=4.970266, mse=4.970266, rank=0.000000 (Î»_rank=0.0) | Valid mse=4.112769 | Baseline(DMSO) mse=20.909212\n",
      "ðŸ’¾ saved checkpoint: /data/aiffel/babayakga/checkpoints/f_r_withcellline/fr_epoch2_20251224_234816.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [2:11:19<00:00,  1.27it/s, mse=3.5937, rank=2.4922, Î»_rank=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train total=4.101466, mse=3.625382, rank=2.380414 (Î»_rank=0.2) | Valid mse=3.341106 | Baseline(DMSO) mse=20.909212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [2:05:18<00:00,  1.33it/s, mse=3.9715, rank=2.3105, Î»_rank=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train total=3.505763, mse=3.028709, rank=2.385272 (Î»_rank=0.2) | Valid mse=2.866567 | Baseline(DMSO) mse=20.909212\n",
      "ðŸ’¾ saved checkpoint: /data/aiffel/babayakga/checkpoints/f_r_withcellline/fr_epoch4_20251225_042620.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [2:04:22<00:00,  1.34it/s, mse=4.6376, rank=2.0918, Î»_rank=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train total=3.175252, mse=2.697655, rank=2.387986 (Î»_rank=0.2) | Valid mse=2.604213 | Baseline(DMSO) mse=20.909212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [1:58:09<00:00,  1.41it/s, mse=2.4020, rank=2.6074, Î»_rank=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train total=2.787942, mse=2.308350, rank=2.397962 (Î»_rank=0.2) | Valid mse=2.180845 | Baseline(DMSO) mse=20.909212\n",
      "ðŸ’¾ saved checkpoint: /data/aiffel/babayakga/checkpoints/f_r_withcellline/fr_epoch6_20251225_085916.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [2:33:13<00:00,  1.09it/s, mse=2.3121, rank=2.6582, Î»_rank=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Train total=2.566124, mse=2.085309, rank=2.404073 (Î»_rank=0.2) | Valid mse=1.992714 | Baseline(DMSO) mse=20.909212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [1:35:49<00:00,  1.74it/s, mse=2.1831, rank=2.8555, Î»_rank=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Train total=2.440751, mse=1.959138, rank=2.408071 (Î»_rank=0.2) | Valid mse=2.055772 | Baseline(DMSO) mse=20.909212\n",
      "ðŸ’¾ saved checkpoint: /data/aiffel/babayakga/checkpoints/f_r_withcellline/fr_epoch8_20251225_132613.pt\n",
      "âœ… DONE\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# (ÐžÐŸÐ¦Ð˜ÐžÐÐÐ›Ð¬ÐÐž, ÐÐž Ð Ð•ÐšÐžÐœÐ•ÐÐ”Ð£Ð®) Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ñ‡Ð¸ÑÐ»Ð¾ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²\n",
    "# Ñ‡Ñ‚Ð¾Ð±Ñ‹ CPU Ð½Ðµ \"Ð²Ð·Ð»ÐµÑ‚Ð°Ð»\" Ð¸Ð·-Ð·Ð° OpenMP/BLAS Ð¸ Ð½Ðµ Ñ€Ð¾Ð½ÑÐ» kernel\n",
    "# ---------------------------------------------------------\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"4\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"4\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"4\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"4\")\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 0) PATHS / CONFIG\n",
    "# =========================================================\n",
    "GENE_META_PATH = \"/data/aiffel/data/Tahoe-100M/metadata/gene_metadata.parquet\"\n",
    "DRUG_META_PATH = \"/data/aiffel/data/Tahoe-100M/metadata/drug_metadata.parquet\"\n",
    "COUNTS_CSV     = \"/data/aiffel/babayakga/making_data/aiffel/babayakga/making_data/tahoe_counts_per_drug_cell_line.csv\"\n",
    "\n",
    "PARQUET_DIR    = \"/data/aiffel/data/Tahoe-100M/data\"\n",
    "DMSO_H5AD      = \"/data/aiffel/babayakga/outputs/dmso.h5ad\"\n",
    "\n",
    "# pretrained embeddings\n",
    "PRETRAINED_GENE_NPY = \"/data/aiffel/babayakga/pretraining/checkpoints_with_cell/gene_embeddings.npy\"\n",
    "\n",
    "CELL_CKPT_DIR  = \"/data/aiffel/babayakga/pretraining/checkpoints_with_cell\"\n",
    "CELL2ID_CSV    = os.path.join(CELL_CKPT_DIR, \"cell2id.csv\")\n",
    "CELL_EMB_NPY   = os.path.join(CELL_CKPT_DIR, \"cell_embeddings.npy\")\n",
    "\n",
    "# smiles embedding for drugs (must match drug_metadata row order)\n",
    "SMILES_EMB_PATH = \"/data/aiffel/babayakga/f_p module/f_r/drug_smiles_emb_all.pt\"\n",
    "\n",
    "# training\n",
    "CONTROL_DRUG = \"DMSO_TF\"\n",
    "SEED = 42\n",
    "\n",
    "MIN_GENE_TOKEN_ID = 3   # âœ… exclude 0/1/2 (not real genes)\n",
    "\n",
    "TOP_K      = 1000\n",
    "MAX_LEN    = 512        # gene tokens length (not counting prefix)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "TOTAL_EPOCHS     = 8\n",
    "WARMUP_EPOCHS    = 2\n",
    "lambda_rank_main = 0.2\n",
    "\n",
    "STEPS_PER_EPOCH  = 10000\n",
    "VAL_STEPS        = 900\n",
    "\n",
    "GRAD_CLIP = 1.0\n",
    "LR = 3e-4\n",
    "\n",
    "CKPT_DIR   = \"/data/aiffel/babayakga/checkpoints/f_r_withcellline\"\n",
    "SAVE_EVERY = 2\n",
    "\n",
    "# Ñ‚Ð²Ð¾Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€ GPU\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1) VOCAB (special + ENSG) from gene_metadata\n",
    "# =========================================================\n",
    "def build_vocab_from_gene_metadata(gene_meta_path: str):\n",
    "    \"\"\"\n",
    "    vocab-space: special tokens + all ensembl_id\n",
    "    gene-space: Tahoe token_id (0..N_GENES-1)\n",
    "    \"\"\"\n",
    "    SPECIAL_TOKENS = [\"[PAD]\", \"[CLS]\", \"[DRUG]\", \"[TARGET]\", \"[CELL]\", \"[MASK]\"]\n",
    "\n",
    "    gene_md = pd.read_parquet(gene_meta_path).copy()\n",
    "    gene_md[\"ensembl_id\"] = gene_md[\"ensembl_id\"].astype(str)\n",
    "    gene_md[\"token_id\"]   = gene_md[\"token_id\"].astype(int)\n",
    "    gene_md = gene_md.sort_values(\"token_id\").reset_index(drop=True)\n",
    "\n",
    "    N_GENES = int(gene_md[\"token_id\"].max()) + 1\n",
    "\n",
    "    local_token_to_id = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
    "    for ensg in gene_md[\"ensembl_id\"].tolist():\n",
    "        if ensg not in local_token_to_id:\n",
    "            local_token_to_id[ensg] = len(local_token_to_id)\n",
    "\n",
    "    token_id_to_vocab_id = {\n",
    "        int(tid): int(local_token_to_id[str(ensg)])\n",
    "        for tid, ensg in zip(gene_md[\"token_id\"].values, gene_md[\"ensembl_id\"].values)\n",
    "    }\n",
    "\n",
    "    ensg_to_token_id = {\n",
    "        str(ensg): int(tid)\n",
    "        for ensg, tid in zip(gene_md[\"ensembl_id\"].values, gene_md[\"token_id\"].values)\n",
    "    }\n",
    "\n",
    "    PAD_ID  = local_token_to_id[\"[PAD]\"]\n",
    "    CLS_ID  = local_token_to_id[\"[CLS]\"]\n",
    "    DRUG_ID = local_token_to_id[\"[DRUG]\"]\n",
    "    CELL_ID = local_token_to_id[\"[CELL]\"]\n",
    "\n",
    "    return local_token_to_id, token_id_to_vocab_id, ensg_to_token_id, N_GENES, SPECIAL_TOKENS, PAD_ID, CLS_ID, DRUG_ID, CELL_ID\n",
    "\n",
    "\n",
    "local_token_to_id, token_id_to_vocab_id, ensg_to_token_id, N_GENES, SPECIAL_TOKENS, PAD_ID, CLS_ID, DRUGTOK_ID, CELLTOK_ID = \\\n",
    "    build_vocab_from_gene_metadata(GENE_META_PATH)\n",
    "\n",
    "VOCAB_SIZE = len(local_token_to_id)\n",
    "print(\"VOCAB_SIZE(vocab-space):\", VOCAB_SIZE)\n",
    "print(\"N_GENES(gene-space):\", N_GENES)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) LOAD cell2id mapping (MUST match pretrained cell embeddings)\n",
    "# =========================================================\n",
    "if not os.path.exists(CELL2ID_CSV):\n",
    "    raise FileNotFoundError(f\"cell2id.csv not found: {CELL2ID_CSV}\")\n",
    "if not os.path.exists(CELL_EMB_NPY):\n",
    "    raise FileNotFoundError(f\"cell_embeddings.npy not found: {CELL_EMB_NPY}\")\n",
    "\n",
    "cell2id_df = pd.read_csv(CELL2ID_CSV)\n",
    "cell2id_df[\"cell_line_id\"] = cell2id_df[\"cell_line_id\"].astype(str)\n",
    "cell_line2id = {c: int(i) for c, i in zip(cell2id_df[\"cell_line_id\"], cell2id_df[\"cell_id\"])}\n",
    "\n",
    "NUM_CELL_LINE = len(cell_line2id)\n",
    "W_cell = np.load(CELL_EMB_NPY)\n",
    "print(\"NUM_CELL_LINE(from cell2id.csv):\", NUM_CELL_LINE, \"| cell_emb rows:\", W_cell.shape[0])\n",
    "\n",
    "assert W_cell.shape[0] == NUM_CELL_LINE, f\"cell2id size {NUM_CELL_LINE} != cell_emb rows {W_cell.shape[0]}\"\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3) SPLIT PAIRS (drug, cell_line) from COUNTS_CSV\n",
    "# =========================================================\n",
    "DRUG_COL, CELL_COL, N_COL = \"drug\", \"cell_line_id\", \"n_cells\"\n",
    "\n",
    "counts = pd.read_csv(COUNTS_CSV)\n",
    "counts[DRUG_COL] = counts[DRUG_COL].astype(str)\n",
    "counts[CELL_COL] = counts[CELL_COL].astype(str)\n",
    "\n",
    "MIN_TRAIN = 1000\n",
    "MIN_EVAL  = 1000\n",
    "\n",
    "train_pool = counts[counts[N_COL] >= MIN_TRAIN].copy()\n",
    "eval_pool  = counts[counts[N_COL] >= MIN_EVAL].copy()\n",
    "\n",
    "pairs_df = train_pool[[DRUG_COL, CELL_COL]].drop_duplicates()\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    pairs_df,\n",
    "    test_size=0.1,\n",
    "    random_state=SEED,\n",
    "    stratify=pairs_df[DRUG_COL] if len(pairs_df) else None,\n",
    ")\n",
    "\n",
    "train_df = train_df[train_df[DRUG_COL] != CONTROL_DRUG]\n",
    "val_df   = val_df[val_df[DRUG_COL]   != CONTROL_DRUG]\n",
    "\n",
    "train_pairs = list(zip(train_df[DRUG_COL], train_df[CELL_COL]))\n",
    "val_pairs   = list(zip(val_df[DRUG_COL],   val_df[CELL_COL]))\n",
    "\n",
    "eval_pairs_df = eval_pool[[DRUG_COL, CELL_COL]].drop_duplicates()\n",
    "eval_pairs_df = eval_pairs_df[eval_pairs_df[DRUG_COL] != CONTROL_DRUG]\n",
    "eval_pairs = list(zip(eval_pairs_df[DRUG_COL], eval_pairs_df[CELL_COL]))\n",
    "\n",
    "print(\"train pairs:\", len(train_pairs))\n",
    "print(\"val pairs:\", len(val_pairs))\n",
    "print(f\"eval pairs (>={MIN_EVAL}):\", len(eval_pairs))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4) INDEX PARQUET row-groups for valid pairs\n",
    "# =========================================================\n",
    "PARQUET_FILES = sorted(glob.glob(os.path.join(PARQUET_DIR, \"**\", \"*.parquet\"), recursive=True))\n",
    "print(\"parquet files found:\", len(PARQUET_FILES))\n",
    "\n",
    "PARQUET_DRUG_COL = \"drug\"\n",
    "PARQUET_CELL_COL = \"cell_line_id\"\n",
    "\n",
    "def build_pair_to_locations(parquet_files, valid_pairs_set, drug_col, cell_col):\n",
    "    out = defaultdict(list)\n",
    "    for f in tqdm(parquet_files, desc=\"Index parquet row-groups\"):\n",
    "        try:\n",
    "            pf = pq.ParquetFile(f)\n",
    "        except Exception:\n",
    "            continue\n",
    "        for rg in range(pf.num_row_groups):\n",
    "            try:\n",
    "                tbl = pf.read_row_group(rg, columns=[drug_col, cell_col])\n",
    "                # Ð’ÐÐ˜ÐœÐÐÐ˜Ð•: Ñ‚ÑƒÑ‚ pandas ÐžÐš, Ð¿Ð¾Ñ‚Ð¾Ð¼Ñƒ Ñ‡Ñ‚Ð¾ ÑÑ‚Ð¾ Ñ€Ð°Ð·Ð¾Ð²Ð°Ñ Ð¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ñ (Ð¾Ð´Ð¸Ð½ Ñ€Ð°Ð·)\n",
    "                df = tbl.to_pandas()\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            pairs_here = set(zip(df[drug_col].astype(str), df[cell_col].astype(str)))\n",
    "            inter = pairs_here.intersection(valid_pairs_set)\n",
    "            for p in inter:\n",
    "                out[p].append((f, rg))\n",
    "    return dict(out)\n",
    "\n",
    "valid_pairs_set = set(train_pairs) | set(val_pairs)\n",
    "pair_to_locations = build_pair_to_locations(\n",
    "    parquet_files=PARQUET_FILES,\n",
    "    valid_pairs_set=valid_pairs_set,\n",
    "    drug_col=PARQUET_DRUG_COL,\n",
    "    cell_col=PARQUET_CELL_COL\n",
    ")\n",
    "print(\"indexed pairs:\", len(pair_to_locations))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5) DMSO baselines + topK variance genes (gene-space token_id)\n",
    "#    âœ… filter token_id < 3 here too\n",
    "# =========================================================\n",
    "def build_dmso_baselines_gene_space(dmso_h5ad_path: str, control_drug: str, N_GENES: int, ensg_to_token_id: dict,\n",
    "                                    drug_col=\"drug\", cell_col=\"cell_line_id\", dtype=np.float32):\n",
    "    adata = sc.read_h5ad(dmso_h5ad_path)\n",
    "    obs = adata.obs\n",
    "    X = adata.X.tocsr() if sparse.issparse(adata.X) else sparse.csr_matrix(adata.X)\n",
    "\n",
    "    m = (obs[drug_col].astype(str).values == control_drug)\n",
    "    idx_ctrl = np.where(m)[0]\n",
    "    if idx_ctrl.size == 0:\n",
    "        raise ValueError(f\"No control rows: {control_drug}\")\n",
    "\n",
    "    ensgs = adata.var_names.astype(str).tolist()\n",
    "    token_ids, cols = [], []\n",
    "    for j, ensg in enumerate(ensgs):\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        tid = int(tid)\n",
    "        if tid < MIN_GENE_TOKEN_ID:\n",
    "            continue\n",
    "        token_ids.append(tid)\n",
    "        cols.append(j)\n",
    "\n",
    "    token_ids = np.asarray(token_ids, dtype=np.int64)\n",
    "    cols      = np.asarray(cols, dtype=np.int64)\n",
    "\n",
    "    Xc = X[idx_ctrl][:, cols]\n",
    "    mean_global_sub = np.asarray(Xc.mean(axis=0)).ravel().astype(dtype)\n",
    "\n",
    "    baseline_global = np.zeros(N_GENES, dtype=dtype)\n",
    "    baseline_global[token_ids] = mean_global_sub\n",
    "\n",
    "    baseline_by_cl = {}\n",
    "    cls_all = obs[cell_col].astype(str).values\n",
    "    for cl in np.unique(cls_all):\n",
    "        cl_idx = np.where(m & (cls_all == cl))[0]\n",
    "        if cl_idx.size == 0:\n",
    "            continue\n",
    "        Xcl = X[cl_idx][:, cols]\n",
    "        mean_cl_sub = np.asarray(Xcl.mean(axis=0)).ravel().astype(dtype)\n",
    "        v = np.zeros(N_GENES, dtype=dtype)\n",
    "        v[token_ids] = mean_cl_sub\n",
    "        baseline_by_cl[str(cl)] = v\n",
    "\n",
    "    return baseline_global, baseline_by_cl\n",
    "\n",
    "\n",
    "def topk_by_variance_gene_space(dmso_h5ad_path: str, control_drug: str, N_GENES: int, ensg_to_token_id: dict,\n",
    "                               drug_col=\"drug\", top_k=1000):\n",
    "    adata = sc.read_h5ad(dmso_h5ad_path)\n",
    "    obs = adata.obs\n",
    "    X = adata.X.tocsr() if sparse.issparse(adata.X) else sparse.csr_matrix(adata.X)\n",
    "\n",
    "    m = (obs[drug_col].astype(str).values == control_drug)\n",
    "    idx = np.where(m)[0]\n",
    "    if idx.size == 0:\n",
    "        raise ValueError(f\"No control rows: {control_drug}\")\n",
    "\n",
    "    ensgs = adata.var_names.astype(str).tolist()\n",
    "    token_ids, cols = [], []\n",
    "    for j, ensg in enumerate(ensgs):\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        tid = int(tid)\n",
    "        if tid < MIN_GENE_TOKEN_ID:\n",
    "            continue\n",
    "        token_ids.append(tid)\n",
    "        cols.append(j)\n",
    "\n",
    "    token_ids = np.asarray(token_ids, dtype=np.int64)\n",
    "    cols      = np.asarray(cols, dtype=np.int64)\n",
    "\n",
    "    Xc = X[idx][:, cols]\n",
    "    ex  = np.asarray(Xc.mean(axis=0)).ravel()\n",
    "    ex2 = np.asarray(Xc.power(2).mean(axis=0)).ravel()\n",
    "    var = ex2 - ex**2\n",
    "\n",
    "    top_local = np.argsort(-var)[:top_k]\n",
    "    top_gene_token_ids = token_ids[top_local]\n",
    "    return top_gene_token_ids.astype(np.int64)\n",
    "\n",
    "\n",
    "baseline_global, baseline_by_cl = build_dmso_baselines_gene_space(\n",
    "    dmso_h5ad_path=DMSO_H5AD,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    N_GENES=N_GENES,\n",
    "    ensg_to_token_id=ensg_to_token_id,\n",
    ")\n",
    "\n",
    "sorted_gene_token_ids = topk_by_variance_gene_space(\n",
    "    dmso_h5ad_path=DMSO_H5AD,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    N_GENES=N_GENES,\n",
    "    ensg_to_token_id=ensg_to_token_id,\n",
    "    top_k=TOP_K,\n",
    ")\n",
    "\n",
    "assert (sorted_gene_token_ids >= MIN_GENE_TOKEN_ID).all(), \"TOP_K contains token_id < 3 !\"\n",
    "\n",
    "print(\"baseline_global:\", baseline_global.shape, \"baseline_by_cl:\", len(baseline_by_cl))\n",
    "print(\"sorted_gene_token_ids:\", sorted_gene_token_ids.shape, sorted_gene_token_ids[:10])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6) DRUG -> id + SMILES embeddings\n",
    "# =========================================================\n",
    "drug_meta_df = pd.read_parquet(DRUG_META_PATH).copy()\n",
    "drug_meta_df[\"drug\"] = drug_meta_df[\"drug\"].astype(str)\n",
    "drugs = drug_meta_df[\"drug\"].tolist()\n",
    "drug2id = {d: i for i, d in enumerate(drugs)}\n",
    "print(\"num drugs:\", len(drug2id))\n",
    "\n",
    "smiles_tensor = torch.load(SMILES_EMB_PATH, map_location=\"cpu\").to(torch.float32)\n",
    "assert smiles_tensor.shape[0] == len(drug_meta_df), \"SMILES rows != drug_metadata rows\"\n",
    "\n",
    "drug_to_smiles_emb = {d: smiles_tensor[i] for i, d in enumerate(drugs)}\n",
    "smiles_dim = int(smiles_tensor.shape[-1])\n",
    "print(\"smiles_dim:\", smiles_dim)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7) pair weights\n",
    "# =========================================================\n",
    "def make_pair_weights_from_counts(counts_df, pairs, drug_col=\"drug\", cell_col=\"cell_line_id\", n_col=\"n_cells\",\n",
    "                                  mode=\"inv_sqrt\", eps=1.0):\n",
    "    tmp = counts_df[[drug_col, cell_col, n_col]].copy()\n",
    "    tmp[drug_col] = tmp[drug_col].astype(str)\n",
    "    tmp[cell_col] = tmp[cell_col].astype(str)\n",
    "\n",
    "    pair2n = {(d, c): int(n) for d, c, n in tmp.values}\n",
    "\n",
    "    w = []\n",
    "    for p in pairs:\n",
    "        n = pair2n.get(p, 0)\n",
    "        if mode == \"inv\":\n",
    "            ww = 1.0 / (n + eps)\n",
    "        elif mode == \"inv_log\":\n",
    "            ww = 1.0 / np.log1p(n + eps)\n",
    "        else:\n",
    "            ww = 1.0 / np.sqrt(n + eps)\n",
    "        w.append(float(ww))\n",
    "\n",
    "    w = np.asarray(w, dtype=np.float64)\n",
    "    w = np.clip(w, 0.0, None)\n",
    "    w = w / (w.sum() + 1e-12)\n",
    "    pair2w = {p: float(wi) for p, wi in zip(pairs, w)}\n",
    "    return w, pair2w\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8) DATASET (Iterable, aligned)  âœ… SAFE-FAST (NO PANDAS)\n",
    "# =========================================================\n",
    "class _PFCache:\n",
    "    \"\"\"ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ LRU-ÐºÑÑˆ ParquetFile Ð²Ð½ÑƒÑ‚Ñ€Ð¸ worker (Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ Ð¿Ð¾ Ð¿Ð°Ð¼ÑÑ‚Ð¸).\"\"\"\n",
    "    def __init__(self, max_items=32):\n",
    "        self.max_items = int(max_items)\n",
    "        self.cache = OrderedDict()\n",
    "\n",
    "    def get(self, path: str) -> pq.ParquetFile:\n",
    "        pf = self.cache.get(path, None)\n",
    "        if pf is not None:\n",
    "            self.cache.move_to_end(path)\n",
    "            return pf\n",
    "        pf = pq.ParquetFile(path)\n",
    "        self.cache[path] = pf\n",
    "        if len(self.cache) > self.max_items:\n",
    "            self.cache.popitem(last=False)\n",
    "        return pf\n",
    "\n",
    "\n",
    "class FRSeqExpressionParquetDatasetAligned(IterableDataset):\n",
    "    \"\"\"\n",
    "    input_ids: [CLS][DRUG][CELL] + gene tokens (vocab-space ids)\n",
    "    values:    delta (val - baseline[cell or global]) aligned with tokens\n",
    "    y_topk:    true expression on TOP_K genes (gene-space token_id list)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pair_to_locations,\n",
    "        pairs,\n",
    "        token_id_to_vocab_id,\n",
    "        sorted_gene_token_ids,\n",
    "        baseline_global,\n",
    "        baseline_by_cellline,\n",
    "        cell_line2id,\n",
    "        drug2id,\n",
    "        drug_to_smiles_emb,\n",
    "        pair_weights=None,\n",
    "        seed=42,\n",
    "        max_gene_len=512,\n",
    "        top_k=1000,\n",
    "        batch_size=16,\n",
    "        pad_id=0,\n",
    "        cls_id=1,\n",
    "        drugtok_id=2,\n",
    "        celltok_id=4,\n",
    "        drug_col=\"drug\",\n",
    "        cell_col=\"cell_line_id\",\n",
    "        genes_col=\"genes\",\n",
    "        expr_col=\"expressions\",\n",
    "        cap_per_pair_in_rg=256,       # âœ… Ð²Ð°Ð¶Ð½Ñ‹Ð¹ \"Ð¿Ñ€ÐµÐ´Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÐµÐ»ÑŒ\"\n",
    "        max_tries=30,\n",
    "        shuffle=False,\n",
    "        pf_cache_size=32,             # âœ… Ñ€Ð°Ð·Ð¼ÐµÑ€ LRU ÐºÑÑˆÐ° Ñ„Ð°Ð¹Ð»Ð¾Ð²\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pair_to_locations = pair_to_locations\n",
    "        self.pairs = list(pairs)\n",
    "\n",
    "        self.token_id_to_vocab_id = token_id_to_vocab_id\n",
    "        self.q = np.asarray(sorted_gene_token_ids, dtype=np.int64)\n",
    "\n",
    "        self.baseline_global = np.asarray(baseline_global, dtype=np.float32)\n",
    "        self.baseline_by_cellline = baseline_by_cellline or {}\n",
    "\n",
    "        self.cell_line2id = cell_line2id\n",
    "        self.drug2id = drug2id\n",
    "        self.drug_to_smiles_emb = drug_to_smiles_emb\n",
    "\n",
    "        self.max_gene_len = int(max_gene_len)\n",
    "        self.top_k = int(top_k)\n",
    "        self.batch_size = int(batch_size)\n",
    "\n",
    "        self.pad_id = int(pad_id)\n",
    "        self.cls_id = int(cls_id)\n",
    "        self.drugtok_id = int(drugtok_id)\n",
    "        self.celltok_id = int(celltok_id)\n",
    "\n",
    "        self.drug_col = drug_col\n",
    "        self.cell_col = cell_col\n",
    "        self.genes_col = genes_col\n",
    "        self.expr_col = expr_col\n",
    "\n",
    "        self.cap_per_pair_in_rg = int(cap_per_pair_in_rg) if cap_per_pair_in_rg is not None else None\n",
    "        self.max_tries = int(max_tries)\n",
    "        self.shuffle = bool(shuffle)\n",
    "\n",
    "        self.num_prefix = 3  # âœ… [CLS][DRUG][CELL]\n",
    "        self.seq_len = self.num_prefix + self.max_gene_len\n",
    "\n",
    "        any_vec = next(iter(self.drug_to_smiles_emb.values()))\n",
    "        self.smiles_dim = int(any_vec.shape[-1])\n",
    "\n",
    "        self.seed = int(seed)\n",
    "        self.pf_cache_size = int(pf_cache_size)\n",
    "\n",
    "        # weights\n",
    "        if pair_weights is None:\n",
    "            self.pair_weights = None\n",
    "        elif isinstance(pair_weights, dict):\n",
    "            w = np.asarray([pair_weights.get(p, 0.0) for p in self.pairs], dtype=np.float64)\n",
    "            w = np.clip(w, 0.0, None)\n",
    "            w = w / (w.sum() + 1e-12)\n",
    "            self.pair_weights = w\n",
    "        else:\n",
    "            w = np.asarray(pair_weights, dtype=np.float64)\n",
    "            assert len(w) == len(self.pairs), \"pair_weights length must match pairs length\"\n",
    "            w = np.clip(w, 0.0, None)\n",
    "            w = w / (w.sum() + 1e-12)\n",
    "            self.pair_weights = w\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_numpy_str(chunked_arr):\n",
    "        # zero_copy_only=False Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½ÐµÐµ (Ð¸Ð½Ð°Ñ‡Ðµ Ð¸Ð½Ð¾Ð³Ð´Ð° Ð¿Ð°Ð´Ð°ÐµÑ‚ Ð½Ð° chunked)\n",
    "        return chunked_arr.combine_chunks().to_numpy(zero_copy_only=False).astype(str)\n",
    "\n",
    "    def _prepare_sparse(self, genes, expr):\n",
    "        idx = np.asarray(genes, dtype=np.int64)\n",
    "        val = np.asarray(expr, dtype=np.float32)\n",
    "        if idx.size == 0:\n",
    "            return idx, val\n",
    "\n",
    "        keep = idx >= MIN_GENE_TOKEN_ID\n",
    "        idx = idx[keep]\n",
    "        val = val[keep]\n",
    "        if idx.size == 0:\n",
    "            return idx, val\n",
    "\n",
    "        order = np.argsort(idx)\n",
    "        return idx[order], val[order]\n",
    "\n",
    "    def _make_y_true_topk(self, idx_sorted, val_sorted):\n",
    "        q = self.q\n",
    "        if idx_sorted.size == 0:\n",
    "            return np.zeros(q.shape[0], dtype=np.float32)\n",
    "\n",
    "        pos = np.searchsorted(idx_sorted, q)\n",
    "        y = np.zeros(q.shape[0], dtype=np.float32)\n",
    "\n",
    "        in_bounds = (pos < idx_sorted.size)\n",
    "        match = np.zeros(q.shape[0], dtype=bool)\n",
    "        match[in_bounds] = (idx_sorted[pos[in_bounds]] == q[in_bounds])\n",
    "        ok = in_bounds & match\n",
    "        y[ok] = val_sorted[pos[ok]]\n",
    "        return y.astype(np.float32)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        wid = 0 if worker_info is None else int(worker_info.id)\n",
    "        rng = np.random.default_rng(self.seed + 1337 * wid)\n",
    "\n",
    "        pairs = self.pairs\n",
    "        weights = self.pair_weights\n",
    "\n",
    "        cols = [self.drug_col, self.cell_col, self.genes_col, self.expr_col]\n",
    "\n",
    "        # âœ… ÐºÑÑˆ ParquetFile Ð²Ð½ÑƒÑ‚Ñ€Ð¸ worker\n",
    "        pf_cache = _PFCache(max_items=self.pf_cache_size)\n",
    "\n",
    "        while True:\n",
    "            # weighted pair sampling\n",
    "            if weights is None:\n",
    "                drug_name, cell_line = pairs[rng.integers(0, len(pairs))]\n",
    "            else:\n",
    "                idxp = rng.choice(len(pairs), p=weights)\n",
    "                drug_name, cell_line = pairs[idxp]\n",
    "\n",
    "            locs = self.pair_to_locations.get((drug_name, cell_line), [])\n",
    "            if not locs:\n",
    "                continue\n",
    "\n",
    "            # baseline (cell-specific if exists)\n",
    "            baseline = self.baseline_by_cellline.get(cell_line, self.baseline_global)\n",
    "\n",
    "            # cell id must exist in pretrained mapping\n",
    "            if cell_line not in self.cell_line2id:\n",
    "                continue\n",
    "            cell_id = self.cell_line2id[cell_line]\n",
    "\n",
    "            drug_id = self.drug2id.get(drug_name, 0)\n",
    "\n",
    "            sm = self.drug_to_smiles_emb.get(drug_name, None)\n",
    "            if sm is None:\n",
    "                smiles_emb = torch.zeros(self.smiles_dim, dtype=torch.float32)\n",
    "            else:\n",
    "                smiles_emb = sm.detach().to(torch.float32) if isinstance(sm, torch.Tensor) \\\n",
    "                    else torch.tensor(sm, dtype=torch.float32)\n",
    "\n",
    "            for _ in range(self.max_tries):\n",
    "                fpath, rg_id = locs[rng.integers(0, len(locs))]\n",
    "\n",
    "                try:\n",
    "                    pf = pf_cache.get(fpath)\n",
    "                    table = pf.read_row_group(rg_id, columns=cols)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                # --- Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ Ð¿Ð¾ (drug, cell) Ð±ÐµÐ· pandas ---\n",
    "                try:\n",
    "                    drug_arr = self._to_numpy_str(table[self.drug_col])\n",
    "                    cell_arr = self._to_numpy_str(table[self.cell_col])\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                mask_pair = (drug_arr == str(drug_name)) & (cell_arr == str(cell_line))\n",
    "                idxs = np.where(mask_pair)[0]\n",
    "                if idxs.size < self.batch_size:\n",
    "                    continue\n",
    "\n",
    "                # âœ… cap Ð´Ð»Ñ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ñ CPU (Ð½Ðµ Ð´Ð°Ñ‘Ð¼ Ð¾Ð³Ñ€Ð¾Ð¼Ð½Ñ‹Ð¼ row_group Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð½Ð°Ñ)\n",
    "                if self.cap_per_pair_in_rg is not None and idxs.size > self.cap_per_pair_in_rg:\n",
    "                    idxs = rng.choice(idxs, size=self.cap_per_pair_in_rg, replace=False)\n",
    "\n",
    "                if idxs.size < self.batch_size:\n",
    "                    continue\n",
    "\n",
    "                choose = rng.choice(idxs, size=self.batch_size, replace=False)\n",
    "\n",
    "                # ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ genes/expr (ChunkedArray)\n",
    "                genes_col = table[self.genes_col].combine_chunks()\n",
    "                exprs_col = table[self.expr_col].combine_chunks()\n",
    "\n",
    "                # allocate batch arrays\n",
    "                input_ids = np.full((self.batch_size, self.seq_len), self.pad_id, dtype=np.int64)\n",
    "                values    = np.zeros((self.batch_size, self.seq_len), dtype=np.float32)\n",
    "                mask      = np.zeros((self.batch_size, self.seq_len), dtype=np.int64)\n",
    "                y_topk    = np.zeros((self.batch_size, self.top_k), dtype=np.float32)\n",
    "\n",
    "                cell_batch   = np.full((self.batch_size,), cell_id, dtype=np.int64)\n",
    "                drug_batch   = np.full((self.batch_size,), drug_id, dtype=np.int64)\n",
    "\n",
    "                # smiles -> numpy (Ð¾Ð´Ð¸Ð½ Ñ€Ð°Ð·)\n",
    "                sm_np = smiles_emb.detach().cpu().numpy() if isinstance(smiles_emb, torch.Tensor) else np.asarray(smiles_emb, dtype=np.float32)\n",
    "                smiles_batch = np.repeat(sm_np[None, :], repeats=self.batch_size, axis=0).astype(np.float32)\n",
    "\n",
    "                # prefix: [CLS][DRUG][CELL]\n",
    "                input_ids[:, 0] = self.cls_id\n",
    "                input_ids[:, 1] = self.drugtok_id\n",
    "                input_ids[:, 2] = self.celltok_id\n",
    "                mask[:, :self.num_prefix] = 1\n",
    "\n",
    "                # build each sample\n",
    "                for b, j in enumerate(choose):\n",
    "                    try:\n",
    "                        genes = genes_col[int(j)].as_py()\n",
    "                        expr  = exprs_col[int(j)].as_py()\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                    idx, val = self._prepare_sparse(genes, expr)\n",
    "\n",
    "                    # y_true on TOP_K\n",
    "                    y_topk[b] = self._make_y_true_topk(idx, val)\n",
    "\n",
    "                    if idx.size == 0:\n",
    "                        continue\n",
    "\n",
    "                    # delta\n",
    "                    base_vals = baseline[idx]\n",
    "                    delta = (val - base_vals)\n",
    "\n",
    "                    # choose top genes by |delta|\n",
    "                    k = min(self.max_gene_len, idx.size)\n",
    "                    if k <= 0:\n",
    "                        continue\n",
    "\n",
    "                    if k == idx.size:\n",
    "                        top_pos = np.argsort(-np.abs(delta))\n",
    "                    else:\n",
    "                        top_pos = np.argpartition(-np.abs(delta), k - 1)[:k]\n",
    "                        top_pos = top_pos[np.argsort(-np.abs(delta[top_pos]))]\n",
    "\n",
    "                    sel_gene_token_ids = idx[top_pos]\n",
    "                    sel_delta = delta[top_pos]\n",
    "\n",
    "                    # token_id -> vocab_id (drop missing)\n",
    "                    sel_vocab_ids = np.asarray(\n",
    "                        [self.token_id_to_vocab_id.get(int(t), -1) for t in sel_gene_token_ids],\n",
    "                        dtype=np.int64\n",
    "                    )\n",
    "                    ok = sel_vocab_ids != -1\n",
    "                    sel_vocab_ids = sel_vocab_ids[ok]\n",
    "                    sel_delta = sel_delta[ok]\n",
    "\n",
    "                    L = min(self.max_gene_len, sel_vocab_ids.size)\n",
    "                    if L <= 0:\n",
    "                        continue\n",
    "\n",
    "                    start = self.num_prefix\n",
    "                    input_ids[b, start:start+L] = sel_vocab_ids[:L]\n",
    "                    values[b,    start:start+L] = sel_delta[:L]\n",
    "                    mask[b,      start:start+L] = 1\n",
    "\n",
    "                yield (\n",
    "                    torch.tensor(input_ids, dtype=torch.long),\n",
    "                    torch.tensor(values, dtype=torch.float32),\n",
    "                    torch.tensor(mask, dtype=torch.long),\n",
    "                    torch.tensor(y_topk, dtype=torch.float32),\n",
    "                    torch.tensor(cell_batch, dtype=torch.long),\n",
    "                    torch.tensor(drug_batch, dtype=torch.long),\n",
    "                    torch.tensor(smiles_batch, dtype=torch.float32),\n",
    "                )\n",
    "                break\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9) DataLoaders (SAFE)\n",
    "# =========================================================\n",
    "train_w, _ = make_pair_weights_from_counts(counts, train_pairs, mode=\"inv_sqrt\")\n",
    "\n",
    "train_ds = FRSeqExpressionParquetDatasetAligned(\n",
    "    pair_to_locations=pair_to_locations,\n",
    "    pairs=train_pairs,\n",
    "    pair_weights=train_w,\n",
    "    token_id_to_vocab_id=token_id_to_vocab_id,\n",
    "    sorted_gene_token_ids=sorted_gene_token_ids,\n",
    "    baseline_global=baseline_global,\n",
    "    baseline_by_cellline=baseline_by_cl,\n",
    "    cell_line2id=cell_line2id,\n",
    "    drug2id=drug2id,\n",
    "    drug_to_smiles_emb=drug_to_smiles_emb,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_gene_len=MAX_LEN,\n",
    "    top_k=TOP_K,\n",
    "    shuffle=False,\n",
    "    cap_per_pair_in_rg=256,     # âœ… safety knob\n",
    "    pf_cache_size=32,\n",
    ")\n",
    "\n",
    "val_ds = FRSeqExpressionParquetDatasetAligned(\n",
    "    pair_to_locations=pair_to_locations,\n",
    "    pairs=val_pairs,\n",
    "    pair_weights=None,\n",
    "    token_id_to_vocab_id=token_id_to_vocab_id,\n",
    "    sorted_gene_token_ids=sorted_gene_token_ids,\n",
    "    baseline_global=baseline_global,\n",
    "    baseline_by_cellline=baseline_by_cl,\n",
    "    cell_line2id=cell_line2id,\n",
    "    drug2id=drug2id,\n",
    "    drug_to_smiles_emb=drug_to_smiles_emb,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_gene_len=MAX_LEN,\n",
    "    top_k=TOP_K,\n",
    "    shuffle=False,\n",
    "    cap_per_pair_in_rg=256,\n",
    "    pf_cache_size=32,\n",
    ")\n",
    "\n",
    "# âœ… Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹: Ð½ÐµÐ¼Ð½Ð¾Ð³Ð¾ Ð²Ð¾Ñ€ÐºÐµÑ€Ð¾Ð² + Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ð¹ prefetch\n",
    "NUM_WORKERS = 2\n",
    "PREFETCH = 1\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=None,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    prefetch_factor=PREFETCH,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=None,\n",
    "    num_workers=1,\n",
    "    prefetch_factor=1,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10) MODEL (Cell2Sentence-like Encoder for f_r)\n",
    "# =========================================================\n",
    "class Cell2SentenceEncoderFR(nn.Module):\n",
    "    \"\"\"\n",
    "    prefix: [CLS][DRUG][CELL] + gene tokens\n",
    "    - token_emb: vocab-space ids\n",
    "    - values: delta\n",
    "    - inject smiles into position 1 ([DRUG])\n",
    "    - inject cell_line embedding into position 2 ([CELL]) âœ… pretrained\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, n_heads, num_layers, max_len_with_prefix, smiles_dim, num_cell_lines, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n",
    "        self.value_proj = nn.Sequential(\n",
    "            nn.Linear(1, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "        )\n",
    "        self.pos_emb = nn.Embedding(max_len_with_prefix, d_model)\n",
    "\n",
    "        self.cell_line_emb = nn.Embedding(num_cell_lines, d_model)\n",
    "        self.smiles_proj = nn.Linear(smiles_dim, d_model)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads,\n",
    "            dim_feedforward=4*d_model,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, input_ids, values, attention_mask, cell_line_id, smiles_emb):\n",
    "        B, L = input_ids.shape\n",
    "        device_ = input_ids.device\n",
    "\n",
    "        x = self.token_emb(input_ids) + self.value_proj(values.unsqueeze(-1))\n",
    "\n",
    "        pos = torch.arange(L, device=device_).unsqueeze(0).expand(B, L)\n",
    "        x = x + self.pos_emb(pos)\n",
    "\n",
    "        # inject drug / cell info\n",
    "        x[:, 1, :] = x[:, 1, :] + self.smiles_proj(smiles_emb.to(device=device_, dtype=torch.float32)).to(x.dtype)\n",
    "        x[:, 2, :] = x[:, 2, :] + self.cell_line_emb(cell_line_id.to(device=device_)).to(x.dtype)\n",
    "\n",
    "        key_padding_mask = (attention_mask == 0)\n",
    "        h = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        return h[:, 0, :]  # CLS\n",
    "\n",
    "\n",
    "class FRModelExpression(nn.Module):\n",
    "    def __init__(self, encoder, d_model, out_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = nn.Linear(d_model, out_dim)\n",
    "\n",
    "    def forward(self, input_ids, values, mask, cell_line_id, smiles_emb):\n",
    "        h = self.encoder(input_ids, values, mask, cell_line_id, smiles_emb)\n",
    "        return self.head(h)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 11) Load pretrained gene + cell embeddings\n",
    "# =========================================================\n",
    "def load_pretrained_token_emb_from_gene_metadata(token_emb: nn.Embedding, npy_path: str, gene_meta_path: str, local_token_to_id: dict, device):\n",
    "    W = np.load(npy_path)  # (N_genes, d_model)\n",
    "    Wt = torch.tensor(W, dtype=torch.float32, device=device)\n",
    "\n",
    "    gene_md = pd.read_parquet(gene_meta_path).copy()\n",
    "    gene_md[\"ensembl_id\"] = gene_md[\"ensembl_id\"].astype(str)\n",
    "    gene_md[\"token_id\"] = gene_md[\"token_id\"].astype(int)\n",
    "    gene_md = gene_md.sort_values(\"token_id\").reset_index(drop=True)\n",
    "\n",
    "    if Wt.shape[1] != token_emb.weight.shape[1]:\n",
    "        raise ValueError(f\"d mismatch: npy d={Wt.shape[1]} vs token_emb d={token_emb.weight.shape[1]}\")\n",
    "\n",
    "    n = min(len(gene_md), Wt.shape[0])\n",
    "    loaded = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            ensg = gene_md.loc[i, \"ensembl_id\"]\n",
    "            vid = local_token_to_id.get(ensg, None)\n",
    "            if vid is None:\n",
    "                continue\n",
    "            token_emb.weight[vid].copy_(Wt[i])\n",
    "            loaded += 1\n",
    "    print(f\"âœ… Loaded pretrained gene token_emb: {loaded} genes\")\n",
    "\n",
    "\n",
    "def load_pretrained_cell_emb(cell_emb: nn.Embedding, cell_emb_npy: str, device):\n",
    "    W = np.load(cell_emb_npy)  # (num_cell_lines, d_model)\n",
    "    Wt = torch.tensor(W, dtype=torch.float32, device=device)\n",
    "\n",
    "    if Wt.shape != cell_emb.weight.shape:\n",
    "        raise ValueError(f\"cell_emb shape mismatch: npy={tuple(Wt.shape)} vs emb={tuple(cell_emb.weight.shape)}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cell_emb.weight.copy_(Wt)\n",
    "    print(f\"âœ… Loaded pretrained cell_line_emb: {tuple(Wt.shape)}\")\n",
    "\n",
    "\n",
    "def sanity_check_gene_emb_mapping(\n",
    "    gene_meta_path,\n",
    "    local_token_to_id,\n",
    "    token_emb: torch.nn.Embedding,\n",
    "    pretrained_gene_npy,\n",
    "    n_check=20,\n",
    "    seed=0,\n",
    "):\n",
    "    gene_md = pd.read_parquet(gene_meta_path).copy()\n",
    "    gene_md[\"ensembl_id\"] = gene_md[\"ensembl_id\"].astype(str)\n",
    "    gene_md[\"token_id\"]   = gene_md[\"token_id\"].astype(int)\n",
    "    gene_md = gene_md.sort_values(\"token_id\").reset_index(drop=True)\n",
    "\n",
    "    W = np.load(pretrained_gene_npy)  # (N_genes, d_model)\n",
    "    assert W.shape[1] == token_emb.weight.shape[1]\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idxs = rng.integers(0, min(len(gene_md), W.shape[0]), size=n_check)\n",
    "\n",
    "    max_abs = 0.0\n",
    "    bad = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in idxs:\n",
    "            ensg = gene_md.loc[i, \"ensembl_id\"]\n",
    "            vid = local_token_to_id.get(ensg, None)\n",
    "            if vid is None:\n",
    "                continue\n",
    "\n",
    "            a = token_emb.weight[vid].detach().cpu().numpy()\n",
    "            b = W[i]\n",
    "\n",
    "            diff = np.max(np.abs(a - b))\n",
    "            max_abs = max(max_abs, float(diff))\n",
    "            if diff > 1e-6:\n",
    "                bad += 1\n",
    "                print(\"Mismatch:\", \"i=\", i, \"ensg=\", ensg, \"vid=\", vid, \"max_abs_diff=\", diff)\n",
    "\n",
    "    print(f\"[sanity] checked={n_check}, bad={bad}, max_abs_diff={max_abs}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 12) Init model\n",
    "# =========================================================\n",
    "D_MODEL = 256\n",
    "assert W_cell.shape[1] == D_MODEL, f\"cell_emb dim {W_cell.shape[1]} != D_MODEL {D_MODEL}\"\n",
    "\n",
    "encoder = Cell2SentenceEncoderFR(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=8,\n",
    "    num_layers=4,\n",
    "    max_len_with_prefix=(3 + MAX_LEN),\n",
    "    smiles_dim=smiles_dim,\n",
    "    num_cell_lines=NUM_CELL_LINE,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "load_pretrained_token_emb_from_gene_metadata(\n",
    "    token_emb=encoder.token_emb,\n",
    "    npy_path=PRETRAINED_GENE_NPY,\n",
    "    gene_meta_path=GENE_META_PATH,\n",
    "    local_token_to_id=local_token_to_id,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "load_pretrained_cell_emb(\n",
    "    cell_emb=encoder.cell_line_emb,\n",
    "    cell_emb_npy=CELL_EMB_NPY,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "fr_model = FRModelExpression(encoder=encoder, d_model=D_MODEL, out_dim=TOP_K).to(device)\n",
    "optimizer = torch.optim.AdamW(fr_model.parameters(), lr=LR, weight_decay=0.01)\n",
    "scaler = GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "print(\"âœ… f_r model ready\")\n",
    "sanity_check_gene_emb_mapping(GENE_META_PATH, local_token_to_id, encoder.token_emb, PRETRAINED_GENE_NPY)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 13) Losses (MSE + ranking)\n",
    "# =========================================================\n",
    "mse_loss = nn.MSELoss()\n",
    "baseline_vec = torch.tensor(baseline_global[sorted_gene_token_ids], dtype=torch.float32, device=device)  # (TOP_K,)\n",
    "\n",
    "def expr_ranking_loss(y_pred, y_true, baseline_vec, top_pos=30, num_neg=80, margin=0.0):\n",
    "    device_ = y_pred.device\n",
    "    B, K = y_pred.shape\n",
    "\n",
    "    base = baseline_vec.view(1, K).expand(B, K).to(device=device_, dtype=y_pred.dtype)\n",
    "    dt = y_true - base\n",
    "    dp = y_pred - base\n",
    "\n",
    "    losses = []\n",
    "    for b in range(B):\n",
    "        order = torch.argsort(dt[b].abs(), descending=True)\n",
    "        P = min(top_pos, K)\n",
    "        pos_idx = order[:P]\n",
    "        neg_candidates = order[P:]\n",
    "        if neg_candidates.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        if neg_candidates.numel() > num_neg:\n",
    "            neg_idx = neg_candidates[torch.randperm(neg_candidates.numel(), device=device_)[:num_neg]]\n",
    "        else:\n",
    "            neg_idx = neg_candidates\n",
    "\n",
    "        pos_scores = dp[b, pos_idx]\n",
    "        neg_scores = dp[b, neg_idx]\n",
    "\n",
    "        diff = pos_scores.view(-1, 1) - neg_scores.view(1, -1)\n",
    "        loss_mat = F.relu(margin - diff)\n",
    "        losses.append(loss_mat.mean())\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=device_, dtype=y_pred.dtype)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 14) Checkpoint utils\n",
    "# =========================================================\n",
    "def save_fr_checkpoint(save_dir, fr_model, optimizer, scaler, epoch, metrics=None, extra=None, prefix=\"fr\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    ckpt = {\n",
    "        \"epoch\": int(epoch),\n",
    "        \"model_state\": fr_model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict() if scaler is not None else None,\n",
    "        \"metrics\": metrics or {},\n",
    "        \"extra\": extra or {},\n",
    "    }\n",
    "\n",
    "    path = os.path.join(save_dir, f\"{prefix}_epoch{epoch}_{ts}.pt\")\n",
    "    torch.save(ckpt, path)\n",
    "    print(f\"ðŸ’¾ saved checkpoint: {path}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 15) Eval helpers\n",
    "# =========================================================\n",
    "@torch.no_grad()\n",
    "def eval_mse(fr_model, val_loader, steps, device):\n",
    "    fr_model.eval()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    for batch in islice(val_loader, steps):\n",
    "        input_ids, values, mask, y_true, cell_id, drug_id, smiles = batch\n",
    "        input_ids = input_ids.to(device, non_blocking=True)\n",
    "        values    = values.to(device, non_blocking=True)\n",
    "        mask      = mask.to(device, non_blocking=True)\n",
    "        y_true    = y_true.to(device, non_blocking=True)\n",
    "        cell_id   = cell_id.to(device, non_blocking=True)\n",
    "        smiles    = smiles.to(device, non_blocking=True)\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "            y_pred = fr_model(input_ids, values, mask, cell_id, smiles)\n",
    "            loss = mse_loss(y_pred, y_true)\n",
    "\n",
    "        bs = y_true.size(0)\n",
    "        total += loss.item() * bs\n",
    "        n += bs\n",
    "    return total / max(1, n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def baseline_mse(val_loader, steps, baseline_vec, device):\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "    baseline_vec = baseline_vec.to(device)\n",
    "    for batch in islice(val_loader, steps):\n",
    "        _, _, _, y_true, _, _, _ = batch\n",
    "        y_true = y_true.to(device, non_blocking=True)\n",
    "        bs = y_true.size(0)\n",
    "        pred = baseline_vec.view(1, -1).expand(bs, -1)\n",
    "        loss = F.mse_loss(pred, y_true)\n",
    "        total += loss.item() * bs\n",
    "        n += bs\n",
    "    return total / max(1, n)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 16) TRAIN\n",
    "# =========================================================\n",
    "base_mse = baseline_mse(val_loader, steps=VAL_STEPS, baseline_vec=baseline_vec, device=device)\n",
    "print(f\"Baseline Valid MSE (DMSO) = {base_mse:.6f}\")\n",
    "\n",
    "print(\"ðŸš€ f_r training start\")\n",
    "\n",
    "for epoch in range(1, TOTAL_EPOCHS + 1):\n",
    "    lambda_rank = 0.0 if epoch <= WARMUP_EPOCHS else lambda_rank_main\n",
    "\n",
    "    fr_model.train()\n",
    "    run_mse = 0.0\n",
    "    run_rank = 0.0\n",
    "    run_total = 0.0\n",
    "    n = 0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        islice(train_loader, STEPS_PER_EPOCH),\n",
    "        total=STEPS_PER_EPOCH,\n",
    "        desc=f\"[Epoch {epoch}] Train\",\n",
    "        leave=True,\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        input_ids, values, mask, y_true, cell_id, drug_id, smiles = batch\n",
    "\n",
    "        input_ids = input_ids.to(device, non_blocking=True)\n",
    "        values    = values.to(device, non_blocking=True)\n",
    "        mask      = mask.to(device, non_blocking=True)\n",
    "        y_true    = y_true.to(device, non_blocking=True)\n",
    "        cell_id   = cell_id.to(device, non_blocking=True)\n",
    "        smiles    = smiles.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "            y_pred = fr_model(input_ids, values, mask, cell_id, smiles)\n",
    "            loss_m = mse_loss(y_pred, y_true)\n",
    "\n",
    "            if lambda_rank > 0:\n",
    "                loss_r = expr_ranking_loss(\n",
    "                    y_pred, y_true, baseline_vec,\n",
    "                    top_pos=30, num_neg=80, margin=0.0\n",
    "                )\n",
    "            else:\n",
    "                loss_r = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = loss_m + lambda_rank * loss_r\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            continue\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(fr_model.parameters(), GRAD_CLIP)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        bs = y_true.size(0)\n",
    "        run_mse   += loss_m.item() * bs\n",
    "        run_rank  += loss_r.item() * bs\n",
    "        run_total += loss.item() * bs\n",
    "        n += bs\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"mse\": f\"{loss_m.item():.4f}\",\n",
    "            \"rank\": f\"{loss_r.item():.4f}\",\n",
    "            \"Î»_rank\": float(lambda_rank),\n",
    "        })\n",
    "\n",
    "    train_mse   = run_mse   / max(1, n)\n",
    "    train_rank  = run_rank  / max(1, n)\n",
    "    train_total = run_total / max(1, n)\n",
    "\n",
    "    val_mse = eval_mse(fr_model, val_loader, steps=VAL_STEPS, device=device)\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch}] \"\n",
    "        f\"Train total={train_total:.6f}, mse={train_mse:.6f}, rank={train_rank:.6f} (Î»_rank={lambda_rank}) | \"\n",
    "        f\"Valid mse={val_mse:.6f} | Baseline(DMSO) mse={base_mse:.6f}\"\n",
    "    )\n",
    "\n",
    "    if (epoch % SAVE_EVERY == 0) or (epoch == TOTAL_EPOCHS):\n",
    "        save_fr_checkpoint(\n",
    "            save_dir=CKPT_DIR,\n",
    "            fr_model=fr_model,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "            epoch=epoch,\n",
    "            metrics={\n",
    "                \"train_total\": float(train_total),\n",
    "                \"train_mse\": float(train_mse),\n",
    "                \"train_rank\": float(train_rank),\n",
    "                \"val_mse\": float(val_mse),\n",
    "                \"baseline_mse\": float(base_mse),\n",
    "                \"lambda_rank\": float(lambda_rank),\n",
    "            },\n",
    "            extra={\n",
    "                \"TOP_K\": int(baseline_vec.numel()),\n",
    "                \"STEPS_PER_EPOCH\": int(STEPS_PER_EPOCH),\n",
    "                \"VAL_STEPS\": int(VAL_STEPS),\n",
    "                \"WARMUP_EPOCHS\": int(WARMUP_EPOCHS),\n",
    "                \"lambda_rank_main\": float(lambda_rank_main),\n",
    "                \"baseline_vec\": baseline_vec.detach().float().cpu(),\n",
    "                \"CELL2ID_CSV\": CELL2ID_CSV,\n",
    "                \"CELL_EMB_NPY\": CELL_EMB_NPY,\n",
    "                \"sorted_gene_token_ids\": sorted_gene_token_ids.astype(np.int64)\n",
    "            },\n",
    "            prefix=\"fr\",\n",
    "        )\n",
    "\n",
    "print(\"âœ… DONE\")\n",
    "\n",
    "gene_md = pd.read_parquet(GENE_META_PATH)[[\"token_id\",\"ensembl_id\"]].copy()\n",
    "tid2ensg = dict(zip(gene_md[\"token_id\"].astype(int), gene_md[\"ensembl_id\"].astype(str)))\n",
    "topk_ensg = np.array([tid2ensg[int(t)] for t in sorted_gene_token_ids], dtype=object)\n",
    "\n",
    "np.save(os.path.join(CKPT_DIR, f\"topk_ensg_k{TOP_K}.npy\"), topk_ensg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "995a07d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GLOBAL BASELINE ===\n",
      "    Baseline(DMSO) | n=14400 | RMSE=4.5727 | MAE=1.0536 | Cos(d)=0.0000 | Pear(d)=0.0000 | Spear(d)=0.1993 | Sign@30=0.0000 | R@10=0.2143 | NDCG@10=0.6216 | R@30=0.3374 | NDCG@30=0.2937 | R@50=0.3945 | NDCG@50=0.4885 | R@100=0.4989 | NDCG@100=0.5440\n",
      "=== GLOBAL f_r ===\n",
      "               f_r | n=14400 | RMSE=1.4338 | MAE=0.7684 | Cos(d)=0.8992 | Pear(d)=0.8972 | Spear(d)=0.3740 | Sign@30=0.8374 | R@10=0.2097 | NDCG@10=0.7251 | R@30=0.3539 | NDCG@30=0.4712 | R@50=0.4326 | NDCG@50=0.5195 | R@100=0.5443 | NDCG@100=0.5789\n",
      "\n",
      "=== TOP CELL LINES (by n) â€” f_r ===\n",
      "cell_id=    21 n=   608 RMSE=0.9280 Cos=0.9205 R@30=0.3249\n",
      "cell_id=    22 n=   576 RMSE=1.2060 Cos=0.8987 R@30=0.2634\n",
      "cell_id=    28 n=   560 RMSE=1.4188 Cos=0.8706 R@30=0.3307\n",
      "cell_id=    19 n=   560 RMSE=1.3351 Cos=0.8901 R@30=0.3883\n",
      "cell_id=    31 n=   560 RMSE=1.3043 Cos=0.9092 R@30=0.3905\n",
      "cell_id=    46 n=   544 RMSE=1.3893 Cos=0.9046 R@30=0.3049\n",
      "cell_id=    13 n=   528 RMSE=1.3350 Cos=0.9050 R@30=0.3628\n",
      "cell_id=    38 n=   528 RMSE=2.7416 Cos=0.9096 R@30=0.3627\n",
      "cell_id=    32 n=   512 RMSE=1.2924 Cos=0.8897 R@30=0.3391\n",
      "cell_id=    10 n=   512 RMSE=1.3008 Cos=0.9064 R@30=0.3293\n",
      "cell_id=    26 n=   464 RMSE=1.2242 Cos=0.8950 R@30=0.4085\n",
      "cell_id=    15 n=   416 RMSE=1.2210 Cos=0.9023 R@30=0.3732\n",
      "cell_id=    20 n=   416 RMSE=1.6381 Cos=0.9165 R@30=0.3944\n",
      "cell_id=    24 n=   416 RMSE=1.3315 Cos=0.8819 R@30=0.3563\n",
      "cell_id=    34 n=   400 RMSE=1.3119 Cos=0.9100 R@30=0.3673\n",
      "cell_id=     5 n=   384 RMSE=1.2792 Cos=0.9161 R@30=0.3529\n",
      "cell_id=    33 n=   384 RMSE=1.7787 Cos=0.8983 R@30=0.4245\n",
      "cell_id=    16 n=   352 RMSE=1.5332 Cos=0.9031 R@30=0.3508\n",
      "cell_id=    37 n=   336 RMSE=1.2873 Cos=0.8780 R@30=0.3487\n",
      "cell_id=     2 n=   336 RMSE=1.8934 Cos=0.8821 R@30=0.4122\n",
      "\n",
      "=== TOP DRUGS (by n) â€” f_r ===\n",
      "drug_id=    36 n=   112 RMSE=1.4562 Cos=0.9007 R@30=0.3771\n",
      "drug_id=   360 n=   112 RMSE=1.9257 Cos=0.8983 R@30=0.4637\n",
      "drug_id=   150 n=   112 RMSE=1.3937 Cos=0.8641 R@30=0.3565\n",
      "drug_id=    31 n=   112 RMSE=1.3259 Cos=0.9165 R@30=0.3929\n",
      "drug_id=   107 n=   112 RMSE=1.7689 Cos=0.8872 R@30=0.3613\n",
      "drug_id=    99 n=   112 RMSE=1.3879 Cos=0.8880 R@30=0.3679\n",
      "drug_id=    37 n=   112 RMSE=1.0376 Cos=0.9237 R@30=0.3268\n",
      "drug_id=   272 n=   112 RMSE=1.9175 Cos=0.8499 R@30=0.4271\n",
      "drug_id=    15 n=   112 RMSE=1.8567 Cos=0.8684 R@30=0.4777\n",
      "drug_id=   139 n=    96 RMSE=1.2481 Cos=0.8806 R@30=0.3594\n",
      "drug_id=   299 n=    96 RMSE=1.0971 Cos=0.8946 R@30=0.3372\n",
      "drug_id=   102 n=    96 RMSE=1.0730 Cos=0.9083 R@30=0.3139\n",
      "drug_id=   133 n=    96 RMSE=1.6322 Cos=0.9182 R@30=0.3427\n",
      "drug_id=   318 n=    96 RMSE=1.4534 Cos=0.8985 R@30=0.3063\n",
      "drug_id=   154 n=    96 RMSE=1.8365 Cos=0.8987 R@30=0.4250\n",
      "drug_id=   186 n=    96 RMSE=1.1518 Cos=0.9168 R@30=0.3021\n",
      "drug_id=    48 n=    96 RMSE=2.1283 Cos=0.9193 R@30=0.4038\n",
      "drug_id=   125 n=    96 RMSE=1.0510 Cos=0.9245 R@30=0.3431\n",
      "drug_id=    19 n=    96 RMSE=1.2724 Cos=0.9369 R@30=0.2931\n",
      "drug_id=   269 n=    96 RMSE=1.2864 Cos=0.8721 R@30=0.3205\n",
      "âœ… saved CSVs to: /data/aiffel/babayakga/eval_outputs/fr_withcell\n"
     ]
    }
   ],
   "source": [
    "import os, math\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, Optional, Tuple, List, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    pd = None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Correlations\n",
    "# -----------------------------\n",
    "def pearson_corr(x: torch.Tensor, y: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    x = x - x.mean(dim=1, keepdim=True)\n",
    "    y = y - y.mean(dim=1, keepdim=True)\n",
    "    num = (x * y).sum(dim=1)\n",
    "    den = torch.sqrt((x * x).sum(dim=1).clamp_min(eps)) * torch.sqrt((y * y).sum(dim=1).clamp_min(eps))\n",
    "    return num / den.clamp_min(eps)\n",
    "\n",
    "def _rankdata(x: torch.Tensor) -> torch.Tensor:\n",
    "    order = torch.argsort(x, dim=1, descending=False)\n",
    "    ranks = torch.empty_like(order, dtype=torch.float32)\n",
    "    idx = torch.arange(x.size(1), device=x.device).view(1, -1).expand_as(order)\n",
    "    ranks.scatter_(1, order, idx.to(torch.float32))\n",
    "    return ranks\n",
    "\n",
    "def spearman_corr(x: torch.Tensor, y: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    rx = _rankdata(x)\n",
    "    ry = _rankdata(y)\n",
    "    return pearson_corr(rx, ry, eps=eps)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Top-|Î”| ranking metrics\n",
    "# -----------------------------\n",
    "def topk_precision_recall_ndcg(\n",
    "    pred_scores: torch.Tensor,   # (B, K), e.g. |d_pred|\n",
    "    true_scores: torch.Tensor,   # (B, K), e.g. |d_true|\n",
    "    k: int,\n",
    "    p_pos: int,\n",
    "    eps: float = 1e-8,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    B, Kdim = pred_scores.shape\n",
    "    k = min(k, Kdim)\n",
    "    p_pos = min(p_pos, Kdim)\n",
    "\n",
    "    gt_pos_idx = torch.topk(true_scores, k=p_pos, dim=1, largest=True).indices  # (B, p_pos)\n",
    "    pred_topk_idx = torch.topk(pred_scores, k=k, dim=1, largest=True).indices  # (B, k)\n",
    "\n",
    "    gt_mask = torch.zeros((B, Kdim), device=pred_scores.device, dtype=torch.bool)\n",
    "    gt_mask.scatter_(1, gt_pos_idx, True)\n",
    "\n",
    "    hits = gt_mask.gather(1, pred_topk_idx)  # (B, k)\n",
    "    hit_count = hits.sum(dim=1).to(torch.float32)\n",
    "\n",
    "    precision = hit_count / float(k)\n",
    "    recall = hit_count / float(p_pos)\n",
    "\n",
    "    pos = torch.arange(k, device=pred_scores.device, dtype=torch.float32)\n",
    "    denom = torch.log2(pos + 2.0)\n",
    "    dcg = (hits.to(torch.float32) / denom.view(1, -1)).sum(dim=1)\n",
    "\n",
    "    ideal_ones = min(p_pos, k)\n",
    "    idcg = (torch.ones((ideal_ones,), device=pred_scores.device, dtype=torch.float32) /\n",
    "            torch.log2(torch.arange(ideal_ones, device=pred_scores.device, dtype=torch.float32) + 2.0)).sum()\n",
    "\n",
    "    ndcg = dcg / idcg.clamp_min(eps)\n",
    "    return precision, recall, ndcg\n",
    "\n",
    "def sign_accuracy_on_top_pos(d_pred: torch.Tensor, d_true: torch.Tensor, p_pos: int = 30, eps: float = 1e-8) -> torch.Tensor:\n",
    "    B, Kdim = d_true.shape\n",
    "    p_pos = min(p_pos, Kdim)\n",
    "    idx = torch.topk(d_true.abs(), k=p_pos, dim=1, largest=True).indices  # (B, p_pos)\n",
    "\n",
    "    tp = d_true.gather(1, idx)\n",
    "    pp = d_pred.gather(1, idx)\n",
    "\n",
    "    valid = tp.abs() > eps\n",
    "    match = (torch.sign(tp) == torch.sign(pp)) & valid\n",
    "    denom = valid.sum(dim=1).clamp_min(1)\n",
    "    return match.sum(dim=1).to(torch.float32) / denom.to(torch.float32)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class FREvalConfig:\n",
    "    steps: Optional[int] = None\n",
    "    amp: bool = True\n",
    "    top_pos: int = 30\n",
    "    eval_ks: Tuple[int, ...] = (10, 30, 50, 100)\n",
    "    per_stratum: bool = True\n",
    "    max_groups_report: int = 30\n",
    "    min_group_size: int = 50\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Accumulator\n",
    "# -----------------------------\n",
    "class _MetricAccum:\n",
    "    def __init__(self, eval_ks: Tuple[int, ...], top_pos: int):\n",
    "        self.eval_ks = eval_ks\n",
    "        self.top_pos = top_pos\n",
    "        self.n = 0\n",
    "        self.sum_mse = 0.0\n",
    "        self.sum_mae = 0.0\n",
    "        self.sum_cos = 0.0\n",
    "        self.sum_pear = 0.0\n",
    "        self.sum_spear = 0.0\n",
    "        self.sum_sign = 0.0\n",
    "        self.sum_prec = {k: 0.0 for k in eval_ks}\n",
    "        self.sum_rec  = {k: 0.0 for k in eval_ks}\n",
    "        self.sum_ndcg = {k: 0.0 for k in eval_ks}\n",
    "\n",
    "    def add_batch(self, y_pred: torch.Tensor, y_true: torch.Tensor, base: torch.Tensor):\n",
    "        B = y_true.size(0)\n",
    "\n",
    "        mse_each = F.mse_loss(y_pred, y_true, reduction=\"none\").mean(dim=1)\n",
    "        mae_each = (y_pred - y_true).abs().mean(dim=1)\n",
    "\n",
    "        d_true = y_true - base\n",
    "        d_pred = y_pred - base\n",
    "\n",
    "        cos_each  = F.cosine_similarity(d_pred, d_true, dim=1)\n",
    "        pear_each = pearson_corr(d_pred, d_true)\n",
    "        spear_each = spearman_corr(d_pred, d_true)\n",
    "\n",
    "        abs_dt = d_true.abs()\n",
    "        abs_dp = d_pred.abs()\n",
    "\n",
    "        sign_each = sign_accuracy_on_top_pos(d_pred, d_true, p_pos=self.top_pos)\n",
    "\n",
    "        self.n += B\n",
    "        self.sum_mse += float(mse_each.sum().item())\n",
    "        self.sum_mae += float(mae_each.sum().item())\n",
    "        self.sum_cos += float(cos_each.sum().item())\n",
    "        self.sum_pear += float(pear_each.sum().item())\n",
    "        self.sum_spear += float(spear_each.sum().item())\n",
    "        self.sum_sign += float(sign_each.sum().item())\n",
    "\n",
    "        for k in self.eval_ks:\n",
    "            p, r, nd = topk_precision_recall_ndcg(abs_dp, abs_dt, k=k, p_pos=self.top_pos)\n",
    "            self.sum_prec[k] += float(p.sum().item())\n",
    "            self.sum_rec[k]  += float(r.sum().item())\n",
    "            self.sum_ndcg[k] += float(nd.sum().item())\n",
    "\n",
    "    def to_dict(self) -> Dict[str, float]:\n",
    "        n = max(1, self.n)\n",
    "        out = {\n",
    "            \"n_samples\": int(self.n),\n",
    "            \"mse\": self.sum_mse / n,\n",
    "            \"rmse\": math.sqrt(self.sum_mse / n),\n",
    "            \"mae\": self.sum_mae / n,\n",
    "            \"cosine_d\": self.sum_cos / n,\n",
    "            \"pearson_d\": self.sum_pear / n,\n",
    "            \"spearman_d\": self.sum_spear / n,\n",
    "            f\"signacc_top{self.top_pos}\": self.sum_sign / n,\n",
    "        }\n",
    "        for k in self.eval_ks:\n",
    "            out[f\"precision@{k}\"] = self.sum_prec[k] / n\n",
    "            out[f\"recall@{k}\"]    = self.sum_rec[k] / n\n",
    "            out[f\"ndcg@{k}\"]      = self.sum_ndcg[k] / n\n",
    "        return out\n",
    "\n",
    "\n",
    "def _pretty_line(name: str, m: Dict[str, float], cfg: FREvalConfig) -> str:\n",
    "    parts = [\n",
    "        f\"{name:>18}\",\n",
    "        f\"n={m['n_samples']}\",\n",
    "        f\"RMSE={m['rmse']:.4f}\",\n",
    "        f\"MAE={m['mae']:.4f}\",\n",
    "        f\"Cos(d)={m['cosine_d']:.4f}\",\n",
    "        f\"Pear(d)={m['pearson_d']:.4f}\",\n",
    "        f\"Spear(d)={m['spearman_d']:.4f}\",\n",
    "        f\"Sign@{cfg.top_pos}={m[f'signacc_top{cfg.top_pos}']:.4f}\",\n",
    "    ]\n",
    "    for k in cfg.eval_ks:\n",
    "        parts += [f\"R@{k}={m[f'recall@{k}']:.4f}\", f\"NDCG@{k}={m[f'ndcg@{k}']:.4f}\"]\n",
    "    return \" | \".join(parts)\n",
    "\n",
    "\n",
    "def _group_report(group_name: str, group_dict: Dict[int, _MetricAccum], cfg: FREvalConfig, sort_by: str = \"n_samples\"):\n",
    "    rows = []\n",
    "    for gid, acc in group_dict.items():\n",
    "        d = acc.to_dict()\n",
    "        d[group_name] = int(gid)\n",
    "        rows.append(d)\n",
    "\n",
    "    rows_print = [r for r in rows if r[\"n_samples\"] >= cfg.min_group_size]\n",
    "    if len(rows_print) > 0 and sort_by in rows_print[0]:\n",
    "        rows_print.sort(key=lambda r: r[sort_by], reverse=True)\n",
    "    else:\n",
    "        rows_print.sort(key=lambda r: r[\"n_samples\"], reverse=True)\n",
    "\n",
    "    return rows_print[: cfg.max_groups_report], rows\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_fr_with_strata(fr_model, loader: Iterable, baseline_vec: torch.Tensor, device: torch.device, cfg: FREvalConfig):\n",
    "    fr_model.eval()\n",
    "    baseline_vec = baseline_vec.to(device=device)\n",
    "\n",
    "    global_acc = _MetricAccum(cfg.eval_ks, cfg.top_pos)\n",
    "    cell_acc: Dict[int, _MetricAccum] = {}\n",
    "    drug_acc: Dict[int, _MetricAccum] = {}\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        if cfg.steps is not None and step >= cfg.steps:\n",
    "            break\n",
    "\n",
    "        input_ids, values, mask, y_true, cell_id, drug_id, smiles = batch\n",
    "\n",
    "        input_ids = input_ids.to(device, non_blocking=True)\n",
    "        values    = values.to(device, non_blocking=True)\n",
    "        mask      = mask.to(device, non_blocking=True)\n",
    "        y_true    = y_true.to(device, non_blocking=True)\n",
    "        cell_id   = cell_id.to(device, non_blocking=True)\n",
    "        drug_id   = drug_id.to(device, non_blocking=True)\n",
    "        smiles    = smiles.to(device, non_blocking=True)\n",
    "\n",
    "        B, Kdim = y_true.shape\n",
    "        base = baseline_vec.view(1, Kdim).expand(B, Kdim)\n",
    "\n",
    "        with autocast(device_type=device.type, enabled=(cfg.amp and device.type == \"cuda\")):\n",
    "            # âœ… Ð½Ð¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¶Ð´Ñ‘Ñ‚ cell_line_id Ð¸ smiles_emb -> ÑÐ¾Ð²Ð¿Ð°Ð´Ð°ÐµÑ‚\n",
    "            y_pred = fr_model(input_ids, values, mask, cell_id, smiles)\n",
    "\n",
    "        global_acc.add_batch(y_pred, y_true, base)\n",
    "\n",
    "        if cfg.per_stratum:\n",
    "            # per cell\n",
    "            for cid in torch.unique(cell_id).tolist():\n",
    "                cid = int(cid)\n",
    "                m = (cell_id == cid)\n",
    "                if cid not in cell_acc:\n",
    "                    cell_acc[cid] = _MetricAccum(cfg.eval_ks, cfg.top_pos)\n",
    "                cell_acc[cid].add_batch(y_pred[m], y_true[m], base[m])\n",
    "\n",
    "            # per drug\n",
    "            for did in torch.unique(drug_id).tolist():\n",
    "                did = int(did)\n",
    "                m = (drug_id == did)\n",
    "                if did not in drug_acc:\n",
    "                    drug_acc[did] = _MetricAccum(cfg.eval_ks, cfg.top_pos)\n",
    "                drug_acc[did].add_batch(y_pred[m], y_true[m], base[m])\n",
    "\n",
    "    result = {\"global\": global_acc.to_dict(), \"per_cell\": None, \"per_drug\": None}\n",
    "    if cfg.per_stratum:\n",
    "        cell_top, cell_all = _group_report(\"cell_id\", cell_acc, cfg, sort_by=\"n_samples\")\n",
    "        drug_top, drug_all = _group_report(\"drug_id\", drug_acc, cfg, sort_by=\"n_samples\")\n",
    "        result[\"per_cell\"] = {\"top_report\": cell_top, \"all_rows\": cell_all}\n",
    "        result[\"per_drug\"] = {\"top_report\": drug_top, \"all_rows\": drug_all}\n",
    "    return result\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_baseline_dmso_with_strata(loader: Iterable, baseline_vec: torch.Tensor, device: torch.device, cfg: FREvalConfig):\n",
    "    baseline_vec = baseline_vec.to(device=device)\n",
    "\n",
    "    global_acc = _MetricAccum(cfg.eval_ks, cfg.top_pos)\n",
    "    cell_acc: Dict[int, _MetricAccum] = {}\n",
    "    drug_acc: Dict[int, _MetricAccum] = {}\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        if cfg.steps is not None and step >= cfg.steps:\n",
    "            break\n",
    "\n",
    "        _, _, _, y_true, cell_id, drug_id, _ = batch\n",
    "        y_true  = y_true.to(device, non_blocking=True)\n",
    "        cell_id = cell_id.to(device, non_blocking=True)\n",
    "        drug_id = drug_id.to(device, non_blocking=True)\n",
    "\n",
    "        B, Kdim = y_true.shape\n",
    "        base = baseline_vec.view(1, Kdim).expand(B, Kdim)\n",
    "        y_pred = base\n",
    "\n",
    "        global_acc.add_batch(y_pred, y_true, base)\n",
    "\n",
    "        if cfg.per_stratum:\n",
    "            for cid in torch.unique(cell_id).tolist():\n",
    "                cid = int(cid)\n",
    "                m = (cell_id == cid)\n",
    "                if cid not in cell_acc:\n",
    "                    cell_acc[cid] = _MetricAccum(cfg.eval_ks, cfg.top_pos)\n",
    "                cell_acc[cid].add_batch(y_pred[m], y_true[m], base[m])\n",
    "\n",
    "            for did in torch.unique(drug_id).tolist():\n",
    "                did = int(did)\n",
    "                m = (drug_id == did)\n",
    "                if did not in drug_acc:\n",
    "                    drug_acc[did] = _MetricAccum(cfg.eval_ks, cfg.top_pos)\n",
    "                drug_acc[did].add_batch(y_pred[m], y_true[m], base[m])\n",
    "\n",
    "    result = {\"global\": global_acc.to_dict(), \"per_cell\": None, \"per_drug\": None}\n",
    "    if cfg.per_stratum:\n",
    "        cell_top, cell_all = _group_report(\"cell_id\", cell_acc, cfg, sort_by=\"n_samples\")\n",
    "        drug_top, drug_all = _group_report(\"drug_id\", drug_acc, cfg, sort_by=\"n_samples\")\n",
    "        result[\"per_cell\"] = {\"top_report\": cell_top, \"all_rows\": cell_all}\n",
    "        result[\"per_drug\"] = {\"top_report\": drug_top, \"all_rows\": drug_all}\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================\n",
    "# RUN\n",
    "# ============================\n",
    "eval_cfg = FREvalConfig(\n",
    "    steps=VAL_STEPS,     # Ð¸Ð»Ð¸ None\n",
    "    amp=True,\n",
    "    top_pos=30,\n",
    "    eval_ks=(10, 30, 50, 100),\n",
    "    per_stratum=True,\n",
    "    max_groups_report=20,\n",
    "    min_group_size=50,\n",
    ")\n",
    "\n",
    "base_res = eval_baseline_dmso_with_strata(val_loader, baseline_vec, device, eval_cfg)\n",
    "print(\"=== GLOBAL BASELINE ===\")\n",
    "print(_pretty_line(\"Baseline(DMSO)\", base_res[\"global\"], eval_cfg))\n",
    "\n",
    "fr_res = eval_fr_with_strata(fr_model, val_loader, baseline_vec, device, eval_cfg)\n",
    "print(\"=== GLOBAL f_r ===\")\n",
    "print(_pretty_line(\"f_r\", fr_res[\"global\"], eval_cfg))\n",
    "\n",
    "print(\"\\n=== TOP CELL LINES (by n) â€” f_r ===\")\n",
    "for row in fr_res[\"per_cell\"][\"top_report\"]:\n",
    "    print(f\"cell_id={row['cell_id']:>6} n={row['n_samples']:>6} RMSE={row['rmse']:.4f} Cos={row['cosine_d']:.4f} R@30={row.get('recall@30', float('nan')):.4f}\")\n",
    "\n",
    "print(\"\\n=== TOP DRUGS (by n) â€” f_r ===\")\n",
    "for row in fr_res[\"per_drug\"][\"top_report\"]:\n",
    "    print(f\"drug_id={row['drug_id']:>6} n={row['n_samples']:>6} RMSE={row['rmse']:.4f} Cos={row['cosine_d']:.4f} R@30={row.get('recall@30', float('nan')):.4f}\")\n",
    "\n",
    "# optional save\n",
    "if pd is not None:\n",
    "    out_dir = \"/data/aiffel/babayakga/eval_outputs/fr_withcell\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    df_cell = pd.DataFrame(fr_res[\"per_cell\"][\"all_rows\"])\n",
    "    df_drug = pd.DataFrame(fr_res[\"per_drug\"][\"all_rows\"])\n",
    "\n",
    "    df_cell.to_csv(os.path.join(out_dir, \"per_cell_metrics.csv\"), index=False)\n",
    "    df_drug.to_csv(os.path.join(out_dir, \"per_drug_metrics.csv\"), index=False)\n",
    "    print(f\"âœ… saved CSVs to: {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff85fe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ f_r training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [3:01:43<00:00,  1.09s/it, mse=2.3042, rank=0.0000, Î»_rank=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train total=1.847853, mse=1.847853, rank=0.000000 (Î»_rank=0.0) | Valid mse=1.970388 | Baseline(DMSO) mse=20.909212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [2:52:08<00:00,  1.03s/it, mse=2.1046, rank=0.0000, Î»_rank=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Train total=1.791824, mse=1.791824, rank=0.000000 (Î»_rank=0.0) | Valid mse=1.850703 | Baseline(DMSO) mse=20.909212\n",
      "ðŸ’¾ saved checkpoint: /data/aiffel/babayakga/checkpoints/f_r_withcellline/fr_epoch2_20251226_185142.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [2:13:33<00:00,  1.25it/s, mse=2.1844, rank=2.8223, Î»_rank=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Train total=2.278205, mse=1.793413, rank=2.423961 (Î»_rank=0.2) | Valid mse=1.829237 | Baseline(DMSO) mse=20.909212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [2:40:48<00:00,  1.04it/s, mse=2.1488, rank=2.6523, Î»_rank=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Train total=2.230828, mse=1.747259, rank=2.417844 (Î»_rank=0.2) | Valid mse=1.768221 | Baseline(DMSO) mse=20.909212\n",
      "ðŸ’¾ saved checkpoint: /data/aiffel/babayakga/checkpoints/f_r_withcellline/fr_epoch4_20251227_001233.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [2:17:48<00:00,  1.21it/s, mse=2.1317, rank=2.6191, Î»_rank=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Train total=2.206838, mse=1.723612, rank=2.416120 (Î»_rank=0.2) | Valid mse=1.716062 | Baseline(DMSO) mse=20.909212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [2:28:52<00:00,  1.12it/s, mse=2.2265, rank=2.4688, Î»_rank=0.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Train total=2.179475, mse=1.696342, rank=2.415661 (Î»_rank=0.2) | Valid mse=1.749337 | Baseline(DMSO) mse=20.909212\n",
      "ðŸ’¾ saved checkpoint: /data/aiffel/babayakga/checkpoints/f_r_withcellline/fr_epoch6_20251227_052053.pt\n",
      "âœ… DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸš€ f_r training start\")\n",
    "TOTAL_EPOCHS=6\n",
    "for epoch in range(1, TOTAL_EPOCHS + 1):\n",
    "    lambda_rank = 0.0 if epoch <= WARMUP_EPOCHS else lambda_rank_main\n",
    "\n",
    "    fr_model.train()\n",
    "    run_mse = 0.0\n",
    "    run_rank = 0.0\n",
    "    run_total = 0.0\n",
    "    n = 0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        islice(train_loader, STEPS_PER_EPOCH),\n",
    "        total=STEPS_PER_EPOCH,\n",
    "        desc=f\"[Epoch {epoch}] Train\",\n",
    "        leave=True,\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "\n",
    "    for batch in pbar:\n",
    "        input_ids, values, mask, y_true, cell_id, drug_id, smiles = batch\n",
    "\n",
    "        input_ids = input_ids.to(device, non_blocking=True)\n",
    "        values    = values.to(device, non_blocking=True)\n",
    "        mask      = mask.to(device, non_blocking=True)\n",
    "        y_true    = y_true.to(device, non_blocking=True)\n",
    "        cell_id   = cell_id.to(device, non_blocking=True)\n",
    "        smiles    = smiles.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type=\"cuda\", enabled=(device.type == \"cuda\")):\n",
    "            y_pred = fr_model(input_ids, values, mask, cell_id, smiles)\n",
    "            loss_m = mse_loss(y_pred, y_true)\n",
    "\n",
    "            if lambda_rank > 0:\n",
    "                loss_r = expr_ranking_loss(\n",
    "                    y_pred, y_true, baseline_vec,\n",
    "                    top_pos=30, num_neg=80, margin=0.0\n",
    "                )\n",
    "            else:\n",
    "                loss_r = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = loss_m + lambda_rank * loss_r\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            continue\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(fr_model.parameters(), GRAD_CLIP)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        bs = y_true.size(0)\n",
    "        run_mse   += loss_m.item() * bs\n",
    "        run_rank  += loss_r.item() * bs\n",
    "        run_total += loss.item() * bs\n",
    "        n += bs\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"mse\": f\"{loss_m.item():.4f}\",\n",
    "            \"rank\": f\"{loss_r.item():.4f}\",\n",
    "            \"Î»_rank\": float(lambda_rank),\n",
    "        })\n",
    "\n",
    "    train_mse   = run_mse   / max(1, n)\n",
    "    train_rank  = run_rank  / max(1, n)\n",
    "    train_total = run_total / max(1, n)\n",
    "\n",
    "    val_mse = eval_mse(fr_model, val_loader, steps=VAL_STEPS, device=device)\n",
    "\n",
    "    print(\n",
    "        f\"[Epoch {epoch}] \"\n",
    "        f\"Train total={train_total:.6f}, mse={train_mse:.6f}, rank={train_rank:.6f} (Î»_rank={lambda_rank}) | \"\n",
    "        f\"Valid mse={val_mse:.6f} | Baseline(DMSO) mse={base_mse:.6f}\"\n",
    "    )\n",
    "\n",
    "    if (epoch % SAVE_EVERY == 0) or (epoch == TOTAL_EPOCHS):\n",
    "        save_fr_checkpoint(\n",
    "            save_dir=CKPT_DIR,\n",
    "            fr_model=fr_model,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "            epoch=epoch,\n",
    "            metrics={\n",
    "                \"train_total\": float(train_total),\n",
    "                \"train_mse\": float(train_mse),\n",
    "                \"train_rank\": float(train_rank),\n",
    "                \"val_mse\": float(val_mse),\n",
    "                \"baseline_mse\": float(base_mse),\n",
    "                \"lambda_rank\": float(lambda_rank),\n",
    "            },\n",
    "            extra={\n",
    "                \"TOP_K\": int(baseline_vec.numel()),\n",
    "                \"STEPS_PER_EPOCH\": int(STEPS_PER_EPOCH),\n",
    "                \"VAL_STEPS\": int(VAL_STEPS),\n",
    "                \"WARMUP_EPOCHS\": int(WARMUP_EPOCHS),\n",
    "                \"lambda_rank_main\": float(lambda_rank_main),\n",
    "                \"baseline_vec\": baseline_vec.detach().float().cpu(),\n",
    "                \"CELL2ID_CSV\": CELL2ID_CSV,\n",
    "                \"CELL_EMB_NPY\": CELL_EMB_NPY,\n",
    "                \"sorted_gene_token_ids\": sorted_gene_token_ids.astype(np.int64)\n",
    "            },\n",
    "            prefix=\"fr\",\n",
    "        )\n",
    "\n",
    "print(\"âœ… DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babayakga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
