{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fdd6a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# ‚úÖ TARGET-ONLY OUTPUT + (1st gene/expr token drop) + (NO SMILES MSE)\n",
    "#\n",
    "# What changed vs your current version:\n",
    "# 1) OUTPUT space is TARGET-ONLY (M_TGT = genes that ever appeared as targets, ~100)\n",
    "#    - y_targets: (B, M_TGT)\n",
    "#    - pos_weight: (M_TGT,)\n",
    "#    - loss/eval gene bank = subset_gene_emb[target_sub_ids]  -> (M_TGT, d)\n",
    "# 2) INPUT token space stays SUBSET = HVG ‚à™ TARGETS (context preserved)\n",
    "# 3) Drop the first (genes[0], expressions[0]) as \"service/non-real\" token\n",
    "# 4) Remove MSE from SMILES part:\n",
    "#    - train smiles loss = SupCon + alpha_align * cosine_align\n",
    "#    - validation metrics: only SMILES_SupCon, SMILES_COS (no SMILES_MSE)\n",
    "# =========================================================\n",
    "\n",
    "import os, glob, ast, random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scanpy as sc\n",
    "from scipy import sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6ae873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drugs total=379, with>=1 target=264\n",
      "HVG token_ids: 4000\n",
      "SUBSET (HVG ‚à™ TARGETS) size: 4184\n",
      "TARGET-ONLY size: 278\n",
      "VOCAB_SIZE: 4190\n",
      "drugs with>=1 target (target-only vec): 264\n",
      "‚úÖ SMILES aligned | missing=0/379\n",
      "baseline_global: (62713,) baseline_by_cl: 50\n",
      "train pairs: 10505\n",
      "val pairs: 1168\n",
      "parquet files found: 3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Index parquet row-groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3388/3388 [09:23<00:00,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexed pairs: 11673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 0) PATHS / HYPERPARAMS\n",
    "# =========================================================\n",
    "PARQUET_DIR    = \"/data/aiffel/data/Tahoe-100M/data\"\n",
    "GENE_META_PATH = \"/data/aiffel/data/Tahoe-100M/metadata/gene_metadata.parquet\"\n",
    "DRUG_META_PATH = \"/data/aiffel/data/Tahoe-100M/metadata/drug_metadata.parquet\"\n",
    "COUNTS_CSV     = \"/data/aiffel/babayakga/making_data/aiffel/babayakga/making_data/tahoe_counts_per_drug_cell_line.csv\"\n",
    "DMSO_PATH      = \"/data/aiffel/babayakga/outputs/dmso.h5ad\"\n",
    "\n",
    "SMILES_EMB_PATH       = \"/data/aiffel/babayakga/smiles_emb/drug_smiles_emb_all1.pt\"\n",
    "PRETRAINED_GENE_NPY   = \"/data/aiffel/babayakga/pretraining/checkpoints_with_cell/gene_embeddings.npy\"\n",
    "CELL2ID_CSV           = \"/data/aiffel/babayakga/pretraining/checkpoints_with_cell/cell2id.csv\"\n",
    "CELL_EMB_NPY          = \"/data/aiffel/babayakga/pretraining/checkpoints_with_cell/cell_embeddings.npy\"\n",
    "\n",
    "CONTROL_DRUG = \"DMSO_TF\"\n",
    "SEED = 42\n",
    "\n",
    "MAX_SEQ_LEN = 256\n",
    "BATCH_SIZE  = 128\n",
    "STEPS_PER_EPOCH = 10000\n",
    "VAL_STEPS       = 500\n",
    "\n",
    "lambda_cos    = 1.0\n",
    "lambda_bce    = 0.25\n",
    "lambda_rank   = 0.75\n",
    "\n",
    "lambda_smiles = 0.05\n",
    "alpha_align   = 0.5\n",
    "tau_smiles    = 0.07\n",
    "\n",
    "rank_num_neg = 512\n",
    "rank_num_pos = 8\n",
    "bce_num_neg  = 2048\n",
    "bce_pos_cap  = None\n",
    "\n",
    "EPOCHS = 20\n",
    "LR = 1e-5\n",
    "\n",
    "POSW_MODE = \"sqrt_clip\"  # [\"none\", \"clip\", \"sqrt_clip\"]\n",
    "POSW_MAX  = 50.0\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "HVG_K = 4000\n",
    "PAIRS_PER_BATCH = 16\n",
    "\n",
    "# ‚úÖ drop first \"service\" element from genes/expressions\n",
    "DROP_FIRST_GENE_TOKEN = True\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ‚úÖ TF32 safe speed-up on Ampere+\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# (extra) checkpoint save\n",
    "# =========================================================\n",
    "CKPT_DIR = \"/data/aiffel/babayakga/checkpoints/fp_target_only_no_mse_drop1\"\n",
    "SAVE_EVERY = 2\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(save_dir, model, optimizer, scaler, epoch, metrics=None, prefix=\"fp\"):\n",
    "    ckpt = {\n",
    "        \"epoch\": int(epoch),\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scaler_state_dict\": scaler.state_dict() if scaler is not None else None,\n",
    "        \"metrics\": metrics if metrics is not None else {},\n",
    "    }\n",
    "    path = os.path.join(save_dir, f\"{prefix}_epoch{epoch:03d}.pt\")\n",
    "    torch.save(ckpt, path)\n",
    "    print(f\"üíæ checkpoint saved: {path}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1) gene_metadata load\n",
    "# =========================================================\n",
    "gene_md = pd.read_parquet(GENE_META_PATH).copy()\n",
    "gene_md[\"gene_symbol\"] = gene_md[\"gene_symbol\"].astype(str)\n",
    "gene_md[\"ensembl_id\"]  = gene_md[\"ensembl_id\"].astype(str)\n",
    "gene_md[\"token_id\"]    = gene_md[\"token_id\"].astype(int)\n",
    "gene_md = gene_md.sort_values(\"token_id\").reset_index(drop=True)\n",
    "\n",
    "N_GENES = int(gene_md[\"token_id\"].max()) + 1\n",
    "symbol_to_ensg_lower = dict(zip(gene_md[\"gene_symbol\"].str.lower(), gene_md[\"ensembl_id\"]))\n",
    "ensg_to_token_id = dict(zip(gene_md[\"ensembl_id\"].values, gene_md[\"token_id\"].values))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) drug_metadata -> targets\n",
    "# =========================================================\n",
    "def parse_targets(x):\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return [str(t).strip() for t in x if str(t).strip()]\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "            try:\n",
    "                out = ast.literal_eval(s)\n",
    "                if isinstance(out, (list, tuple)):\n",
    "                    return [str(t).strip() for t in out if str(t).strip()]\n",
    "            except Exception:\n",
    "                pass\n",
    "        for sep in [\";\", \",\"]:\n",
    "            if sep in s:\n",
    "                return [t.strip() for t in s.split(sep) if t.strip()]\n",
    "        return [s]\n",
    "    return [str(x).strip()]\n",
    "\n",
    "drug_meta_df = pd.read_parquet(DRUG_META_PATH).copy()\n",
    "drug_meta_df[\"drug\"] = drug_meta_df[\"drug\"].astype(str)\n",
    "\n",
    "drug_to_target_tokenids = {}\n",
    "all_target_tokenids = set()\n",
    "\n",
    "for _, row in drug_meta_df.iterrows():\n",
    "    drug = str(row[\"drug\"])\n",
    "    targets = parse_targets(row.get(\"targets\", None))\n",
    "\n",
    "    tids = []\n",
    "    for t in targets:\n",
    "        t = str(t).strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        if t.startswith(\"ENSG\"):\n",
    "            ensg = t\n",
    "        else:\n",
    "            ensg = symbol_to_ensg_lower.get(t.lower(), None)\n",
    "        if ensg is None:\n",
    "            continue\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        tid = int(tid)\n",
    "        if 0 <= tid < N_GENES:\n",
    "            tids.append(tid)\n",
    "\n",
    "    tids = sorted(set(tids))\n",
    "    drug_to_target_tokenids[drug] = tids\n",
    "    all_target_tokenids.update(tids)\n",
    "\n",
    "drug_has_targets = {d: (len(tids) > 0) for d, tids in drug_to_target_tokenids.items()}\n",
    "print(f\"drugs total={len(drug_to_target_tokenids)}, with>=1 target={sum(drug_has_targets.values())}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3) HVG from DMSO\n",
    "# =========================================================\n",
    "def compute_hvg_token_ids_from_dmso(dmso_h5ad_path: str, control_drug: str, HVG_K: int, ensg_to_token_id: dict):\n",
    "    ad = sc.read_h5ad(dmso_h5ad_path)\n",
    "    obs = ad.obs\n",
    "    m = (obs[\"drug\"].astype(str).values == control_drug)\n",
    "    idx = np.where(m)[0]\n",
    "    if idx.size == 0:\n",
    "        raise ValueError(f\"DMSO adata ÏïàÏóê control drug({control_drug})Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "    X = ad.X.tocsr() if sparse.issparse(ad.X) else sparse.csr_matrix(ad.X)\n",
    "    Xc = X[idx]\n",
    "\n",
    "    mean = np.asarray(Xc.mean(axis=0)).ravel()\n",
    "    mean2 = np.asarray(Xc.multiply(Xc).mean(axis=0)).ravel()\n",
    "    var = (mean2 - mean**2).astype(np.float32)\n",
    "\n",
    "    ensgs = ad.var_names.astype(str).tolist()\n",
    "\n",
    "    token_ids = []\n",
    "    vars_ = []\n",
    "    for j, ensg in enumerate(ensgs):\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        token_ids.append(int(tid))\n",
    "        vars_.append(float(var[j]))\n",
    "\n",
    "    token_ids = np.asarray(token_ids, dtype=np.int64)\n",
    "    vars_ = np.asarray(vars_, dtype=np.float32)\n",
    "\n",
    "    if token_ids.size == 0:\n",
    "        raise ValueError(\"DMSO var_namesÏôÄ gene_metadataÏùò ENSG mappingÏù¥ Í±∞Ïùò Ïïà ÎßûÏäµÎãàÎã§.\")\n",
    "\n",
    "    k = min(int(HVG_K), token_ids.size)\n",
    "    top = np.argpartition(-vars_, k-1)[:k]\n",
    "    return set(token_ids[top].tolist())\n",
    "\n",
    "hvg_token_ids = compute_hvg_token_ids_from_dmso(\n",
    "    dmso_h5ad_path=DMSO_PATH,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    HVG_K=HVG_K,\n",
    "    ensg_to_token_id=ensg_to_token_id,\n",
    ")\n",
    "print(\"HVG token_ids:\", len(hvg_token_ids))\n",
    "\n",
    "# INPUT subset = HVG ‚à™ TARGETS\n",
    "subset_token_ids = sorted(set(hvg_token_ids) | set(all_target_tokenids))\n",
    "M_SUB = len(subset_token_ids)\n",
    "print(\"SUBSET (HVG ‚à™ TARGETS) size:\", M_SUB)\n",
    "\n",
    "# OUTPUT target-only = TARGETS only\n",
    "target_token_ids = sorted(set(all_target_tokenids))\n",
    "M_TGT = len(target_token_ids)\n",
    "print(\"TARGET-ONLY size:\", M_TGT)\n",
    "\n",
    "old_tid_to_subid = {tid: i for i, tid in enumerate(subset_token_ids)}\n",
    "old_tid_to_tgtid = {tid: i for i, tid in enumerate(target_token_ids)}\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4) subset vocab + LUT (fast mapping)\n",
    "# =========================================================\n",
    "SPECIAL_TOKENS = [\"[PAD]\", \"[CLS]\", \"[DRUG]\", \"[TARGET]\", \"[CELL]\", \"[MASK]\"]\n",
    "local_token_to_id = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
    "N_SPECIAL = len(SPECIAL_TOKENS)\n",
    "\n",
    "VOCAB_SIZE = N_SPECIAL + M_SUB\n",
    "PAD_ID = local_token_to_id[\"[PAD]\"]\n",
    "CLS_ID = local_token_to_id[\"[CLS]\"]\n",
    "CELLTOK_ID = local_token_to_id[\"[CELL]\"]\n",
    "\n",
    "print(\"VOCAB_SIZE:\", VOCAB_SIZE)\n",
    "\n",
    "old_tid_to_vocab_lut = np.full((N_GENES,), -1, dtype=np.int64)\n",
    "for sid, old_tid in enumerate(subset_token_ids):\n",
    "    if 0 <= old_tid < N_GENES:\n",
    "        old_tid_to_vocab_lut[old_tid] = N_SPECIAL + sid\n",
    "\n",
    "subset_token_ids_np = np.asarray(subset_token_ids, dtype=np.int64)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5) y_targets (drug -> TARGET-ONLY multi-hot)\n",
    "# =========================================================\n",
    "drug_to_target_vec_tgt = {}\n",
    "for d, tids in drug_to_target_tokenids.items():\n",
    "    vec = np.zeros(M_TGT, dtype=np.float32)\n",
    "    for tid in tids:\n",
    "        j = old_tid_to_tgtid.get(int(tid), None)\n",
    "        if j is not None:\n",
    "            vec[j] = 1.0\n",
    "    drug_to_target_vec_tgt[d] = vec\n",
    "\n",
    "n_nonzero = sum(float(v.sum()) > 0 for v in drug_to_target_vec_tgt.values())\n",
    "print(\"drugs with>=1 target (target-only vec):\", n_nonzero)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6) SMILES embeddings\n",
    "# =========================================================\n",
    "obj = torch.load(SMILES_EMB_PATH, map_location=\"cpu\")\n",
    "assert isinstance(obj, dict) and \"drug\" in obj and \"emb\" in obj\n",
    "\n",
    "drug_list_saved = [str(d) for d in obj[\"drug\"]]\n",
    "emb_matrix = obj[\"emb\"].to(dtype=torch.float32).cpu().numpy()\n",
    "SMILES_DIM = int(emb_matrix.shape[1])\n",
    "\n",
    "drug_to_smiles_np_raw = {d: emb_matrix[i].astype(np.float32, copy=False) for i, d in enumerate(drug_list_saved)}\n",
    "\n",
    "drug_names = drug_meta_df[\"drug\"].astype(str).tolist()\n",
    "drug_to_smiles_np = {}\n",
    "missing = 0\n",
    "for d in drug_names:\n",
    "    v = drug_to_smiles_np_raw.get(d, None)\n",
    "    if v is None:\n",
    "        drug_to_smiles_np[d] = np.zeros((SMILES_DIM,), dtype=np.float32)\n",
    "        missing += 1\n",
    "    else:\n",
    "        drug_to_smiles_np[d] = v\n",
    "print(f\"‚úÖ SMILES aligned | missing={missing}/{len(drug_names)}\")\n",
    "\n",
    "drug2id = {d: i for i, d in enumerate(sorted(set(drug_names)))}\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7) DMSO baselines (gene-space)\n",
    "# =========================================================\n",
    "def build_dmso_baselines_gene_space(dmso_h5ad_path: str, control_drug: str, N_GENES: int, ensg_to_token_id: dict):\n",
    "    adata = sc.read_h5ad(dmso_h5ad_path)\n",
    "    obs = adata.obs\n",
    "    X = adata.X.tocsr() if sparse.issparse(adata.X) else sparse.csr_matrix(adata.X)\n",
    "\n",
    "    m = (obs[\"drug\"].astype(str).values == control_drug)\n",
    "    idx = np.where(m)[0]\n",
    "    if idx.size == 0:\n",
    "        raise ValueError(f\"DMSO adata ÏïàÏóê control drug({control_drug})Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "    ensgs = adata.var_names.astype(str).tolist()\n",
    "\n",
    "    token_ids, cols = [], []\n",
    "    for j, ensg in enumerate(ensgs):\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        token_ids.append(int(tid))\n",
    "        cols.append(j)\n",
    "\n",
    "    token_ids = np.asarray(token_ids, dtype=np.int64)\n",
    "    cols = np.asarray(cols, dtype=np.int64)\n",
    "\n",
    "    Xc = X[idx][:, cols]\n",
    "    mean_global_sub = np.asarray(Xc.mean(axis=0)).ravel().astype(np.float32)\n",
    "\n",
    "    baseline_global = np.zeros(N_GENES, dtype=np.float32)\n",
    "    baseline_global[token_ids] = mean_global_sub\n",
    "\n",
    "    baseline_by_cl = {}\n",
    "    cl_values = obs[\"cell_line_id\"].astype(str).values\n",
    "    for cl in np.unique(cl_values):\n",
    "        cl_idx = np.where(m & (cl_values == cl))[0]\n",
    "        if cl_idx.size == 0:\n",
    "            continue\n",
    "        Xcl = X[cl_idx][:, cols]\n",
    "        mean_cl_sub = np.asarray(Xcl.mean(axis=0)).ravel().astype(np.float32)\n",
    "        v = np.zeros(N_GENES, dtype=np.float32)\n",
    "        v[token_ids] = mean_cl_sub\n",
    "        baseline_by_cl[str(cl)] = v\n",
    "\n",
    "    return baseline_global, baseline_by_cl\n",
    "\n",
    "baseline_global, baseline_by_cl = build_dmso_baselines_gene_space(\n",
    "    dmso_h5ad_path=DMSO_PATH,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    N_GENES=N_GENES,\n",
    "    ensg_to_token_id=ensg_to_token_id,\n",
    ")\n",
    "print(\"baseline_global:\", baseline_global.shape, \"baseline_by_cl:\", len(baseline_by_cl))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8) split (drug, cell_line) pairs + weights\n",
    "# =========================================================\n",
    "DRUG_COL, CELL_COL, N_COL = \"drug\", \"cell_line_id\", \"n_cells\"\n",
    "MIN_TRAIN = 1000\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "counts = pd.read_csv(COUNTS_CSV)\n",
    "counts[DRUG_COL] = counts[DRUG_COL].astype(str)\n",
    "counts[CELL_COL] = counts[CELL_COL].astype(str)\n",
    "\n",
    "train_pool = counts[counts[N_COL] >= MIN_TRAIN].copy()\n",
    "pairs_df = train_pool[[DRUG_COL, CELL_COL]].drop_duplicates()\n",
    "\n",
    "pairs_df = pairs_df[pairs_df[DRUG_COL] != CONTROL_DRUG]\n",
    "pairs_df = pairs_df[pairs_df[DRUG_COL].map(lambda d: drug_has_targets.get(str(d), False))]\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    pairs_df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=pairs_df[DRUG_COL],\n",
    ")\n",
    "\n",
    "train_pairs = list(zip(train_df[DRUG_COL], train_df[CELL_COL]))\n",
    "val_pairs   = list(zip(val_df[DRUG_COL],   val_df[CELL_COL]))\n",
    "\n",
    "print(\"train pairs:\", len(train_pairs))\n",
    "print(\"val pairs:\", len(val_pairs))\n",
    "\n",
    "def make_pair_weights_from_counts(counts_df, pairs, drug_col=\"drug\", cell_col=\"cell_line_id\", n_col=\"n_cells\",\n",
    "                                  mode=\"inv_sqrt\", eps=1.0):\n",
    "    tmp = counts_df[[drug_col, cell_col, n_col]].copy()\n",
    "    tmp[drug_col] = tmp[drug_col].astype(str)\n",
    "    tmp[cell_col] = tmp[cell_col].astype(str)\n",
    "\n",
    "    pair2n = {(d, c): int(n) for d, c, n in tmp.values}\n",
    "\n",
    "    w = []\n",
    "    for p in pairs:\n",
    "        n = pair2n.get(p, 0)\n",
    "        if mode == \"inv\":\n",
    "            ww = 1.0 / (n + eps)\n",
    "        elif mode == \"inv_log\":\n",
    "            ww = 1.0 / np.log1p(n + eps)\n",
    "        else:\n",
    "            ww = 1.0 / np.sqrt(n + eps)\n",
    "        w.append(float(ww))\n",
    "\n",
    "    w = np.asarray(w, dtype=np.float64)\n",
    "    w = np.clip(w, 0.0, None)\n",
    "    w = w / (w.sum() + 1e-12)\n",
    "    return w\n",
    "\n",
    "w_train = make_pair_weights_from_counts(counts, train_pairs, mode=\"inv_sqrt\")\n",
    "w_val   = make_pair_weights_from_counts(counts, val_pairs,   mode=\"inv_sqrt\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9) parquet row-group indexing\n",
    "# =========================================================\n",
    "PARQUET_FILES = sorted(glob.glob(os.path.join(PARQUET_DIR, \"**\", \"*.parquet\"), recursive=True))\n",
    "print(\"parquet files found:\", len(PARQUET_FILES))\n",
    "\n",
    "def build_pair_to_locations(parquet_files, valid_pairs_set, drug_col=\"drug\", cell_col=\"cell_line_id\"):\n",
    "    out = defaultdict(list)\n",
    "    for f in tqdm(parquet_files, desc=\"Index parquet row-groups\", dynamic_ncols=True):\n",
    "        pf = pq.ParquetFile(f)\n",
    "        for rg in range(pf.num_row_groups):\n",
    "            tbl = pf.read_row_group(rg, columns=[drug_col, cell_col])\n",
    "            df = tbl.to_pandas()\n",
    "            pairs_here = set(zip(df[drug_col].astype(str), df[cell_col].astype(str)))\n",
    "            inter = pairs_here.intersection(valid_pairs_set)\n",
    "            for p in inter:\n",
    "                out[p].append((f, rg))\n",
    "    return out\n",
    "\n",
    "valid_pairs_set = set(train_pairs) | set(val_pairs)\n",
    "pair_to_locations = build_pair_to_locations(PARQUET_FILES, valid_pairs_set, drug_col=DRUG_COL, cell_col=CELL_COL)\n",
    "print(\"indexed pairs:\", len(pair_to_locations))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10) Dataset (drop first genes/expr token + target-only labels)\n",
    "# =========================================================\n",
    "class TahoeFPParquetDatasetMultiPair(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pair_to_locations,\n",
    "        pairs,\n",
    "        baseline_global,\n",
    "        baseline_by_cellline,\n",
    "        drug_to_target_vec_target_only,   # (M_TGT,)\n",
    "        drug2id,\n",
    "        drug_to_smiles_np,\n",
    "        n_genes_full,\n",
    "        steps,\n",
    "        max_seq_len=256,\n",
    "        batch_size=128,\n",
    "        pairs_per_batch=16,\n",
    "        control_drug=\"DMSO_TF\",\n",
    "        pad_id=0,\n",
    "        cls_id=1,\n",
    "        celltok_id=4,\n",
    "        cell_line2id=None,\n",
    "        unk_cell_id=0,\n",
    "        pair_weights=None,\n",
    "        seed=42,\n",
    "        drug_col=\"drug\",\n",
    "        cell_col=\"cell_line_id\",\n",
    "        genes_col=\"genes\",\n",
    "        expr_col=\"expressions\",\n",
    "        cap_per_pair_in_rg=None,\n",
    "        max_tries_per_pair=20,\n",
    "        invalid_global_gene_tids=(1, 2),\n",
    "        subset_token_ids_np=None,\n",
    "        old_tid_to_vocab_lut=None,\n",
    "        m_tgt: int = 0,\n",
    "        drop_first_gene_token: bool = True,  # ‚úÖ new\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pair_to_locations = pair_to_locations\n",
    "        self.pairs = list(pairs)\n",
    "\n",
    "        self.baseline_global = np.asarray(baseline_global, dtype=np.float32)\n",
    "        self.baseline_by_cellline = baseline_by_cellline or {}\n",
    "\n",
    "        self.drug_to_target_vec_target_only = drug_to_target_vec_target_only\n",
    "        self.drug2id = drug2id\n",
    "        self.drug_to_smiles_np = drug_to_smiles_np\n",
    "\n",
    "        self.n_genes_full = int(n_genes_full)\n",
    "        self.steps = int(steps)\n",
    "        self.max_seq_len = int(max_seq_len)\n",
    "        self.batch_size = int(batch_size)\n",
    "\n",
    "        self.pairs_per_batch = int(pairs_per_batch)\n",
    "        assert self.batch_size % self.pairs_per_batch == 0\n",
    "        self.cells_per_pair = self.batch_size // self.pairs_per_batch\n",
    "\n",
    "        self.control_drug = str(control_drug)\n",
    "        self.pad_id = int(pad_id)\n",
    "        self.cls_id = int(cls_id)\n",
    "\n",
    "        self.celltok_id = int(celltok_id)\n",
    "        self.cell_line2id = cell_line2id or {}\n",
    "        self.unk_cell_id = int(unk_cell_id)\n",
    "\n",
    "        self.drug_col = drug_col\n",
    "        self.cell_col = cell_col\n",
    "        self.genes_col = genes_col\n",
    "        self.expr_col = expr_col\n",
    "\n",
    "        self.cap_per_pair_in_rg = cap_per_pair_in_rg\n",
    "        self.max_tries_per_pair = int(max_tries_per_pair)\n",
    "        self.seed = int(seed)\n",
    "\n",
    "        any_vec = next(iter(self.drug_to_smiles_np.values()))\n",
    "        self.smiles_dim = int(any_vec.shape[-1])\n",
    "\n",
    "        self.invalid_global_gene_tids = np.asarray(list(set(int(x) for x in invalid_global_gene_tids)), dtype=np.int64)\n",
    "\n",
    "        self.m_tgt = int(m_tgt)\n",
    "        assert self.m_tgt > 0\n",
    "\n",
    "        self.drop_first_gene_token = bool(drop_first_gene_token)\n",
    "\n",
    "        # weights\n",
    "        if pair_weights is None:\n",
    "            self.pair_weights = None\n",
    "        else:\n",
    "            w = np.asarray(pair_weights, dtype=np.float64)\n",
    "            assert len(w) == len(self.pairs)\n",
    "            w = np.clip(w, 0.0, None)\n",
    "            w = w / (w.sum() + 1e-12)\n",
    "            self.pair_weights = w\n",
    "\n",
    "        self.subset_token_ids_np = subset_token_ids_np\n",
    "        self.old_tid_to_vocab_lut = old_tid_to_vocab_lut\n",
    "\n",
    "        self._pf_cache = {}\n",
    "\n",
    "    def _get_pf(self, file_path):\n",
    "        pf = self._pf_cache.get(file_path, None)\n",
    "        if pf is None:\n",
    "            pf = pq.ParquetFile(file_path)\n",
    "            self._pf_cache[file_path] = pf\n",
    "        return pf\n",
    "\n",
    "    def _read_row_group_df(self, file_path, rg_id, columns):\n",
    "        pf = self._get_pf(file_path)\n",
    "        return pf.read_row_group(rg_id, columns=columns).to_pandas()\n",
    "\n",
    "    def _prepare_sparse_sorted(self, genes, expr):\n",
    "        idx = np.asarray(genes, dtype=np.int64)\n",
    "        val = np.asarray(expr, dtype=np.float32)\n",
    "\n",
    "        # ‚úÖ drop the first \"service\" element (genes[0], expr[0])\n",
    "        if self.drop_first_gene_token:\n",
    "            if idx.size > 0 and val.size > 0:\n",
    "                L = min(idx.size, val.size)\n",
    "                idx = idx[:L]\n",
    "                val = val[:L]\n",
    "                if L >= 1:\n",
    "                    idx = idx[1:]\n",
    "                    val = val[1:]\n",
    "\n",
    "        if idx.size == 0 or val.size == 0:\n",
    "            return np.asarray([], dtype=np.int64), np.asarray([], dtype=np.float32)\n",
    "\n",
    "        L = min(idx.size, val.size)\n",
    "        idx = idx[:L]\n",
    "        val = val[:L]\n",
    "\n",
    "        if self.invalid_global_gene_tids.size > 0:\n",
    "            m_bad = np.isin(idx, self.invalid_global_gene_tids, assume_unique=False)\n",
    "            if m_bad.any():\n",
    "                keep = ~m_bad\n",
    "                idx = idx[keep]\n",
    "                val = val[keep]\n",
    "                if idx.size == 0:\n",
    "                    return idx, val\n",
    "\n",
    "        m = (idx >= 0) & (idx < self.n_genes_full)\n",
    "        idx = idx[m]\n",
    "        val = val[m]\n",
    "        if idx.size == 0:\n",
    "            return idx, val\n",
    "\n",
    "        order = np.argsort(idx)\n",
    "        return idx[order], val[order]\n",
    "\n",
    "    def _fill_one_row(self, row_genes, row_expr, baseline_vec, input_ids_row, values_row, attn_row):\n",
    "        idx_sorted, val_sorted = self._prepare_sparse_sorted(row_genes, row_expr)\n",
    "        if idx_sorted.size == 0:\n",
    "            return\n",
    "\n",
    "        delta = val_sorted - baseline_vec[idx_sorted]\n",
    "\n",
    "        mask_sub = np.isin(idx_sorted, self.subset_token_ids_np, assume_unique=False)\n",
    "        if not mask_sub.any():\n",
    "            return\n",
    "\n",
    "        idx_sorted = idx_sorted[mask_sub]\n",
    "        delta = delta[mask_sub]\n",
    "        if idx_sorted.size == 0:\n",
    "            return\n",
    "\n",
    "        k = min(self.max_seq_len, idx_sorted.size)\n",
    "        if k <= 0:\n",
    "            return\n",
    "\n",
    "        if k == idx_sorted.size:\n",
    "            top_pos = np.argsort(-np.abs(delta))\n",
    "        else:\n",
    "            top_pos = np.argpartition(-np.abs(delta), k - 1)[:k]\n",
    "            top_pos = top_pos[np.argsort(-np.abs(delta[top_pos]))]\n",
    "\n",
    "        sel_token_ids = idx_sorted[top_pos]\n",
    "        sel_delta = delta[top_pos]\n",
    "\n",
    "        sel_vocab_ids = self.old_tid_to_vocab_lut[sel_token_ids]\n",
    "        ok = sel_vocab_ids != -1\n",
    "        if not ok.any():\n",
    "            return\n",
    "\n",
    "        sel_vocab_ids = sel_vocab_ids[ok]\n",
    "        sel_delta = sel_delta[ok]\n",
    "\n",
    "        L = min(self.max_seq_len, sel_vocab_ids.size)\n",
    "        if L <= 0:\n",
    "            return\n",
    "\n",
    "        input_ids_row[2:2+L] = sel_vocab_ids[:L]\n",
    "        values_row[2:2+L]    = sel_delta[:L]\n",
    "        attn_row[2:2+L]      = 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        base_seed = self.seed if worker_info is None else (self.seed + worker_info.id)\n",
    "        rng = np.random.default_rng(base_seed)\n",
    "\n",
    "        pairs = self.pairs\n",
    "        weights = self.pair_weights\n",
    "\n",
    "        cols = [self.drug_col, self.cell_col, self.genes_col, self.expr_col]\n",
    "\n",
    "        cnt = 0\n",
    "        while True:\n",
    "            if weights is None:\n",
    "                chosen_idx = rng.integers(0, len(pairs), size=self.pairs_per_batch)\n",
    "            else:\n",
    "                chosen_idx = rng.choice(len(pairs), size=self.pairs_per_batch, replace=True, p=weights)\n",
    "\n",
    "            chosen_pairs = [pairs[i] for i in chosen_idx]\n",
    "\n",
    "            seq_len = 2 + self.max_seq_len\n",
    "            input_ids = np.full((self.batch_size, seq_len), self.pad_id, dtype=np.int64)\n",
    "            values    = np.zeros((self.batch_size, seq_len), dtype=np.float32)\n",
    "            attn      = np.zeros((self.batch_size, seq_len), dtype=np.int64)\n",
    "\n",
    "            input_ids[:, 0] = self.cls_id\n",
    "            input_ids[:, 1] = self.celltok_id\n",
    "            attn[:, 0:2] = 1\n",
    "\n",
    "            y_batch = np.zeros((self.batch_size, self.m_tgt), dtype=np.float32)\n",
    "            smiles_batch = np.zeros((self.batch_size, self.smiles_dim), dtype=np.float32)\n",
    "            drug_id_batch = np.zeros((self.batch_size,), dtype=np.int64)\n",
    "            cell_id_batch = np.zeros((self.batch_size,), dtype=np.int64)\n",
    "\n",
    "            row_ptr = 0\n",
    "            built_any = False\n",
    "\n",
    "            for (drug_name, cell_line) in chosen_pairs:\n",
    "                drug_name = str(drug_name)\n",
    "                cell_line = str(cell_line)\n",
    "\n",
    "                if drug_name == self.control_drug:\n",
    "                    continue\n",
    "\n",
    "                y_vec = self.drug_to_target_vec_target_only.get(drug_name, None)\n",
    "                if y_vec is None or float(np.sum(y_vec)) <= 0.0:\n",
    "                    continue\n",
    "\n",
    "                locs = self.pair_to_locations.get((drug_name, cell_line), [])\n",
    "                if not locs:\n",
    "                    continue\n",
    "\n",
    "                baseline = self.baseline_by_cellline.get(cell_line, self.baseline_global)\n",
    "\n",
    "                sm_vec = self.drug_to_smiles_np.get(drug_name, None)\n",
    "                if sm_vec is None:\n",
    "                    sm_vec = np.zeros((self.smiles_dim,), dtype=np.float32)\n",
    "\n",
    "                did = int(self.drug2id.get(drug_name, 0))\n",
    "                cid = int(self.cell_line2id.get(cell_line, self.unk_cell_id))\n",
    "\n",
    "                for _ in range(self.max_tries_per_pair):\n",
    "                    fpath, rg_id = locs[rng.integers(0, len(locs))]\n",
    "                    df = self._read_row_group_df(fpath, rg_id, columns=cols)\n",
    "\n",
    "                    df = df[(df[self.drug_col].astype(str) == drug_name) &\n",
    "                            (df[self.cell_col].astype(str) == cell_line)]\n",
    "                    if len(df) == 0:\n",
    "                        continue\n",
    "\n",
    "                    if self.cap_per_pair_in_rg is not None and len(df) > self.cap_per_pair_in_rg:\n",
    "                        df = df.sample(self.cap_per_pair_in_rg, replace=False, random_state=None)\n",
    "\n",
    "                    replace = len(df) < self.cells_per_pair\n",
    "                    df = df.sample(self.cells_per_pair, replace=replace, random_state=None)\n",
    "\n",
    "                    for r in df.itertuples(index=False):\n",
    "                        if row_ptr >= self.batch_size:\n",
    "                            break\n",
    "\n",
    "                        y_batch[row_ptr] = y_vec\n",
    "                        smiles_batch[row_ptr] = sm_vec\n",
    "                        drug_id_batch[row_ptr] = did\n",
    "                        cell_id_batch[row_ptr] = cid\n",
    "\n",
    "                        genes = getattr(r, self.genes_col)\n",
    "                        expr  = getattr(r, self.expr_col)\n",
    "\n",
    "                        self._fill_one_row(\n",
    "                            genes, expr, baseline,\n",
    "                            input_ids[row_ptr], values[row_ptr], attn[row_ptr]\n",
    "                        )\n",
    "                        row_ptr += 1\n",
    "\n",
    "                    built_any = True\n",
    "                    break\n",
    "\n",
    "                if row_ptr >= self.batch_size:\n",
    "                    break\n",
    "\n",
    "            if not built_any:\n",
    "                continue\n",
    "\n",
    "            if row_ptr < self.batch_size:\n",
    "                fill = self.batch_size - row_ptr\n",
    "                input_ids[row_ptr:] = input_ids[:fill]\n",
    "                values[row_ptr:]    = values[:fill]\n",
    "                attn[row_ptr:]      = attn[:fill]\n",
    "                y_batch[row_ptr:]   = y_batch[:fill]\n",
    "                smiles_batch[row_ptr:] = smiles_batch[:fill]\n",
    "                drug_id_batch[row_ptr:] = drug_id_batch[:fill]\n",
    "                cell_id_batch[row_ptr:] = cell_id_batch[:fill]\n",
    "\n",
    "            yield {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"values\": torch.tensor(values, dtype=torch.float32),\n",
    "                \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "                \"y_targets\": torch.tensor(y_batch, dtype=torch.float32),   # (B, M_TGT)\n",
    "                \"smiles_emb\": torch.tensor(smiles_batch, dtype=torch.float32),\n",
    "                \"drug_id\": torch.tensor(drug_id_batch, dtype=torch.long),\n",
    "                \"cell_id\": torch.tensor(cell_id_batch, dtype=torch.long),\n",
    "            }\n",
    "\n",
    "            cnt += 1\n",
    "            if cnt >= self.steps:\n",
    "                return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b721d693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded cell2id: 50 NUM_CELL_LINE: 50\n",
      "‚úÖ token_emb loaded: 4184/4184\n",
      "‚úÖ loaded cell embeddings: (50, 256)\n",
      "target_sub_ids: (278,)\n",
      "‚úÖ pos_weight(target-only): torch.Size([278]) | max= 16.217267990112305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aiffel/.cache/tmp/ipykernel_1229270/2984102383.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=USE_AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> TRAIN START (TARGET-ONLY + DROP FIRST TOKEN + NO SMILES MSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/10000 [00:00<?, ?it/s]/data/aiffel/.cache/tmp/ipykernel_1229270/2984102383.py:537: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=True):\n",
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [1:22:56<00:00,  2.01it/s, loss=5.4889, tgt=5.2317, sm=5.1434, rank(last)=5.4443, pos_in_batch=128/128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1/20] train_total=5.4889 | train_target=5.2317 | train_smiles=5.1434 | rank_last=5.4443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VALID metrics (NO MSE): {'Recall@5': 0.10979057348901107, 'Precision@5': 0.030690625000000017, 'Recall@10': 0.16622548363095244, 'Precision@10': 0.022860937500000008, 'SMILES_SupCon': 4.925717622756958, 'SMILES_COS': 0.6084446209669113}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/10000 [00:00<?, ?it/s]/data/aiffel/.cache/tmp/ipykernel_1229270/2984102383.py:537: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=True):\n",
      "Train:  23%|‚ñà‚ñà‚ñé       | 2297/10000 [17:55<1:06:12,  1.94it/s, loss=5.4352, tgt=5.1799, sm=5.1055, rank(last)=5.5646, pos_in_batch=128/128]Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "Traceback (most recent call last):\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000000865da7870009fece'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000000865a52410009fed0'\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000000865a522b0009fecf'\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000007652280b0009fed1'\n",
      "Train:  23%|‚ñà‚ñà‚ñé       | 2297/10000 [17:55<1:00:07,  2.14it/s, loss=5.4352, tgt=5.1799, sm=5.1055, rank(last)=5.5646, pos_in_batch=128/128]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 642\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>> TRAIN START (TARGET-ONLY + DROP FIRST TOKEN + NO SMILES MSE)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 642\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch_fixed_steps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS_PER_EPOCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_sub_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_sub_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    655\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_total=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_total\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    656\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_target=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_target\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    657\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_smiles=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_smiles\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank_last=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank_last\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    660\u001b[0m     valid_metrics \u001b[38;5;241m=\u001b[39m evaluate_fp_targets_and_smiles(\n\u001b[1;32m    661\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    662\u001b[0m         loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    667\u001b[0m         tau_smiles_eval\u001b[38;5;241m=\u001b[39mtau_smiles,\n\u001b[1;32m    668\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[3], line 567\u001b[0m, in \u001b[0;36mtrain_one_epoch_fixed_steps\u001b[0;34m(model, train_loader, device, steps_per_epoch, optimizer, scaler, target_sub_ids, pos_weight, log_every, grad_clip)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misfinite(loss)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    570\u001b[0m any_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 10.1) cell2id mapping + loaders\n",
    "# =========================================================\n",
    "cell2id_df = pd.read_csv(CELL2ID_CSV)\n",
    "cell2id_df[\"cell_line_id\"] = cell2id_df[\"cell_line_id\"].astype(str)\n",
    "cell2id_df[\"cell_id\"] = cell2id_df[\"cell_id\"].astype(int)\n",
    "\n",
    "cell_line2id = dict(zip(cell2id_df[\"cell_line_id\"], cell2id_df[\"cell_id\"]))\n",
    "NUM_CELL_LINE = int(cell2id_df[\"cell_id\"].max()) + 1\n",
    "print(\"loaded cell2id:\", len(cell_line2id), \"NUM_CELL_LINE:\", NUM_CELL_LINE)\n",
    "\n",
    "CAP_PER_PAIR_IN_RG = None\n",
    "\n",
    "train_ds = TahoeFPParquetDatasetMultiPair(\n",
    "    pair_to_locations=pair_to_locations,\n",
    "    pairs=train_pairs,\n",
    "    baseline_global=baseline_global,\n",
    "    baseline_by_cellline=baseline_by_cl,\n",
    "    drug_to_target_vec_target_only=drug_to_target_vec_tgt,\n",
    "    drug2id=drug2id,\n",
    "    drug_to_smiles_np=drug_to_smiles_np,\n",
    "    n_genes_full=N_GENES,\n",
    "    steps=STEPS_PER_EPOCH,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pairs_per_batch=PAIRS_PER_BATCH,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    pad_id=PAD_ID,\n",
    "    cls_id=CLS_ID,\n",
    "    celltok_id=CELLTOK_ID,\n",
    "    cell_line2id=cell_line2id,\n",
    "    unk_cell_id=0,\n",
    "    pair_weights=w_train,\n",
    "    seed=SEED,\n",
    "    cap_per_pair_in_rg=CAP_PER_PAIR_IN_RG,\n",
    "    max_tries_per_pair=20,\n",
    "    subset_token_ids_np=subset_token_ids_np,\n",
    "    old_tid_to_vocab_lut=old_tid_to_vocab_lut,\n",
    "    m_tgt=M_TGT,\n",
    "    drop_first_gene_token=DROP_FIRST_GENE_TOKEN,\n",
    ")\n",
    "\n",
    "val_ds = TahoeFPParquetDatasetMultiPair(\n",
    "    pair_to_locations=pair_to_locations,\n",
    "    pairs=val_pairs,\n",
    "    baseline_global=baseline_global,\n",
    "    baseline_by_cellline=baseline_by_cl,\n",
    "    drug_to_target_vec_target_only=drug_to_target_vec_tgt,\n",
    "    drug2id=drug2id,\n",
    "    drug_to_smiles_np=drug_to_smiles_np,\n",
    "    n_genes_full=N_GENES,\n",
    "    steps=VAL_STEPS,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pairs_per_batch=PAIRS_PER_BATCH,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    pad_id=PAD_ID,\n",
    "    cls_id=CLS_ID,\n",
    "    celltok_id=CELLTOK_ID,\n",
    "    cell_line2id=cell_line2id,\n",
    "    unk_cell_id=0,\n",
    "    pair_weights=w_val,\n",
    "    seed=SEED + 123,\n",
    "    cap_per_pair_in_rg=CAP_PER_PAIR_IN_RG,\n",
    "    max_tries_per_pair=20,\n",
    "    subset_token_ids_np=subset_token_ids_np,\n",
    "    old_tid_to_vocab_lut=old_tid_to_vocab_lut,\n",
    "    m_tgt=M_TGT,\n",
    "    drop_first_gene_token=DROP_FIRST_GENE_TOKEN,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=None,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=(NUM_WORKERS > 0),\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=None,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 11) Model\n",
    "# =========================================================\n",
    "class FPEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, num_layers, pad_id,\n",
    "                 max_len: int, num_cell_lines: int, cell_pos: int = 1):\n",
    "        super().__init__()\n",
    "        self.token_emb  = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.value_proj = nn.Linear(1, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.cell_line_emb = nn.Embedding(num_cell_lines, d_model)\n",
    "        self.cell_pos = int(cell_pos)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=4*d_model,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, input_ids, values, attention_mask, cell_line_id):\n",
    "        B, L = input_ids.shape\n",
    "        dev = input_ids.device\n",
    "\n",
    "        x = self.token_emb(input_ids) + self.value_proj(values.unsqueeze(-1))\n",
    "        pos = torch.arange(L, device=dev).unsqueeze(0).expand(B, L)\n",
    "        x = x + self.pos_emb(pos)\n",
    "\n",
    "        if cell_line_id is not None:\n",
    "            x[:, self.cell_pos, :] = x[:, self.cell_pos, :] + self.cell_line_emb(cell_line_id.to(dev)).to(x.dtype)\n",
    "\n",
    "        key_padding_mask = (attention_mask == 0)\n",
    "        h = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        return h[:, 0, :]\n",
    "\n",
    "\n",
    "class FPModelTied(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, num_layers, pad_id, smiles_dim,\n",
    "                 max_len: int, num_cell_lines: int):\n",
    "        super().__init__()\n",
    "        self.encoder = FPEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            num_layers=num_layers,\n",
    "            pad_id=pad_id,\n",
    "            max_len=max_len,\n",
    "            num_cell_lines=num_cell_lines,\n",
    "            cell_pos=1,\n",
    "        )\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.smiles_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.BatchNorm1d(4*d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(4*d_model, smiles_dim),\n",
    "            nn.LayerNorm(smiles_dim),\n",
    "        )\n",
    "\n",
    "    def gene_emb_subset(self):\n",
    "        return self.encoder.token_emb.weight[N_SPECIAL:, :]  # (M_SUB, d)\n",
    "\n",
    "    def forward(self, input_ids, values, attention_mask, cell_line_id, return_smiles=False):\n",
    "        h_cls = self.encoder(input_ids, values, attention_mask, cell_line_id)\n",
    "        v_pred = self.proj(h_cls)\n",
    "        z_pred = self.smiles_head(h_cls)\n",
    "        if return_smiles:\n",
    "            return v_pred, z_pred\n",
    "        return v_pred\n",
    "\n",
    "\n",
    "D_MODEL = 256\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 4\n",
    "\n",
    "model = FPModelTied(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    num_layers=N_LAYERS,\n",
    "    pad_id=PAD_ID,\n",
    "    smiles_dim=SMILES_DIM,\n",
    "    max_len=(2 + MAX_SEQ_LEN),\n",
    "    num_cell_lines=NUM_CELL_LINE\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 12) Load pretrained embeddings (subset + cell)\n",
    "# =========================================================\n",
    "def load_pretrained_subset_into_token_emb(token_emb: nn.Embedding, npy_path: str, device):\n",
    "    W = np.load(npy_path)  # (N_GENES, d)\n",
    "    Wt = torch.tensor(W, dtype=torch.float32, device=device)\n",
    "    d = token_emb.weight.shape[1]\n",
    "    if Wt.shape[1] != d:\n",
    "        raise ValueError(f\"d mismatch: npy={Wt.shape[1]} vs token_emb={d}\")\n",
    "\n",
    "    loaded = 0\n",
    "    with torch.no_grad():\n",
    "        for sid, old_tid in enumerate(subset_token_ids):\n",
    "            vid = N_SPECIAL + sid\n",
    "            if 0 <= old_tid < Wt.shape[0]:\n",
    "                token_emb.weight[vid].copy_(Wt[int(old_tid)])\n",
    "                loaded += 1\n",
    "    print(f\"‚úÖ token_emb loaded: {loaded}/{len(subset_token_ids)}\")\n",
    "\n",
    "load_pretrained_subset_into_token_emb(model.encoder.token_emb, PRETRAINED_GENE_NPY, device=device)\n",
    "\n",
    "def load_pretrained_cell_embeddings(cell_emb_layer, cell_emb_npy, device):\n",
    "    W = np.load(cell_emb_npy)\n",
    "    if W.shape != tuple(cell_emb_layer.weight.shape):\n",
    "        raise ValueError(f\"cell emb shape mismatch: npy={W.shape} vs layer={tuple(cell_emb_layer.weight.shape)}\")\n",
    "    with torch.no_grad():\n",
    "        cell_emb_layer.weight.copy_(torch.tensor(W, device=device, dtype=cell_emb_layer.weight.dtype))\n",
    "    print(f\"‚úÖ loaded cell embeddings: {W.shape}\")\n",
    "\n",
    "load_pretrained_cell_embeddings(model.encoder.cell_line_emb, CELL_EMB_NPY, device)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 12.1) TARGET-ONLY embedding bank indices inside SUBSET\n",
    "# =========================================================\n",
    "target_sub_ids = [old_tid_to_subid[tid] for tid in target_token_ids]\n",
    "target_sub_ids = torch.tensor(target_sub_ids, dtype=torch.long, device=device)\n",
    "print(\"target_sub_ids:\", tuple(target_sub_ids.shape))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 13) pos_weight (TARGET-ONLY)\n",
    "# =========================================================\n",
    "Y_list = [v for v in drug_to_target_vec_tgt.values() if float(np.sum(v)) > 0]\n",
    "Y_all = np.stack(Y_list, axis=0)\n",
    "\n",
    "pos = Y_all.sum(axis=0)\n",
    "neg = Y_all.shape[0] - pos\n",
    "pw = neg / (pos + 1e-6)\n",
    "\n",
    "if POSW_MODE == \"clip\":\n",
    "    pw = np.minimum(pw, POSW_MAX)\n",
    "elif POSW_MODE == \"sqrt_clip\":\n",
    "    pw = np.sqrt(pw)\n",
    "    pw = np.minimum(pw, POSW_MAX)\n",
    "\n",
    "pos_weight = torch.tensor(pw, dtype=torch.float32, device=device)\n",
    "print(\"‚úÖ pos_weight(target-only):\", pos_weight.shape, \"| max=\", float(pos_weight.max().item()))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 14) Losses\n",
    "# =========================================================\n",
    "def info_nce_ranking_loss_multi_pos(\n",
    "    v_pred: torch.Tensor,\n",
    "    gene_emb: torch.Tensor,\n",
    "    y_targets: torch.Tensor,\n",
    "    num_neg: int = 256,\n",
    "    num_pos: int = 8,\n",
    "    tau: float = 0.1,\n",
    "):\n",
    "    device_ = v_pred.device\n",
    "    B, _ = v_pred.shape\n",
    "    losses = []\n",
    "\n",
    "    for i in range(B):\n",
    "        pos_idx = (y_targets[i] > 0.5).nonzero(as_tuple=True)[0]\n",
    "        if pos_idx.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        neg_idx_all = (y_targets[i] < 0.5).nonzero(as_tuple=True)[0]\n",
    "        if neg_idx_all.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        if num_pos is not None and num_pos > 0 and pos_idx.numel() > num_pos:\n",
    "            pos_idx = pos_idx[torch.randperm(pos_idx.numel(), device=device_)[:num_pos]]\n",
    "\n",
    "        if neg_idx_all.numel() > num_neg:\n",
    "            neg_idx = neg_idx_all[torch.randperm(neg_idx_all.numel(), device=device_)[:num_neg]]\n",
    "        else:\n",
    "            neg_idx = neg_idx_all\n",
    "\n",
    "        pos_emb = gene_emb[pos_idx]\n",
    "        neg_emb = gene_emb[neg_idx]\n",
    "        cand_emb = torch.cat([pos_emb, neg_emb], dim=0)\n",
    "\n",
    "        v = v_pred[i].unsqueeze(0)\n",
    "        scores = F.cosine_similarity(v.expand_as(cand_emb), cand_emb, dim=-1) / tau\n",
    "\n",
    "        P = pos_emb.size(0)\n",
    "        logits = scores.unsqueeze(0).repeat(P, 1)\n",
    "        targets = torch.arange(P, device=device_, dtype=torch.long)\n",
    "        losses.append(F.cross_entropy(logits, targets))\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=device_)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "def bce_with_neg_sampling(\n",
    "    pred_vec: torch.Tensor,\n",
    "    y_targets: torch.Tensor,\n",
    "    gene_emb: torch.Tensor,\n",
    "    pos_weight_full: torch.Tensor,\n",
    "    num_neg: int = 2048,\n",
    "    pos_cap: int | None = None,\n",
    "):\n",
    "    device_ = pred_vec.device\n",
    "    B, _ = pred_vec.shape\n",
    "    losses = []\n",
    "\n",
    "    for i in range(B):\n",
    "        yi = y_targets[i]\n",
    "        pos_idx = (yi > 0.5).nonzero(as_tuple=True)[0]\n",
    "        if pos_idx.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        if (pos_cap is not None) and (pos_idx.numel() > pos_cap):\n",
    "            pos_idx = pos_idx[torch.randperm(pos_idx.numel(), device=device_)[:pos_cap]]\n",
    "\n",
    "        neg_idx_all = (yi < 0.5).nonzero(as_tuple=True)[0]\n",
    "        if neg_idx_all.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        k = min(int(num_neg), neg_idx_all.numel())\n",
    "        neg_idx = neg_idx_all[torch.randperm(neg_idx_all.numel(), device=device_)[:k]]\n",
    "\n",
    "        idx = torch.cat([pos_idx, neg_idx], dim=0)\n",
    "        logits = (pred_vec[i].unsqueeze(0) @ gene_emb[idx].T).squeeze(0)\n",
    "        y_sub = yi[idx]\n",
    "        pw_sub = pos_weight_full[idx]\n",
    "\n",
    "        losses.append(F.binary_cross_entropy_with_logits(logits, y_sub, pos_weight=pw_sub, reduction=\"mean\"))\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=device_)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "def combined_target_loss_neg_sampling_tied(\n",
    "    pred_vec: torch.Tensor,\n",
    "    y_targets: torch.Tensor,\n",
    "    gene_emb: torch.Tensor,\n",
    "    pos_weight: torch.Tensor,\n",
    "    lambda_cos: float = 1.0,\n",
    "    lambda_bce: float = 0.1,\n",
    "    lambda_rank: float = 0.5,\n",
    "    bce_num_neg: int = 2048,\n",
    "    bce_pos_cap: int | None = None,\n",
    "    rank_num_neg: int = 256,\n",
    "    rank_num_pos: int = 8,\n",
    "    tau: float = 0.2,\n",
    "):\n",
    "    device_ = pred_vec.device\n",
    "\n",
    "    true_vec = y_targets @ gene_emb\n",
    "    num_t = y_targets.sum(dim=1, keepdim=True)\n",
    "    mask = (num_t > 0).squeeze(1)\n",
    "\n",
    "    if mask.any():\n",
    "        true_vec_pos = true_vec[mask] / (num_t[mask] + 1e-6)\n",
    "        pred_pos = pred_vec[mask]\n",
    "        loss_cos = 1.0 - F.cosine_similarity(pred_pos, true_vec_pos, dim=-1).mean()\n",
    "    else:\n",
    "        loss_cos = torch.tensor(0.0, device=device_)\n",
    "\n",
    "    loss_bce = bce_with_neg_sampling(\n",
    "        pred_vec=pred_vec,\n",
    "        y_targets=y_targets,\n",
    "        gene_emb=gene_emb,\n",
    "        pos_weight_full=pos_weight,\n",
    "        num_neg=bce_num_neg,\n",
    "        pos_cap=bce_pos_cap,\n",
    "    )\n",
    "\n",
    "    loss_rank = info_nce_ranking_loss_multi_pos(\n",
    "        v_pred=pred_vec,\n",
    "        gene_emb=gene_emb,\n",
    "        y_targets=y_targets,\n",
    "        num_neg=rank_num_neg,\n",
    "        num_pos=rank_num_pos,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    loss = lambda_cos * loss_cos + lambda_bce * loss_bce + lambda_rank * loss_rank\n",
    "    return loss, loss_cos.detach(), loss_bce.detach(), loss_rank.detach()\n",
    "\n",
    "def supervised_contrastive_loss_smiles(z_pred, z_true, drug_ids, tau: float = 0.07, remove_diagonal: bool = True):\n",
    "    z_pred = F.normalize(z_pred, dim=1)\n",
    "    z_true = F.normalize(z_true, dim=1)\n",
    "\n",
    "    logits = (z_pred @ z_true.T) / tau\n",
    "    labels = drug_ids.view(-1, 1)\n",
    "    mask = torch.eq(labels, labels.T).float().to(z_pred.device)\n",
    "\n",
    "    if remove_diagonal:\n",
    "        mask.fill_diagonal_(0.0)\n",
    "\n",
    "    logits = logits - logits.max(dim=1, keepdim=True).values.detach()\n",
    "    log_prob = logits - torch.log(torch.exp(logits).sum(dim=1, keepdim=True) + 1e-6)\n",
    "\n",
    "    pos_cnt = mask.sum(dim=1)\n",
    "    valid = (pos_cnt > 0).float()\n",
    "\n",
    "    mean_log_prob_pos = (mask * log_prob).sum(dim=1) / (pos_cnt + 1e-6)\n",
    "    loss_per = -mean_log_prob_pos * valid\n",
    "    return loss_per.sum() / (valid.sum() + 1e-6)\n",
    "\n",
    "# ‚úÖ NO MSE: cosine align only\n",
    "def smiles_align_loss_cosine(z_pred: torch.Tensor, z_true: torch.Tensor):\n",
    "    return 1.0 - F.cosine_similarity(z_pred, z_true, dim=-1).mean()\n",
    "\n",
    "def smiles_mean_cosine(z_pred: torch.Tensor, z_true: torch.Tensor):\n",
    "    return F.cosine_similarity(z_pred, z_true, dim=-1).mean()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 15) Eval (NO SMILES MSE)\n",
    "# =========================================================\n",
    "def compute_recall_precision_at_k(scores: torch.Tensor, y_true: torch.Tensor, k: int = 20):\n",
    "    B, M = scores.shape\n",
    "    kk = min(k, M)\n",
    "    _, topk_idx = torch.topk(scores, k=kk, dim=1)\n",
    "\n",
    "    recalls, precisions = [], []\n",
    "    for i in range(B):\n",
    "        true_labels = y_true[i]\n",
    "        num_pos_ = true_labels.sum().item()\n",
    "        if num_pos_ == 0:\n",
    "            continue\n",
    "        topk = topk_idx[i]\n",
    "        num_pos_in_topk = true_labels[topk].sum().item()\n",
    "        recalls.append(num_pos_in_topk / max(num_pos_, 1e-6))\n",
    "        precisions.append(num_pos_in_topk / max(kk, 1))\n",
    "\n",
    "    if len(recalls) == 0:\n",
    "        return 0.0, 0.0\n",
    "    return float(sum(recalls) / len(recalls)), float(sum(precisions) / len(precisions))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_fp_targets_and_smiles(model, loader, device, target_sub_ids, k_list=(5,10), use_cosine=True, tau_smiles_eval=0.07):\n",
    "    model.eval()\n",
    "\n",
    "    gene_emb = model.gene_emb_subset()[target_sub_ids].to(device)  # (M_TGT, d)\n",
    "    if use_cosine:\n",
    "        g_norm = gene_emb / (gene_emb.norm(dim=1, keepdim=True) + 1e-8)\n",
    "    else:\n",
    "        g_norm = gene_emb\n",
    "\n",
    "    recall_sums = {k: 0.0 for k in k_list}\n",
    "    prec_sums   = {k: 0.0 for k in k_list}\n",
    "    counts_     = {k: 0   for k in k_list}\n",
    "\n",
    "    smiles_supcon_sum = 0.0\n",
    "    smiles_cos_sum = 0.0\n",
    "    n_smiles = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids   = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        values      = batch[\"values\"].to(device, non_blocking=True)\n",
    "        attn        = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        y_targets   = batch[\"y_targets\"].to(device, non_blocking=True)\n",
    "        smiles_true = batch[\"smiles_emb\"].to(device, non_blocking=True)\n",
    "        drug_ids    = batch[\"drug_id\"].to(device, non_blocking=True)\n",
    "        cell_id     = batch[\"cell_id\"].to(device, non_blocking=True)\n",
    "\n",
    "        v_pred, z_pred = model(input_ids, values, attn, cell_line_id=cell_id, return_smiles=True)\n",
    "\n",
    "        if use_cosine:\n",
    "            v_norm = v_pred / (v_pred.norm(dim=1, keepdim=True) + 1e-8)\n",
    "            scores = v_norm @ g_norm.T\n",
    "        else:\n",
    "            scores = v_pred @ g_norm.T\n",
    "\n",
    "        for k in k_list:\n",
    "            r, p = compute_recall_precision_at_k(scores, y_targets, k=k)\n",
    "            recall_sums[k] += r\n",
    "            prec_sums[k]   += p\n",
    "            counts_[k]     += 1\n",
    "\n",
    "        bs = smiles_true.size(0)\n",
    "        supcon = supervised_contrastive_loss_smiles(z_pred, smiles_true, drug_ids, tau=tau_smiles_eval)\n",
    "        smiles_supcon_sum += supcon.item() * bs\n",
    "\n",
    "        cos = smiles_mean_cosine(z_pred, smiles_true)\n",
    "        smiles_cos_sum += cos.item() * bs\n",
    "\n",
    "        n_smiles += bs\n",
    "\n",
    "    out = {}\n",
    "    for k in k_list:\n",
    "        out[f\"Recall@{k}\"] = recall_sums[k] / max(counts_[k], 1)\n",
    "        out[f\"Precision@{k}\"] = prec_sums[k] / max(counts_[k], 1)\n",
    "\n",
    "    out[\"SMILES_SupCon\"] = smiles_supcon_sum / max(n_smiles, 1)\n",
    "    out[\"SMILES_COS\"]    = smiles_cos_sum / max(n_smiles, 1)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 16) Train loop (NO SMILES MSE)\n",
    "# =========================================================\n",
    "def infinite_loader(loader):\n",
    "    while True:\n",
    "        for b in loader:\n",
    "            yield b\n",
    "\n",
    "USE_AMP = (device.type == \"cuda\")\n",
    "scaler = GradScaler(enabled=USE_AMP)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "def train_one_epoch_fixed_steps(\n",
    "    model,\n",
    "    train_loader,\n",
    "    device,\n",
    "    steps_per_epoch,\n",
    "    optimizer,\n",
    "    scaler,\n",
    "    target_sub_ids,\n",
    "    pos_weight,\n",
    "    log_every=50,\n",
    "    grad_clip=1.0,\n",
    "):\n",
    "    model.train()\n",
    "    it = infinite_loader(train_loader)\n",
    "\n",
    "    running_total = 0.0\n",
    "    running_tgt   = 0.0\n",
    "    running_sm    = 0.0\n",
    "    running_rank_last = 0.0\n",
    "    n = 0\n",
    "\n",
    "    pbar = tqdm(range(1, steps_per_epoch + 1), desc=\"Train\", leave=True, dynamic_ncols=True)\n",
    "\n",
    "    for step in pbar:\n",
    "        batch = next(it)\n",
    "\n",
    "        input_ids   = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        values      = batch[\"values\"].to(device, non_blocking=True)\n",
    "        attn        = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        y_targets   = batch[\"y_targets\"].to(device, non_blocking=True)\n",
    "        smiles_true = batch[\"smiles_emb\"].to(device, non_blocking=True)\n",
    "        drug_ids    = batch[\"drug_id\"].to(device, non_blocking=True)\n",
    "        cell_id     = batch[\"cell_id\"].to(device, non_blocking=True)\n",
    "\n",
    "        bs = input_ids.size(0)\n",
    "        n += bs\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if USE_AMP:\n",
    "            with autocast(enabled=True):\n",
    "                v_pred, z_pred = model(input_ids, values, attn, cell_line_id=cell_id, return_smiles=True)\n",
    "\n",
    "                gene_emb = model.gene_emb_subset()[target_sub_ids]  # (M_TGT, d)\n",
    "\n",
    "                loss_targets, loss_cos_t, loss_bce_t, loss_rank_t = combined_target_loss_neg_sampling_tied(\n",
    "                    pred_vec=v_pred,\n",
    "                    y_targets=y_targets,\n",
    "                    gene_emb=gene_emb,\n",
    "                    pos_weight=pos_weight,\n",
    "                    lambda_cos=lambda_cos,\n",
    "                    lambda_bce=lambda_bce,\n",
    "                    lambda_rank=lambda_rank,\n",
    "                    bce_num_neg=bce_num_neg,\n",
    "                    bce_pos_cap=bce_pos_cap,\n",
    "                    rank_num_neg=rank_num_neg,\n",
    "                    rank_num_pos=rank_num_pos,\n",
    "                    tau=0.2,\n",
    "                )\n",
    "\n",
    "                # ‚úÖ SMILES: SupCon + cosine-align only (NO MSE)\n",
    "                loss_supcon = supervised_contrastive_loss_smiles(z_pred, smiles_true, drug_ids, tau=tau_smiles)\n",
    "                loss_align  = smiles_align_loss_cosine(z_pred, smiles_true)\n",
    "                loss_smiles = loss_supcon + alpha_align * loss_align\n",
    "\n",
    "                loss = loss_targets + lambda_smiles * loss_smiles\n",
    "\n",
    "            if not torch.isfinite(loss).all():\n",
    "                continue\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "\n",
    "            any_grad = any(p.grad is not None for p in model.parameters())\n",
    "            if not any_grad:\n",
    "                scaler.update()\n",
    "                continue\n",
    "\n",
    "            if grad_clip is not None and grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        else:\n",
    "            v_pred, z_pred = model(input_ids, values, attn, cell_line_id=cell_id, return_smiles=True)\n",
    "            gene_emb = model.gene_emb_subset()[target_sub_ids]\n",
    "\n",
    "            loss_targets, loss_cos_t, loss_bce_t, loss_rank_t = combined_target_loss_neg_sampling_tied(\n",
    "                pred_vec=v_pred,\n",
    "                y_targets=y_targets,\n",
    "                gene_emb=gene_emb,\n",
    "                pos_weight=pos_weight,\n",
    "                lambda_cos=lambda_cos,\n",
    "                lambda_bce=lambda_bce,\n",
    "                lambda_rank=lambda_rank,\n",
    "                bce_num_neg=bce_num_neg,\n",
    "                bce_pos_cap=bce_pos_cap,\n",
    "                rank_num_neg=rank_num_neg,\n",
    "                rank_num_pos=rank_num_pos,\n",
    "                tau=0.1,\n",
    "            )\n",
    "\n",
    "            loss_supcon = supervised_contrastive_loss_smiles(z_pred, smiles_true, drug_ids, tau=tau_smiles)\n",
    "            loss_align  = smiles_align_loss_cosine(z_pred, smiles_true)\n",
    "            loss_smiles = loss_supcon + alpha_align * loss_align\n",
    "            loss = loss_targets + lambda_smiles * loss_smiles\n",
    "\n",
    "            if not torch.isfinite(loss).all():\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            if grad_clip is not None and grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_total += float(loss.item()) * bs\n",
    "        running_tgt   += float(loss_targets.item()) * bs\n",
    "        running_sm    += float(loss_smiles.item()) * bs\n",
    "        running_rank_last = float(loss_rank_t.item())\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            pos_in_batch = int((y_targets.sum(dim=1) > 0).sum().item())\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{running_total/max(n,1):.4f}\",\n",
    "                \"tgt\":  f\"{running_tgt/max(n,1):.4f}\",\n",
    "                \"sm\":   f\"{running_sm/max(n,1):.4f}\",\n",
    "                \"rank(last)\": f\"{running_rank_last:.4f}\",\n",
    "                \"pos_in_batch\": f\"{pos_in_batch}/{bs}\",\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"train_total\":  running_total / max(n, 1),\n",
    "        \"train_target\": running_tgt   / max(n, 1),\n",
    "        \"train_smiles\": running_sm    / max(n, 1),\n",
    "        \"rank_last\":    running_rank_last,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 17) TRAIN\n",
    "# =========================================================\n",
    "print(\">>> TRAIN START (TARGET-ONLY + DROP FIRST TOKEN + NO SMILES MSE)\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    logs = train_one_epoch_fixed_steps(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        device=device,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        optimizer=optimizer,\n",
    "        scaler=scaler,\n",
    "        target_sub_ids=target_sub_ids,\n",
    "        pos_weight=pos_weight,\n",
    "        log_every=50,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[Epoch {epoch}/{EPOCHS}] \"\n",
    "          f\"train_total={logs['train_total']:.4f} | \"\n",
    "          f\"train_target={logs['train_target']:.4f} | \"\n",
    "          f\"train_smiles={logs['train_smiles']:.4f} | \"\n",
    "          f\"rank_last={logs['rank_last']:.4f}\")\n",
    "\n",
    "    valid_metrics = evaluate_fp_targets_and_smiles(\n",
    "        model=model,\n",
    "        loader=val_loader,\n",
    "        device=device,\n",
    "        target_sub_ids=target_sub_ids,\n",
    "        k_list=(5, 10),\n",
    "        use_cosine=True,\n",
    "        tau_smiles_eval=tau_smiles,\n",
    "    )\n",
    "    print(\"‚úÖ VALID metrics (NO MSE):\", valid_metrics)\n",
    "\n",
    "    if (epoch % SAVE_EVERY == 0) or (epoch == EPOCHS):\n",
    "        save_checkpoint(\n",
    "            save_dir=CKPT_DIR,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "            epoch=epoch,\n",
    "            metrics={\"train\": logs, \"valid\": valid_metrics},\n",
    "            prefix=\"fp_target_only\",\n",
    "        )\n",
    "\n",
    "print(\">>> DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb91ea0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ORACLE sanity: {'Oracle_Recall@5': 0.9872271825396826, 'Oracle_Recall@10': 0.9991666666666668, 'Oracle_Precision@5': 0.36312500000000003, 'Oracle_Precision@10': 0.1912500000000001}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Oracle_Recall@5': 0.9872271825396826,\n",
       " 'Oracle_Recall@10': 0.9991666666666668,\n",
       " 'Oracle_Precision@5': 0.36312500000000003,\n",
       " 'Oracle_Precision@10': 0.1912500000000001}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def sanity_check_oracle_recall(val_loader, model, device, target_sub_ids, k_list=(5,10)):\n",
    "    model.eval()\n",
    "    # gene bank for targets (M_TGT, d)\n",
    "    gene_emb = model.gene_emb_subset()[target_sub_ids].to(device)\n",
    "    g_norm = gene_emb / (gene_emb.norm(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "    recall_sums = {k: 0.0 for k in k_list}\n",
    "    prec_sums   = {k: 0.0 for k in k_list}\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in val_loader:\n",
    "        y = batch[\"y_targets\"].to(device)  # (B, M_TGT)\n",
    "        if y.ndim != 2 or y.size(1) != g_norm.size(0):\n",
    "            print(\"‚ùå SHAPE MISMATCH:\", y.shape, \"vs gene_bank:\", g_norm.shape)\n",
    "            return None\n",
    "\n",
    "        # oracle vector: mean of true target embeddings\n",
    "        true_vec = y @ g_norm                      # (B, d)\n",
    "        denom = y.sum(dim=1, keepdim=True).clamp(min=1.0)\n",
    "        v_oracle = true_vec / denom                # (B, d)\n",
    "        v_norm = v_oracle / (v_oracle.norm(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "        scores = v_norm @ g_norm.T                 # (B, M_TGT)\n",
    "\n",
    "        for k in k_list:\n",
    "            r, p = compute_recall_precision_at_k(scores, y, k=k)\n",
    "            recall_sums[k] += r\n",
    "            prec_sums[k]   += p\n",
    "\n",
    "        n_batches += 1\n",
    "        if n_batches >= 20:  # –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ 20 –±–∞—Ç—á–µ–π\n",
    "            break\n",
    "\n",
    "    out = {f\"Oracle_Recall@{k}\": recall_sums[k]/max(n_batches,1) for k in k_list}\n",
    "    out.update({f\"Oracle_Precision@{k}\": prec_sums[k]/max(n_batches,1) for k in k_list})\n",
    "    print(\"‚úÖ ORACLE sanity:\", out)\n",
    "    return out\n",
    "\n",
    "# –∑–∞–ø—É—Å—Ç–∏\n",
    "sanity_check_oracle_recall(val_loader, model, device, target_sub_ids, k_list=(5,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6b8e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drugs total=379, with>=1 target=264\n",
      "HVG token_ids: 4000\n",
      "SUBSET (HVG ‚à™ TARGETS) size: 4184\n",
      "TARGET-ONLY size: 278\n",
      "VOCAB_SIZE: 4190\n",
      "drugs with>=1 target (target-only vec): 264\n",
      "‚úÖ SMILES aligned | missing=0/379\n",
      "baseline_global: (62713,) baseline_by_cl: 50\n",
      "train pairs: 10505\n",
      "val pairs: 1168\n",
      "parquet files found: 3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Index parquet row-groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3388/3388 [11:53<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexed pairs: 11673\n",
      "loaded cell2id: 50 NUM_CELL_LINE: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aiffel/.cache/tmp/ipykernel_1229270/2194420474.py:1295: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=USE_AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ token_emb loaded: 4184/4184\n",
      "‚úÖ loaded cell embeddings: (50, 256)\n",
      "target_sub_ids: (278,)\n",
      "‚úÖ pos_weight(target-only): torch.Size([278]) | max= 1.0\n",
      ">>> TRAIN START (TARGET-ONLY + DROP FIRST TOKEN + NO SMILES MSE + COSINE-BCE + STRONG RANK)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/10000 [00:00<?, ?it/s]/data/aiffel/.cache/tmp/ipykernel_1229270/2194420474.py:1338: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=True):\n",
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [1:31:27<00:00,  1.82it/s, loss=6.4862, tgt=6.2293, sm=5.1380, rank(last)=5.4741, pos_in_batch=128/128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1/20] train_total=6.4862 | train_target=6.2293 | train_smiles=5.1380 | rank_last=5.4741\n",
      "‚úÖ VALID metrics: {'Recall@5': 0.10973453525641032, 'Precision@5': 0.029868750000000003, 'Recall@10': 0.16933756772741157, 'Precision@10': 0.0232609375, 'SMILES_SupCon': 4.930289658546448, 'SMILES_COS': 0.6480854507684708}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/10000 [00:00<?, ?it/s]/data/aiffel/.cache/tmp/ipykernel_1229270/2194420474.py:1338: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=True):\n",
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [1:21:03<00:00,  2.06it/s, loss=6.3893, tgt=6.1348, sm=5.0901, rank(last)=5.2580, pos_in_batch=128/128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 2/20] train_total=6.3893 | train_target=6.1348 | train_smiles=5.0901 | rank_last=5.2580\n",
      "‚úÖ VALID metrics: {'Recall@5': 0.11743688663766795, 'Precision@5': 0.031562499999999986, 'Recall@10': 0.18473509758470694, 'Precision@10': 0.024776562499999977, 'SMILES_SupCon': 4.9025642166137695, 'SMILES_COS': 0.6413964116573334}\n",
      "üíæ checkpoint saved: /data/aiffel/babayakga/checkpoints/fp_target_only_cosbce_rank1_drop1/fp_target_only_epoch002.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/10000 [00:00<?, ?it/s]/data/aiffel/.cache/tmp/ipykernel_1229270/2194420474.py:1338: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=True):\n",
      "Train:   1%|‚ñè         | 149/10000 [01:08<1:15:03,  2.19it/s, loss=6.3315, tgt=6.0784, sm=5.0627, rank(last)=5.2791, pos_in_batch=128/128]Traceback (most recent call last):\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000000965316a20009feeb'\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000009653168b0009feec'\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000000865a52480009feed'\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000000865a524a0009feee'\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1445\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>> TRAIN START (TARGET-ONLY + DROP FIRST TOKEN + NO SMILES MSE + COSINE-BCE + STRONG RANK)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m-> 1445\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch_fixed_steps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS_PER_EPOCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_sub_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_sub_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1457\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1458\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_total=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_total\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1459\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_target=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_target\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1460\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_smiles=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_smiles\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1461\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank_last=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank_last\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1463\u001b[0m     valid_metrics \u001b[38;5;241m=\u001b[39m evaluate_fp_targets_and_smiles(\n\u001b[1;32m   1464\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1465\u001b[0m         loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m         tau_smiles_eval\u001b[38;5;241m=\u001b[39mtau_smiles,\n\u001b[1;32m   1470\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[5], line 1343\u001b[0m, in \u001b[0;36mtrain_one_epoch_fixed_steps\u001b[0;34m(model, train_loader, device, steps_per_epoch, optimizer, scaler, target_sub_ids, pos_weight, log_every, grad_clip)\u001b[0m\n\u001b[1;32m   1339\u001b[0m v_pred, z_pred \u001b[38;5;241m=\u001b[39m model(input_ids, values, attn, cell_line_id\u001b[38;5;241m=\u001b[39mcell_id, return_smiles\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1341\u001b[0m gene_emb \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgene_emb_subset()[target_sub_ids]  \u001b[38;5;66;03m# (M_TGT, d)\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m loss_targets, loss_cos_t, loss_bce_t, loss_rank_t \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_target_loss_neg_sampling_tied\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_vec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgene_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgene_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_cos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_cos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_bce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_bce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbce_num_neg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbce_num_neg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbce_pos_cap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbce_pos_cap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrank_num_neg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank_num_neg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrank_num_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank_num_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtau_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtau_bce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau_bce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# ‚úÖ SMILES: SupCon + cosine-align only (NO MSE)\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m loss_supcon \u001b[38;5;241m=\u001b[39m supervised_contrastive_loss_smiles(z_pred, smiles_true, drug_ids, tau\u001b[38;5;241m=\u001b[39mtau_smiles)\n",
      "Cell \u001b[0;32mIn[5], line 1156\u001b[0m, in \u001b[0;36mcombined_target_loss_neg_sampling_tied\u001b[0;34m(pred_vec, y_targets, gene_emb, pos_weight, lambda_cos, lambda_bce, lambda_rank, bce_num_neg, bce_pos_cap, rank_num_neg, rank_num_pos, tau_rank, tau_bce)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     loss_cos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice_)\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;66;03m# BCE with cosine logits\u001b[39;00m\n\u001b[0;32m-> 1156\u001b[0m loss_bce \u001b[38;5;241m=\u001b[39m \u001b[43mbce_with_neg_sampling_cosine\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_vec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgene_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgene_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# internally normalized\u001b[39;49;00m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_weight_full\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_neg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbce_num_neg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_cap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbce_pos_cap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtau_bce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau_bce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;66;03m# rank (InfoNCE) with cosine logits\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m loss_rank \u001b[38;5;241m=\u001b[39m info_nce_ranking_loss_multi_pos(\n\u001b[1;32m   1168\u001b[0m     v_pred\u001b[38;5;241m=\u001b[39mpred_vec,\n\u001b[1;32m   1169\u001b[0m     gene_emb\u001b[38;5;241m=\u001b[39mgene_emb,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     tau\u001b[38;5;241m=\u001b[39mtau_rank,\n\u001b[1;32m   1174\u001b[0m )\n",
      "Cell \u001b[0;32mIn[5], line 1115\u001b[0m, in \u001b[0;36mbce_with_neg_sampling_cosine\u001b[0;34m(pred_vec, y_targets, gene_emb, pos_weight_full, num_neg, pos_cap, tau_bce)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     y_sub \u001b[38;5;241m=\u001b[39m yi[idx]\n\u001b[1;32m   1113\u001b[0m     pw_sub \u001b[38;5;241m=\u001b[39m pos_weight_full[idx]\n\u001b[0;32m-> 1115\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpw_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(losses) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice_)\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/nn/functional.py:3529\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3524\u001b[0m         weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m   3526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight, reduction_enum)\n\u001b[0;32m-> 3529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbinary_cross_entropy_with_logits\u001b[39m(\n\u001b[1;32m   3530\u001b[0m     \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m   3531\u001b[0m     target: Tensor,\n\u001b[1;32m   3532\u001b[0m     weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3533\u001b[0m     size_average: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3534\u001b[0m     reduce: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3535\u001b[0m     reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3536\u001b[0m     pos_weight: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3537\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m   3538\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute Binary Cross Entropy between target and input logits.\u001b[39;00m\n\u001b[1;32m   3539\u001b[0m \n\u001b[1;32m   3540\u001b[0m \u001b[38;5;124;03m    See :class:`~torch.nn.BCEWithLogitsLoss` for details.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3569\u001b[0m \u001b[38;5;124;03m         >>> loss.backward()\u001b[39;00m\n\u001b[1;32m   3570\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# FP (TARGET-ONLY) TRAINING SCRIPT\n",
    "# - ‚úÖ TARGET-ONLY (M_TGT=~280)\n",
    "# - ‚úÖ DROP FIRST \"service\" element in genes/expr\n",
    "# - ‚úÖ NO SMILES MSE (SupCon + cosine-align only)\n",
    "# - ‚úÖ FIX: BCE now uses COSINE logits (same geometry as eval/rank)\n",
    "# - ‚úÖ Recommended: lower BCE weight, higher Rank weight\n",
    "# =========================================================\n",
    "\n",
    "import os, glob, ast, random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import scanpy as sc\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 0) PATHS / HYPERPARAMS\n",
    "# =========================================================\n",
    "PARQUET_DIR    = \"/data/aiffel/data/Tahoe-100M/data\"\n",
    "GENE_META_PATH = \"/data/aiffel/data/Tahoe-100M/metadata/gene_metadata.parquet\"\n",
    "DRUG_META_PATH = \"/data/aiffel/data/Tahoe-100M/metadata/drug_metadata.parquet\"\n",
    "COUNTS_CSV     = \"/data/aiffel/babayakga/making_data/aiffel/babayakga/making_data/tahoe_counts_per_drug_cell_line.csv\"\n",
    "DMSO_PATH      = \"/data/aiffel/babayakga/outputs/dmso.h5ad\"\n",
    "\n",
    "SMILES_EMB_PATH       = \"/data/aiffel/babayakga/smiles_emb/drug_smiles_emb_all1.pt\"\n",
    "PRETRAINED_GENE_NPY   = \"/data/aiffel/babayakga/pretraining/checkpoints_with_cell/gene_embeddings.npy\"\n",
    "CELL2ID_CSV           = \"/data/aiffel/babayakga/pretraining/checkpoints_with_cell/cell2id.csv\"\n",
    "CELL_EMB_NPY          = \"/data/aiffel/babayakga/pretraining/checkpoints_with_cell/cell_embeddings.npy\"\n",
    "\n",
    "CONTROL_DRUG = \"DMSO_TF\"\n",
    "SEED = 42\n",
    "\n",
    "MAX_SEQ_LEN = 256\n",
    "BATCH_SIZE  = 128\n",
    "STEPS_PER_EPOCH = 10000\n",
    "VAL_STEPS       = 500\n",
    "\n",
    "# ‚úÖ weights (recommendation to make Recall move)\n",
    "lambda_cos    = 1.0\n",
    "lambda_bce    = 0.05   # ‚¨á\n",
    "lambda_rank   = 1.0    # ‚¨Ü\n",
    "\n",
    "lambda_smiles = 0.05\n",
    "alpha_align   = 0.5\n",
    "tau_smiles    = 0.07\n",
    "\n",
    "# ‚úÖ ranking strength\n",
    "rank_num_neg = 1024\n",
    "rank_num_pos = 4\n",
    "tau_rank     = 0.07\n",
    "\n",
    "# ‚úÖ BCE sampling\n",
    "bce_num_neg  = 2048\n",
    "bce_pos_cap  = None\n",
    "tau_bce      = 0.10    # cosine logits temperature\n",
    "\n",
    "EPOCHS = 20\n",
    "LR = 1e-5\n",
    "\n",
    "# ‚úÖ pos_weight: temporarily off (often helps Recall start moving)\n",
    "POSW_MODE = \"none\"     # [\"none\", \"clip\", \"sqrt_clip\"]\n",
    "POSW_MAX  = 10.0       # if you later use clip/sqrt_clip\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "HVG_K = 4000\n",
    "PAIRS_PER_BATCH = 16\n",
    "\n",
    "# ‚úÖ drop first \"service\" element from genes/expressions\n",
    "DROP_FIRST_GENE_TOKEN = True\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ‚úÖ TF32 safe speed-up on Ampere+\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# (extra) checkpoint save\n",
    "# =========================================================\n",
    "CKPT_DIR = \"/data/aiffel/babayakga/checkpoints/fp_target_only_cosbce_rank1_drop1\"\n",
    "SAVE_EVERY = 2\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(save_dir, model, optimizer, scaler, epoch, metrics=None, prefix=\"fp\"):\n",
    "    ckpt = {\n",
    "        \"epoch\": int(epoch),\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scaler_state_dict\": scaler.state_dict() if scaler is not None else None,\n",
    "        \"metrics\": metrics if metrics is not None else {},\n",
    "    }\n",
    "    path = os.path.join(save_dir, f\"{prefix}_epoch{epoch:03d}.pt\")\n",
    "    torch.save(ckpt, path)\n",
    "    print(f\"üíæ checkpoint saved: {path}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1) gene_metadata load\n",
    "# =========================================================\n",
    "gene_md = pd.read_parquet(GENE_META_PATH).copy()\n",
    "gene_md[\"gene_symbol\"] = gene_md[\"gene_symbol\"].astype(str)\n",
    "gene_md[\"ensembl_id\"]  = gene_md[\"ensembl_id\"].astype(str)\n",
    "gene_md[\"token_id\"]    = gene_md[\"token_id\"].astype(int)\n",
    "gene_md = gene_md.sort_values(\"token_id\").reset_index(drop=True)\n",
    "\n",
    "N_GENES = int(gene_md[\"token_id\"].max()) + 1\n",
    "symbol_to_ensg_lower = dict(zip(gene_md[\"gene_symbol\"].str.lower(), gene_md[\"ensembl_id\"]))\n",
    "ensg_to_token_id = dict(zip(gene_md[\"ensembl_id\"].values, gene_md[\"token_id\"].values))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) drug_metadata -> targets\n",
    "# =========================================================\n",
    "def parse_targets(x):\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return [str(t).strip() for t in x if str(t).strip()]\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "            try:\n",
    "                out = ast.literal_eval(s)\n",
    "                if isinstance(out, (list, tuple)):\n",
    "                    return [str(t).strip() for t in out if str(t).strip()]\n",
    "            except Exception:\n",
    "                pass\n",
    "        for sep in [\";\", \",\"]:\n",
    "            if sep in s:\n",
    "                return [t.strip() for t in s.split(sep) if t.strip()]\n",
    "        return [s]\n",
    "    return [str(x).strip()]\n",
    "\n",
    "drug_meta_df = pd.read_parquet(DRUG_META_PATH).copy()\n",
    "drug_meta_df[\"drug\"] = drug_meta_df[\"drug\"].astype(str)\n",
    "\n",
    "drug_to_target_tokenids = {}\n",
    "all_target_tokenids = set()\n",
    "\n",
    "for _, row in drug_meta_df.iterrows():\n",
    "    drug = str(row[\"drug\"])\n",
    "    targets = parse_targets(row.get(\"targets\", None))\n",
    "\n",
    "    tids = []\n",
    "    for t in targets:\n",
    "        t = str(t).strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        if t.startswith(\"ENSG\"):\n",
    "            ensg = t\n",
    "        else:\n",
    "            ensg = symbol_to_ensg_lower.get(t.lower(), None)\n",
    "        if ensg is None:\n",
    "            continue\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        tid = int(tid)\n",
    "        if 0 <= tid < N_GENES:\n",
    "            tids.append(tid)\n",
    "\n",
    "    tids = sorted(set(tids))\n",
    "    drug_to_target_tokenids[drug] = tids\n",
    "    all_target_tokenids.update(tids)\n",
    "\n",
    "drug_has_targets = {d: (len(tids) > 0) for d, tids in drug_to_target_tokenids.items()}\n",
    "print(f\"drugs total={len(drug_to_target_tokenids)}, with>=1 target={sum(drug_has_targets.values())}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3) HVG from DMSO\n",
    "# =========================================================\n",
    "def compute_hvg_token_ids_from_dmso(dmso_h5ad_path: str, control_drug: str, HVG_K: int, ensg_to_token_id: dict):\n",
    "    ad = sc.read_h5ad(dmso_h5ad_path)\n",
    "    obs = ad.obs\n",
    "    m = (obs[\"drug\"].astype(str).values == control_drug)\n",
    "    idx = np.where(m)[0]\n",
    "    if idx.size == 0:\n",
    "        raise ValueError(f\"DMSO adata ÏïàÏóê control drug({control_drug})Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "    X = ad.X.tocsr() if sparse.issparse(ad.X) else sparse.csr_matrix(ad.X)\n",
    "    Xc = X[idx]\n",
    "\n",
    "    mean = np.asarray(Xc.mean(axis=0)).ravel()\n",
    "    mean2 = np.asarray(Xc.multiply(Xc).mean(axis=0)).ravel()\n",
    "    var = (mean2 - mean**2).astype(np.float32)\n",
    "\n",
    "    ensgs = ad.var_names.astype(str).tolist()\n",
    "\n",
    "    token_ids = []\n",
    "    vars_ = []\n",
    "    for j, ensg in enumerate(ensgs):\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        token_ids.append(int(tid))\n",
    "        vars_.append(float(var[j]))\n",
    "\n",
    "    token_ids = np.asarray(token_ids, dtype=np.int64)\n",
    "    vars_ = np.asarray(vars_, dtype=np.float32)\n",
    "\n",
    "    if token_ids.size == 0:\n",
    "        raise ValueError(\"DMSO var_namesÏôÄ gene_metadataÏùò ENSG mappingÏù¥ Í±∞Ïùò Ïïà ÎßûÏäµÎãàÎã§.\")\n",
    "\n",
    "    k = min(int(HVG_K), token_ids.size)\n",
    "    top = np.argpartition(-vars_, k-1)[:k]\n",
    "    return set(token_ids[top].tolist())\n",
    "\n",
    "hvg_token_ids = compute_hvg_token_ids_from_dmso(\n",
    "    dmso_h5ad_path=DMSO_PATH,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    HVG_K=HVG_K,\n",
    "    ensg_to_token_id=ensg_to_token_id,\n",
    ")\n",
    "print(\"HVG token_ids:\", len(hvg_token_ids))\n",
    "\n",
    "# INPUT subset = HVG ‚à™ TARGETS\n",
    "subset_token_ids = sorted(set(hvg_token_ids) | set(all_target_tokenids))\n",
    "M_SUB = len(subset_token_ids)\n",
    "print(\"SUBSET (HVG ‚à™ TARGETS) size:\", M_SUB)\n",
    "\n",
    "# OUTPUT target-only = TARGETS only\n",
    "target_token_ids = sorted(set(all_target_tokenids))\n",
    "M_TGT = len(target_token_ids)\n",
    "print(\"TARGET-ONLY size:\", M_TGT)\n",
    "\n",
    "old_tid_to_subid = {tid: i for i, tid in enumerate(subset_token_ids)}\n",
    "old_tid_to_tgtid = {tid: i for i, tid in enumerate(target_token_ids)}\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4) subset vocab + LUT (fast mapping)\n",
    "# =========================================================\n",
    "SPECIAL_TOKENS = [\"[PAD]\", \"[CLS]\", \"[DRUG]\", \"[TARGET]\", \"[CELL]\", \"[MASK]\"]\n",
    "local_token_to_id = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
    "N_SPECIAL = len(SPECIAL_TOKENS)\n",
    "\n",
    "VOCAB_SIZE = N_SPECIAL + M_SUB\n",
    "PAD_ID = local_token_to_id[\"[PAD]\"]\n",
    "CLS_ID = local_token_to_id[\"[CLS]\"]\n",
    "CELLTOK_ID = local_token_to_id[\"[CELL]\"]\n",
    "\n",
    "print(\"VOCAB_SIZE:\", VOCAB_SIZE)\n",
    "\n",
    "old_tid_to_vocab_lut = np.full((N_GENES,), -1, dtype=np.int64)\n",
    "for sid, old_tid in enumerate(subset_token_ids):\n",
    "    if 0 <= old_tid < N_GENES:\n",
    "        old_tid_to_vocab_lut[old_tid] = N_SPECIAL + sid\n",
    "\n",
    "subset_token_ids_np = np.asarray(subset_token_ids, dtype=np.int64)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5) y_targets (drug -> TARGET-ONLY multi-hot)\n",
    "# =========================================================\n",
    "drug_to_target_vec_tgt = {}\n",
    "for d, tids in drug_to_target_tokenids.items():\n",
    "    vec = np.zeros(M_TGT, dtype=np.float32)\n",
    "    for tid in tids:\n",
    "        j = old_tid_to_tgtid.get(int(tid), None)\n",
    "        if j is not None:\n",
    "            vec[j] = 1.0\n",
    "    drug_to_target_vec_tgt[d] = vec\n",
    "\n",
    "n_nonzero = sum(float(v.sum()) > 0 for v in drug_to_target_vec_tgt.values())\n",
    "print(\"drugs with>=1 target (target-only vec):\", n_nonzero)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6) SMILES embeddings\n",
    "# =========================================================\n",
    "obj = torch.load(SMILES_EMB_PATH, map_location=\"cpu\")\n",
    "assert isinstance(obj, dict) and \"drug\" in obj and \"emb\" in obj\n",
    "\n",
    "drug_list_saved = [str(d) for d in obj[\"drug\"]]\n",
    "emb_matrix = obj[\"emb\"].to(dtype=torch.float32).cpu().numpy()\n",
    "SMILES_DIM = int(emb_matrix.shape[1])\n",
    "\n",
    "drug_to_smiles_np_raw = {d: emb_matrix[i].astype(np.float32, copy=False) for i, d in enumerate(drug_list_saved)}\n",
    "\n",
    "drug_names = drug_meta_df[\"drug\"].astype(str).tolist()\n",
    "drug_to_smiles_np = {}\n",
    "missing = 0\n",
    "for d in drug_names:\n",
    "    v = drug_to_smiles_np_raw.get(d, None)\n",
    "    if v is None:\n",
    "        drug_to_smiles_np[d] = np.zeros((SMILES_DIM,), dtype=np.float32)\n",
    "        missing += 1\n",
    "    else:\n",
    "        drug_to_smiles_np[d] = v\n",
    "print(f\"‚úÖ SMILES aligned | missing={missing}/{len(drug_names)}\")\n",
    "\n",
    "drug2id = {d: i for i, d in enumerate(sorted(set(drug_names)))}\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7) DMSO baselines (gene-space)\n",
    "# =========================================================\n",
    "def build_dmso_baselines_gene_space(dmso_h5ad_path: str, control_drug: str, N_GENES: int, ensg_to_token_id: dict):\n",
    "    adata = sc.read_h5ad(dmso_h5ad_path)\n",
    "    obs = adata.obs\n",
    "    X = adata.X.tocsr() if sparse.issparse(adata.X) else sparse.csr_matrix(adata.X)\n",
    "\n",
    "    m = (obs[\"drug\"].astype(str).values == control_drug)\n",
    "    idx = np.where(m)[0]\n",
    "    if idx.size == 0:\n",
    "        raise ValueError(f\"DMSO adata ÏïàÏóê control drug({control_drug})Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "    ensgs = adata.var_names.astype(str).tolist()\n",
    "\n",
    "    token_ids, cols = [], []\n",
    "    for j, ensg in enumerate(ensgs):\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        token_ids.append(int(tid))\n",
    "        cols.append(j)\n",
    "\n",
    "    token_ids = np.asarray(token_ids, dtype=np.int64)\n",
    "    cols = np.asarray(cols, dtype=np.int64)\n",
    "\n",
    "    Xc = X[idx][:, cols]\n",
    "    mean_global_sub = np.asarray(Xc.mean(axis=0)).ravel().astype(np.float32)\n",
    "\n",
    "    baseline_global = np.zeros(N_GENES, dtype=np.float32)\n",
    "    baseline_global[token_ids] = mean_global_sub\n",
    "\n",
    "    baseline_by_cl = {}\n",
    "    cl_values = obs[\"cell_line_id\"].astype(str).values\n",
    "    for cl in np.unique(cl_values):\n",
    "        cl_idx = np.where(m & (cl_values == cl))[0]\n",
    "        if cl_idx.size == 0:\n",
    "            continue\n",
    "        Xcl = X[cl_idx][:, cols]\n",
    "        mean_cl_sub = np.asarray(Xcl.mean(axis=0)).ravel().astype(np.float32)\n",
    "        v = np.zeros(N_GENES, dtype=np.float32)\n",
    "        v[token_ids] = mean_cl_sub\n",
    "        baseline_by_cl[str(cl)] = v\n",
    "\n",
    "    return baseline_global, baseline_by_cl\n",
    "\n",
    "baseline_global, baseline_by_cl = build_dmso_baselines_gene_space(\n",
    "    dmso_h5ad_path=DMSO_PATH,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    N_GENES=N_GENES,\n",
    "    ensg_to_token_id=ensg_to_token_id,\n",
    ")\n",
    "print(\"baseline_global:\", baseline_global.shape, \"baseline_by_cl:\", len(baseline_by_cl))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8) split (drug, cell_line) pairs + weights\n",
    "# =========================================================\n",
    "DRUG_COL, CELL_COL, N_COL = \"drug\", \"cell_line_id\", \"n_cells\"\n",
    "MIN_TRAIN = 1000\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "counts = pd.read_csv(COUNTS_CSV)\n",
    "counts[DRUG_COL] = counts[DRUG_COL].astype(str)\n",
    "counts[CELL_COL] = counts[CELL_COL].astype(str)\n",
    "\n",
    "train_pool = counts[counts[N_COL] >= MIN_TRAIN].copy()\n",
    "pairs_df = train_pool[[DRUG_COL, CELL_COL]].drop_duplicates()\n",
    "\n",
    "pairs_df = pairs_df[pairs_df[DRUG_COL] != CONTROL_DRUG]\n",
    "pairs_df = pairs_df[pairs_df[DRUG_COL].map(lambda d: drug_has_targets.get(str(d), False))]\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    pairs_df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=pairs_df[DRUG_COL],\n",
    ")\n",
    "\n",
    "train_pairs = list(zip(train_df[DRUG_COL], train_df[CELL_COL]))\n",
    "val_pairs   = list(zip(val_df[DRUG_COL],   val_df[CELL_COL]))\n",
    "\n",
    "print(\"train pairs:\", len(train_pairs))\n",
    "print(\"val pairs:\", len(val_pairs))\n",
    "\n",
    "def make_pair_weights_from_counts(counts_df, pairs, drug_col=\"drug\", cell_col=\"cell_line_id\", n_col=\"n_cells\",\n",
    "                                  mode=\"inv_sqrt\", eps=1.0):\n",
    "    tmp = counts_df[[drug_col, cell_col, n_col]].copy()\n",
    "    tmp[drug_col] = tmp[drug_col].astype(str)\n",
    "    tmp[cell_col] = tmp[cell_col].astype(str)\n",
    "\n",
    "    pair2n = {(d, c): int(n) for d, c, n in tmp.values}\n",
    "\n",
    "    w = []\n",
    "    for p in pairs:\n",
    "        n = pair2n.get(p, 0)\n",
    "        if mode == \"inv\":\n",
    "            ww = 1.0 / (n + eps)\n",
    "        elif mode == \"inv_log\":\n",
    "            ww = 1.0 / np.log1p(n + eps)\n",
    "        else:\n",
    "            ww = 1.0 / np.sqrt(n + eps)\n",
    "        w.append(float(ww))\n",
    "\n",
    "    w = np.asarray(w, dtype=np.float64)\n",
    "    w = np.clip(w, 0.0, None)\n",
    "    w = w / (w.sum() + 1e-12)\n",
    "    return w\n",
    "\n",
    "w_train = make_pair_weights_from_counts(counts, train_pairs, mode=\"inv_sqrt\")\n",
    "w_val   = make_pair_weights_from_counts(counts, val_pairs,   mode=\"inv_sqrt\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9) parquet row-group indexing\n",
    "# =========================================================\n",
    "PARQUET_FILES = sorted(glob.glob(os.path.join(PARQUET_DIR, \"**\", \"*.parquet\"), recursive=True))\n",
    "print(\"parquet files found:\", len(PARQUET_FILES))\n",
    "\n",
    "def build_pair_to_locations(parquet_files, valid_pairs_set, drug_col=\"drug\", cell_col=\"cell_line_id\"):\n",
    "    out = defaultdict(list)\n",
    "    for f in tqdm(parquet_files, desc=\"Index parquet row-groups\", dynamic_ncols=True):\n",
    "        pf = pq.ParquetFile(f)\n",
    "        for rg in range(pf.num_row_groups):\n",
    "            tbl = pf.read_row_group(rg, columns=[drug_col, cell_col])\n",
    "            df = tbl.to_pandas()\n",
    "            pairs_here = set(zip(df[drug_col].astype(str), df[cell_col].astype(str)))\n",
    "            inter = pairs_here.intersection(valid_pairs_set)\n",
    "            for p in inter:\n",
    "                out[p].append((f, rg))\n",
    "    return out\n",
    "\n",
    "valid_pairs_set = set(train_pairs) | set(val_pairs)\n",
    "pair_to_locations = build_pair_to_locations(PARQUET_FILES, valid_pairs_set, drug_col=DRUG_COL, cell_col=CELL_COL)\n",
    "print(\"indexed pairs:\", len(pair_to_locations))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10) Dataset (drop first genes/expr token + target-only labels)\n",
    "# =========================================================\n",
    "class TahoeFPParquetDatasetMultiPair(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pair_to_locations,\n",
    "        pairs,\n",
    "        baseline_global,\n",
    "        baseline_by_cellline,\n",
    "        drug_to_target_vec_target_only,   # (M_TGT,)\n",
    "        drug2id,\n",
    "        drug_to_smiles_np,\n",
    "        n_genes_full,\n",
    "        steps,\n",
    "        max_seq_len=256,\n",
    "        batch_size=128,\n",
    "        pairs_per_batch=16,\n",
    "        control_drug=\"DMSO_TF\",\n",
    "        pad_id=0,\n",
    "        cls_id=1,\n",
    "        celltok_id=4,\n",
    "        cell_line2id=None,\n",
    "        unk_cell_id=0,\n",
    "        pair_weights=None,\n",
    "        seed=42,\n",
    "        drug_col=\"drug\",\n",
    "        cell_col=\"cell_line_id\",\n",
    "        genes_col=\"genes\",\n",
    "        expr_col=\"expressions\",\n",
    "        cap_per_pair_in_rg=None,\n",
    "        max_tries_per_pair=20,\n",
    "        invalid_global_gene_tids=(1, 2),\n",
    "        subset_token_ids_np=None,\n",
    "        old_tid_to_vocab_lut=None,\n",
    "        m_tgt: int = 0,\n",
    "        drop_first_gene_token: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pair_to_locations = pair_to_locations\n",
    "        self.pairs = list(pairs)\n",
    "\n",
    "        self.baseline_global = np.asarray(baseline_global, dtype=np.float32)\n",
    "        self.baseline_by_cellline = baseline_by_cellline or {}\n",
    "\n",
    "        self.drug_to_target_vec_target_only = drug_to_target_vec_target_only\n",
    "        self.drug2id = drug2id\n",
    "        self.drug_to_smiles_np = drug_to_smiles_np\n",
    "\n",
    "        self.n_genes_full = int(n_genes_full)\n",
    "        self.steps = int(steps)\n",
    "        self.max_seq_len = int(max_seq_len)\n",
    "        self.batch_size = int(batch_size)\n",
    "\n",
    "        self.pairs_per_batch = int(pairs_per_batch)\n",
    "        assert self.batch_size % self.pairs_per_batch == 0\n",
    "        self.cells_per_pair = self.batch_size // self.pairs_per_batch\n",
    "\n",
    "        self.control_drug = str(control_drug)\n",
    "        self.pad_id = int(pad_id)\n",
    "        self.cls_id = int(cls_id)\n",
    "\n",
    "        self.celltok_id = int(celltok_id)\n",
    "        self.cell_line2id = cell_line2id or {}\n",
    "        self.unk_cell_id = int(unk_cell_id)\n",
    "\n",
    "        self.drug_col = drug_col\n",
    "        self.cell_col = cell_col\n",
    "        self.genes_col = genes_col\n",
    "        self.expr_col = expr_col\n",
    "\n",
    "        self.cap_per_pair_in_rg = cap_per_pair_in_rg\n",
    "        self.max_tries_per_pair = int(max_tries_per_pair)\n",
    "        self.seed = int(seed)\n",
    "\n",
    "        any_vec = next(iter(self.drug_to_smiles_np.values()))\n",
    "        self.smiles_dim = int(any_vec.shape[-1])\n",
    "\n",
    "        self.invalid_global_gene_tids = np.asarray(list(set(int(x) for x in invalid_global_gene_tids)), dtype=np.int64)\n",
    "\n",
    "        self.m_tgt = int(m_tgt)\n",
    "        assert self.m_tgt > 0\n",
    "\n",
    "        self.drop_first_gene_token = bool(drop_first_gene_token)\n",
    "\n",
    "        # weights\n",
    "        if pair_weights is None:\n",
    "            self.pair_weights = None\n",
    "        else:\n",
    "            w = np.asarray(pair_weights, dtype=np.float64)\n",
    "            assert len(w) == len(self.pairs)\n",
    "            w = np.clip(w, 0.0, None)\n",
    "            w = w / (w.sum() + 1e-12)\n",
    "            self.pair_weights = w\n",
    "\n",
    "        self.subset_token_ids_np = subset_token_ids_np\n",
    "        self.old_tid_to_vocab_lut = old_tid_to_vocab_lut\n",
    "\n",
    "        self._pf_cache = {}\n",
    "\n",
    "    def _get_pf(self, file_path):\n",
    "        pf = self._pf_cache.get(file_path, None)\n",
    "        if pf is None:\n",
    "            pf = pq.ParquetFile(file_path)\n",
    "            self._pf_cache[file_path] = pf\n",
    "        return pf\n",
    "\n",
    "    def _read_row_group_df(self, file_path, rg_id, columns):\n",
    "        pf = self._get_pf(file_path)\n",
    "        return pf.read_row_group(rg_id, columns=columns).to_pandas()\n",
    "\n",
    "    def _prepare_sparse_sorted(self, genes, expr):\n",
    "        idx = np.asarray(genes, dtype=np.int64)\n",
    "        val = np.asarray(expr, dtype=np.float32)\n",
    "\n",
    "        # ‚úÖ drop the first \"service\" element (genes[0], expr[0])\n",
    "        if self.drop_first_gene_token:\n",
    "            if idx.size > 0 and val.size > 0:\n",
    "                L = min(idx.size, val.size)\n",
    "                idx = idx[:L]\n",
    "                val = val[:L]\n",
    "                if L >= 1:\n",
    "                    idx = idx[1:]\n",
    "                    val = val[1:]\n",
    "\n",
    "        if idx.size == 0 or val.size == 0:\n",
    "            return np.asarray([], dtype=np.int64), np.asarray([], dtype=np.float32)\n",
    "\n",
    "        L = min(idx.size, val.size)\n",
    "        idx = idx[:L]\n",
    "        val = val[:L]\n",
    "\n",
    "        if self.invalid_global_gene_tids.size > 0:\n",
    "            m_bad = np.isin(idx, self.invalid_global_gene_tids, assume_unique=False)\n",
    "            if m_bad.any():\n",
    "                keep = ~m_bad\n",
    "                idx = idx[keep]\n",
    "                val = val[keep]\n",
    "                if idx.size == 0:\n",
    "                    return idx, val\n",
    "\n",
    "        m = (idx >= 0) & (idx < self.n_genes_full)\n",
    "        idx = idx[m]\n",
    "        val = val[m]\n",
    "        if idx.size == 0:\n",
    "            return idx, val\n",
    "\n",
    "        order = np.argsort(idx)\n",
    "        return idx[order], val[order]\n",
    "\n",
    "    def _fill_one_row(self, row_genes, row_expr, baseline_vec, input_ids_row, values_row, attn_row):\n",
    "        idx_sorted, val_sorted = self._prepare_sparse_sorted(row_genes, row_expr)\n",
    "        if idx_sorted.size == 0:\n",
    "            return\n",
    "\n",
    "        delta = val_sorted - baseline_vec[idx_sorted]\n",
    "\n",
    "        mask_sub = np.isin(idx_sorted, self.subset_token_ids_np, assume_unique=False)\n",
    "        if not mask_sub.any():\n",
    "            return\n",
    "\n",
    "        idx_sorted = idx_sorted[mask_sub]\n",
    "        delta = delta[mask_sub]\n",
    "        if idx_sorted.size == 0:\n",
    "            return\n",
    "\n",
    "        k = min(self.max_seq_len, idx_sorted.size)\n",
    "        if k <= 0:\n",
    "            return\n",
    "\n",
    "        if k == idx_sorted.size:\n",
    "            top_pos = np.argsort(-np.abs(delta))\n",
    "        else:\n",
    "            top_pos = np.argpartition(-np.abs(delta), k - 1)[:k]\n",
    "            top_pos = top_pos[np.argsort(-np.abs(delta[top_pos]))]\n",
    "\n",
    "        sel_token_ids = idx_sorted[top_pos]\n",
    "        sel_delta = delta[top_pos]\n",
    "\n",
    "        sel_vocab_ids = self.old_tid_to_vocab_lut[sel_token_ids]\n",
    "        ok = sel_vocab_ids != -1\n",
    "        if not ok.any():\n",
    "            return\n",
    "\n",
    "        sel_vocab_ids = sel_vocab_ids[ok]\n",
    "        sel_delta = sel_delta[ok]\n",
    "\n",
    "        L = min(self.max_seq_len, sel_vocab_ids.size)\n",
    "        if L <= 0:\n",
    "            return\n",
    "\n",
    "        input_ids_row[2:2+L] = sel_vocab_ids[:L]\n",
    "        values_row[2:2+L]    = sel_delta[:L]\n",
    "        attn_row[2:2+L]      = 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        base_seed = self.seed if worker_info is None else (self.seed + worker_info.id)\n",
    "        rng = np.random.default_rng(base_seed)\n",
    "\n",
    "        pairs = self.pairs\n",
    "        weights = self.pair_weights\n",
    "\n",
    "        cols = [self.drug_col, self.cell_col, self.genes_col, self.expr_col]\n",
    "\n",
    "        cnt = 0\n",
    "        while True:\n",
    "            if weights is None:\n",
    "                chosen_idx = rng.integers(0, len(pairs), size=self.pairs_per_batch)\n",
    "            else:\n",
    "                chosen_idx = rng.choice(len(pairs), size=self.pairs_per_batch, replace=True, p=weights)\n",
    "\n",
    "            chosen_pairs = [pairs[i] for i in chosen_idx]\n",
    "\n",
    "            seq_len = 2 + self.max_seq_len\n",
    "            input_ids = np.full((self.batch_size, seq_len), self.pad_id, dtype=np.int64)\n",
    "            values    = np.zeros((self.batch_size, seq_len), dtype=np.float32)\n",
    "            attn      = np.zeros((self.batch_size, seq_len), dtype=np.int64)\n",
    "\n",
    "            input_ids[:, 0] = self.cls_id\n",
    "            input_ids[:, 1] = self.celltok_id\n",
    "            attn[:, 0:2] = 1\n",
    "\n",
    "            y_batch = np.zeros((self.batch_size, self.m_tgt), dtype=np.float32)\n",
    "            smiles_batch = np.zeros((self.batch_size, self.smiles_dim), dtype=np.float32)\n",
    "            drug_id_batch = np.zeros((self.batch_size,), dtype=np.int64)\n",
    "            cell_id_batch = np.zeros((self.batch_size,), dtype=np.int64)\n",
    "\n",
    "            row_ptr = 0\n",
    "            built_any = False\n",
    "\n",
    "            for (drug_name, cell_line) in chosen_pairs:\n",
    "                drug_name = str(drug_name)\n",
    "                cell_line = str(cell_line)\n",
    "\n",
    "                if drug_name == self.control_drug:\n",
    "                    continue\n",
    "\n",
    "                y_vec = self.drug_to_target_vec_target_only.get(drug_name, None)\n",
    "                if y_vec is None or float(np.sum(y_vec)) <= 0.0:\n",
    "                    continue\n",
    "\n",
    "                locs = self.pair_to_locations.get((drug_name, cell_line), [])\n",
    "                if not locs:\n",
    "                    continue\n",
    "\n",
    "                baseline = self.baseline_by_cellline.get(cell_line, self.baseline_global)\n",
    "\n",
    "                sm_vec = self.drug_to_smiles_np.get(drug_name, None)\n",
    "                if sm_vec is None:\n",
    "                    sm_vec = np.zeros((self.smiles_dim,), dtype=np.float32)\n",
    "\n",
    "                did = int(self.drug2id.get(drug_name, 0))\n",
    "                cid = int(self.cell_line2id.get(cell_line, self.unk_cell_id))\n",
    "\n",
    "                for _ in range(self.max_tries_per_pair):\n",
    "                    fpath, rg_id = locs[rng.integers(0, len(locs))]\n",
    "                    df = self._read_row_group_df(fpath, rg_id, columns=cols)\n",
    "\n",
    "                    df = df[(df[self.drug_col].astype(str) == drug_name) &\n",
    "                            (df[self.cell_col].astype(str) == cell_line)]\n",
    "                    if len(df) == 0:\n",
    "                        continue\n",
    "\n",
    "                    if self.cap_per_pair_in_rg is not None and len(df) > self.cap_per_pair_in_rg:\n",
    "                        df = df.sample(self.cap_per_pair_in_rg, replace=False, random_state=None)\n",
    "\n",
    "                    replace = len(df) < self.cells_per_pair\n",
    "                    df = df.sample(self.cells_per_pair, replace=replace, random_state=None)\n",
    "\n",
    "                    for r in df.itertuples(index=False):\n",
    "                        if row_ptr >= self.batch_size:\n",
    "                            break\n",
    "\n",
    "                        y_batch[row_ptr] = y_vec\n",
    "                        smiles_batch[row_ptr] = sm_vec\n",
    "                        drug_id_batch[row_ptr] = did\n",
    "                        cell_id_batch[row_ptr] = cid\n",
    "\n",
    "                        genes = getattr(r, self.genes_col)\n",
    "                        expr  = getattr(r, self.expr_col)\n",
    "\n",
    "                        self._fill_one_row(\n",
    "                            genes, expr, baseline,\n",
    "                            input_ids[row_ptr], values[row_ptr], attn[row_ptr]\n",
    "                        )\n",
    "                        row_ptr += 1\n",
    "\n",
    "                    built_any = True\n",
    "                    break\n",
    "\n",
    "                if row_ptr >= self.batch_size:\n",
    "                    break\n",
    "\n",
    "            if not built_any:\n",
    "                continue\n",
    "\n",
    "            if row_ptr < self.batch_size:\n",
    "                fill = self.batch_size - row_ptr\n",
    "                input_ids[row_ptr:] = input_ids[:fill]\n",
    "                values[row_ptr:]    = values[:fill]\n",
    "                attn[row_ptr:]      = attn[:fill]\n",
    "                y_batch[row_ptr:]   = y_batch[:fill]\n",
    "                smiles_batch[row_ptr:] = smiles_batch[:fill]\n",
    "                drug_id_batch[row_ptr:] = drug_id_batch[:fill]\n",
    "                cell_id_batch[row_ptr:] = cell_id_batch[:fill]\n",
    "\n",
    "            yield {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"values\": torch.tensor(values, dtype=torch.float32),\n",
    "                \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "                \"y_targets\": torch.tensor(y_batch, dtype=torch.float32),   # (B, M_TGT)\n",
    "                \"smiles_emb\": torch.tensor(smiles_batch, dtype=torch.float32),\n",
    "                \"drug_id\": torch.tensor(drug_id_batch, dtype=torch.long),\n",
    "                \"cell_id\": torch.tensor(cell_id_batch, dtype=torch.long),\n",
    "            }\n",
    "\n",
    "            cnt += 1\n",
    "            if cnt >= self.steps:\n",
    "                return\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10.1) cell2id mapping + loaders\n",
    "# =========================================================\n",
    "cell2id_df = pd.read_csv(CELL2ID_CSV)\n",
    "cell2id_df[\"cell_line_id\"] = cell2id_df[\"cell_line_id\"].astype(str)\n",
    "cell2id_df[\"cell_id\"] = cell2id_df[\"cell_id\"].astype(int)\n",
    "\n",
    "cell_line2id = dict(zip(cell2id_df[\"cell_line_id\"], cell2id_df[\"cell_id\"]))\n",
    "NUM_CELL_LINE = int(cell2id_df[\"cell_id\"].max()) + 1\n",
    "print(\"loaded cell2id:\", len(cell_line2id), \"NUM_CELL_LINE:\", NUM_CELL_LINE)\n",
    "\n",
    "CAP_PER_PAIR_IN_RG = None\n",
    "\n",
    "train_ds = TahoeFPParquetDatasetMultiPair(\n",
    "    pair_to_locations=pair_to_locations,\n",
    "    pairs=train_pairs,\n",
    "    baseline_global=baseline_global,\n",
    "    baseline_by_cellline=baseline_by_cl,\n",
    "    drug_to_target_vec_target_only=drug_to_target_vec_tgt,\n",
    "    drug2id=drug2id,\n",
    "    drug_to_smiles_np=drug_to_smiles_np,\n",
    "    n_genes_full=N_GENES,\n",
    "    steps=STEPS_PER_EPOCH,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pairs_per_batch=PAIRS_PER_BATCH,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    pad_id=PAD_ID,\n",
    "    cls_id=CLS_ID,\n",
    "    celltok_id=CELLTOK_ID,\n",
    "    cell_line2id=cell_line2id,\n",
    "    unk_cell_id=0,\n",
    "    pair_weights=w_train,\n",
    "    seed=SEED,\n",
    "    cap_per_pair_in_rg=CAP_PER_PAIR_IN_RG,\n",
    "    max_tries_per_pair=20,\n",
    "    subset_token_ids_np=subset_token_ids_np,\n",
    "    old_tid_to_vocab_lut=old_tid_to_vocab_lut,\n",
    "    m_tgt=M_TGT,\n",
    "    drop_first_gene_token=DROP_FIRST_GENE_TOKEN,\n",
    ")\n",
    "\n",
    "val_ds = TahoeFPParquetDatasetMultiPair(\n",
    "    pair_to_locations=pair_to_locations,\n",
    "    pairs=val_pairs,\n",
    "    baseline_global=baseline_global,\n",
    "    baseline_by_cellline=baseline_by_cl,\n",
    "    drug_to_target_vec_target_only=drug_to_target_vec_tgt,\n",
    "    drug2id=drug2id,\n",
    "    drug_to_smiles_np=drug_to_smiles_np,\n",
    "    n_genes_full=N_GENES,\n",
    "    steps=VAL_STEPS,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pairs_per_batch=PAIRS_PER_BATCH,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    pad_id=PAD_ID,\n",
    "    cls_id=CLS_ID,\n",
    "    celltok_id=CELLTOK_ID,\n",
    "    cell_line2id=cell_line2id,\n",
    "    unk_cell_id=0,\n",
    "    pair_weights=w_val,\n",
    "    seed=SEED + 123,\n",
    "    cap_per_pair_in_rg=CAP_PER_PAIR_IN_RG,\n",
    "    max_tries_per_pair=20,\n",
    "    subset_token_ids_np=subset_token_ids_np,\n",
    "    old_tid_to_vocab_lut=old_tid_to_vocab_lut,\n",
    "    m_tgt=M_TGT,\n",
    "    drop_first_gene_token=DROP_FIRST_GENE_TOKEN,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=None,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=(NUM_WORKERS > 0),\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=None,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 11) Model\n",
    "# =========================================================\n",
    "class FPEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, num_layers, pad_id,\n",
    "                 max_len: int, num_cell_lines: int, cell_pos: int = 1):\n",
    "        super().__init__()\n",
    "        self.token_emb  = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.value_proj = nn.Linear(1, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.cell_line_emb = nn.Embedding(num_cell_lines, d_model)\n",
    "        self.cell_pos = int(cell_pos)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=4*d_model,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, input_ids, values, attention_mask, cell_line_id):\n",
    "        B, L = input_ids.shape\n",
    "        dev = input_ids.device\n",
    "\n",
    "        x = self.token_emb(input_ids) + self.value_proj(values.unsqueeze(-1))\n",
    "        pos = torch.arange(L, device=dev).unsqueeze(0).expand(B, L)\n",
    "        x = x + self.pos_emb(pos)\n",
    "\n",
    "        if cell_line_id is not None:\n",
    "            x[:, self.cell_pos, :] = x[:, self.cell_pos, :] + self.cell_line_emb(cell_line_id.to(dev)).to(x.dtype)\n",
    "\n",
    "        key_padding_mask = (attention_mask == 0)\n",
    "        h = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        return h[:, 0, :]\n",
    "\n",
    "\n",
    "class FPModelTied(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, num_layers, pad_id, smiles_dim,\n",
    "                 max_len: int, num_cell_lines: int):\n",
    "        super().__init__()\n",
    "        self.encoder = FPEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            num_layers=num_layers,\n",
    "            pad_id=pad_id,\n",
    "            max_len=max_len,\n",
    "            num_cell_lines=num_cell_lines,\n",
    "            cell_pos=1,\n",
    "        )\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.smiles_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.BatchNorm1d(4*d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(4*d_model, smiles_dim),\n",
    "            nn.LayerNorm(smiles_dim),\n",
    "        )\n",
    "\n",
    "    def gene_emb_subset(self):\n",
    "        return self.encoder.token_emb.weight[N_SPECIAL:, :]  # (M_SUB, d)\n",
    "\n",
    "    def forward(self, input_ids, values, attention_mask, cell_line_id, return_smiles=False):\n",
    "        h_cls = self.encoder(input_ids, values, attention_mask, cell_line_id)\n",
    "        v_pred = self.proj(h_cls)\n",
    "        z_pred = self.smiles_head(h_cls)\n",
    "        if return_smiles:\n",
    "            return v_pred, z_pred\n",
    "        return v_pred\n",
    "\n",
    "\n",
    "D_MODEL = 256\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 4\n",
    "\n",
    "model = FPModelTied(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    num_layers=N_LAYERS,\n",
    "    pad_id=PAD_ID,\n",
    "    smiles_dim=SMILES_DIM,\n",
    "    max_len=(2 + MAX_SEQ_LEN),\n",
    "    num_cell_lines=NUM_CELL_LINE\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 12) Load pretrained embeddings (subset + cell)\n",
    "# =========================================================\n",
    "def load_pretrained_subset_into_token_emb(token_emb: nn.Embedding, npy_path: str, device):\n",
    "    W = np.load(npy_path)  # (N_GENES, d)\n",
    "    Wt = torch.tensor(W, dtype=torch.float32, device=device)\n",
    "    d = token_emb.weight.shape[1]\n",
    "    if Wt.shape[1] != d:\n",
    "        raise ValueError(f\"d mismatch: npy={Wt.shape[1]} vs token_emb={d}\")\n",
    "\n",
    "    loaded = 0\n",
    "    with torch.no_grad():\n",
    "        for sid, old_tid in enumerate(subset_token_ids):\n",
    "            vid = N_SPECIAL + sid\n",
    "            if 0 <= old_tid < Wt.shape[0]:\n",
    "                token_emb.weight[vid].copy_(Wt[int(old_tid)])\n",
    "                loaded += 1\n",
    "    print(f\"‚úÖ token_emb loaded: {loaded}/{len(subset_token_ids)}\")\n",
    "\n",
    "load_pretrained_subset_into_token_emb(model.encoder.token_emb, PRETRAINED_GENE_NPY, device=device)\n",
    "\n",
    "def load_pretrained_cell_embeddings(cell_emb_layer, cell_emb_npy, device):\n",
    "    W = np.load(cell_emb_npy)\n",
    "    if W.shape != tuple(cell_emb_layer.weight.shape):\n",
    "        raise ValueError(f\"cell emb shape mismatch: npy={W.shape} vs layer={tuple(cell_emb_layer.weight.shape)}\")\n",
    "    with torch.no_grad():\n",
    "        cell_emb_layer.weight.copy_(torch.tensor(W, device=device, dtype=cell_emb_layer.weight.dtype))\n",
    "    print(f\"‚úÖ loaded cell embeddings: {W.shape}\")\n",
    "\n",
    "load_pretrained_cell_embeddings(model.encoder.cell_line_emb, CELL_EMB_NPY, device)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 12.1) TARGET-ONLY embedding bank indices inside SUBSET\n",
    "# =========================================================\n",
    "target_sub_ids = [old_tid_to_subid[tid] for tid in target_token_ids]\n",
    "target_sub_ids = torch.tensor(target_sub_ids, dtype=torch.long, device=device)\n",
    "print(\"target_sub_ids:\", tuple(target_sub_ids.shape))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 13) pos_weight (TARGET-ONLY)\n",
    "# =========================================================\n",
    "Y_list = [v for v in drug_to_target_vec_tgt.values() if float(np.sum(v)) > 0]\n",
    "Y_all = np.stack(Y_list, axis=0)\n",
    "\n",
    "pos = Y_all.sum(axis=0)\n",
    "neg = Y_all.shape[0] - pos\n",
    "pw = neg / (pos + 1e-6)\n",
    "\n",
    "if POSW_MODE == \"clip\":\n",
    "    pw = np.minimum(pw, POSW_MAX)\n",
    "elif POSW_MODE == \"sqrt_clip\":\n",
    "    pw = np.sqrt(pw)\n",
    "    pw = np.minimum(pw, POSW_MAX)\n",
    "elif POSW_MODE == \"none\":\n",
    "    pw = np.ones_like(pw, dtype=np.float32)\n",
    "\n",
    "pos_weight = torch.tensor(pw, dtype=torch.float32, device=device)\n",
    "print(\"‚úÖ pos_weight(target-only):\", pos_weight.shape, \"| max=\", float(pos_weight.max().item()))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 14) Losses\n",
    "# =========================================================\n",
    "def info_nce_ranking_loss_multi_pos(\n",
    "    v_pred: torch.Tensor,\n",
    "    gene_emb: torch.Tensor,\n",
    "    y_targets: torch.Tensor,\n",
    "    num_neg: int = 256,\n",
    "    num_pos: int = 8,\n",
    "    tau: float = 0.1,\n",
    "):\n",
    "    device_ = v_pred.device\n",
    "    B, _ = v_pred.shape\n",
    "    losses = []\n",
    "\n",
    "    # cosine geometry\n",
    "    v_pred = F.normalize(v_pred, dim=1)\n",
    "    gene_emb = F.normalize(gene_emb, dim=1)\n",
    "\n",
    "    for i in range(B):\n",
    "        pos_idx = (y_targets[i] > 0.5).nonzero(as_tuple=True)[0]\n",
    "        if pos_idx.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        neg_idx_all = (y_targets[i] < 0.5).nonzero(as_tuple=True)[0]\n",
    "        if neg_idx_all.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        if num_pos is not None and num_pos > 0 and pos_idx.numel() > num_pos:\n",
    "            pos_idx = pos_idx[torch.randperm(pos_idx.numel(), device=device_)[:num_pos]]\n",
    "\n",
    "        if neg_idx_all.numel() > num_neg:\n",
    "            neg_idx = neg_idx_all[torch.randperm(neg_idx_all.numel(), device=device_)[:num_neg]]\n",
    "        else:\n",
    "            neg_idx = neg_idx_all\n",
    "\n",
    "        pos_emb = gene_emb[pos_idx]\n",
    "        neg_emb = gene_emb[neg_idx]\n",
    "        cand_emb = torch.cat([pos_emb, neg_emb], dim=0)\n",
    "\n",
    "        v = v_pred[i].unsqueeze(0)\n",
    "        scores = (v @ cand_emb.T).squeeze(0) / tau  # cosine logits\n",
    "\n",
    "        P = pos_emb.size(0)\n",
    "        logits = scores.unsqueeze(0).repeat(P, 1)\n",
    "        targets = torch.arange(P, device=device_, dtype=torch.long)\n",
    "        losses.append(F.cross_entropy(logits, targets))\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=device_)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "# ‚úÖ FIXED: BCE now uses cosine logits (same geometry as eval/rank)\n",
    "def bce_with_neg_sampling_cosine(\n",
    "    pred_vec: torch.Tensor,\n",
    "    y_targets: torch.Tensor,\n",
    "    gene_emb: torch.Tensor,\n",
    "    pos_weight_full: torch.Tensor,\n",
    "    num_neg: int = 2048,\n",
    "    pos_cap: int | None = None,\n",
    "    tau_bce: float = 0.10,\n",
    "):\n",
    "    device_ = pred_vec.device\n",
    "    B, _ = pred_vec.shape\n",
    "    losses = []\n",
    "\n",
    "    gene_emb = F.normalize(gene_emb, dim=1)\n",
    "\n",
    "    for i in range(B):\n",
    "        yi = y_targets[i]\n",
    "        pos_idx = (yi > 0.5).nonzero(as_tuple=True)[0]\n",
    "        if pos_idx.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        if (pos_cap is not None) and (pos_idx.numel() > pos_cap):\n",
    "            pos_idx = pos_idx[torch.randperm(pos_idx.numel(), device=device_)[:pos_cap]]\n",
    "\n",
    "        neg_idx_all = (yi < 0.5).nonzero(as_tuple=True)[0]\n",
    "        if neg_idx_all.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        k = min(int(num_neg), neg_idx_all.numel())\n",
    "        neg_idx = neg_idx_all[torch.randperm(neg_idx_all.numel(), device=device_)[:k]]\n",
    "\n",
    "        idx = torch.cat([pos_idx, neg_idx], dim=0)\n",
    "\n",
    "        v = F.normalize(pred_vec[i], dim=0)  # (d,)\n",
    "        logits = (v @ gene_emb[idx].T) / tau_bce  # cosine logits\n",
    "\n",
    "        y_sub = yi[idx]\n",
    "        pw_sub = pos_weight_full[idx]\n",
    "\n",
    "        losses.append(F.binary_cross_entropy_with_logits(logits, y_sub, pos_weight=pw_sub, reduction=\"mean\"))\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=device_)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "def combined_target_loss_neg_sampling_tied(\n",
    "    pred_vec: torch.Tensor,\n",
    "    y_targets: torch.Tensor,\n",
    "    gene_emb: torch.Tensor,\n",
    "    pos_weight: torch.Tensor,\n",
    "    lambda_cos: float = 1.0,\n",
    "    lambda_bce: float = 0.1,\n",
    "    lambda_rank: float = 0.5,\n",
    "    bce_num_neg: int = 2048,\n",
    "    bce_pos_cap: int | None = None,\n",
    "    rank_num_neg: int = 256,\n",
    "    rank_num_pos: int = 8,\n",
    "    tau_rank: float = 0.1,\n",
    "    tau_bce: float = 0.10,\n",
    "):\n",
    "    device_ = pred_vec.device\n",
    "\n",
    "    # cosine (direction) loss\n",
    "    gene_emb_norm = F.normalize(gene_emb, dim=1)\n",
    "    pred_norm = F.normalize(pred_vec, dim=1)\n",
    "\n",
    "    true_vec = y_targets @ gene_emb_norm\n",
    "    num_t = y_targets.sum(dim=1, keepdim=True)\n",
    "    mask = (num_t > 0).squeeze(1)\n",
    "\n",
    "    if mask.any():\n",
    "        true_vec_pos = true_vec[mask] / (num_t[mask] + 1e-6)\n",
    "        true_vec_pos = F.normalize(true_vec_pos, dim=1)\n",
    "        pred_pos = pred_norm[mask]\n",
    "        loss_cos = 1.0 - (pred_pos * true_vec_pos).sum(dim=1).mean()\n",
    "    else:\n",
    "        loss_cos = torch.tensor(0.0, device=device_)\n",
    "\n",
    "    # BCE with cosine logits\n",
    "    loss_bce = bce_with_neg_sampling_cosine(\n",
    "        pred_vec=pred_vec,\n",
    "        y_targets=y_targets,\n",
    "        gene_emb=gene_emb,            # internally normalized\n",
    "        pos_weight_full=pos_weight,\n",
    "        num_neg=bce_num_neg,\n",
    "        pos_cap=bce_pos_cap,\n",
    "        tau_bce=tau_bce,\n",
    "    )\n",
    "\n",
    "    # rank (InfoNCE) with cosine logits\n",
    "    loss_rank = info_nce_ranking_loss_multi_pos(\n",
    "        v_pred=pred_vec,\n",
    "        gene_emb=gene_emb,\n",
    "        y_targets=y_targets,\n",
    "        num_neg=rank_num_neg,\n",
    "        num_pos=rank_num_pos,\n",
    "        tau=tau_rank,\n",
    "    )\n",
    "\n",
    "    loss = lambda_cos * loss_cos + lambda_bce * loss_bce + lambda_rank * loss_rank\n",
    "    return loss, loss_cos.detach(), loss_bce.detach(), loss_rank.detach()\n",
    "\n",
    "\n",
    "def supervised_contrastive_loss_smiles(z_pred, z_true, drug_ids, tau: float = 0.07, remove_diagonal: bool = True):\n",
    "    z_pred = F.normalize(z_pred, dim=1)\n",
    "    z_true = F.normalize(z_true, dim=1)\n",
    "\n",
    "    logits = (z_pred @ z_true.T) / tau\n",
    "    labels = drug_ids.view(-1, 1)\n",
    "    mask = torch.eq(labels, labels.T).float().to(z_pred.device)\n",
    "\n",
    "    if remove_diagonal:\n",
    "        mask.fill_diagonal_(0.0)\n",
    "\n",
    "    logits = logits - logits.max(dim=1, keepdim=True).values.detach()\n",
    "    log_prob = logits - torch.log(torch.exp(logits).sum(dim=1, keepdim=True) + 1e-6)\n",
    "\n",
    "    pos_cnt = mask.sum(dim=1)\n",
    "    valid = (pos_cnt > 0).float()\n",
    "\n",
    "    mean_log_prob_pos = (mask * log_prob).sum(dim=1) / (pos_cnt + 1e-6)\n",
    "    loss_per = -mean_log_prob_pos * valid\n",
    "    return loss_per.sum() / (valid.sum() + 1e-6)\n",
    "\n",
    "# ‚úÖ NO MSE: cosine align only\n",
    "def smiles_align_loss_cosine(z_pred: torch.Tensor, z_true: torch.Tensor):\n",
    "    return 1.0 - F.cosine_similarity(z_pred, z_true, dim=-1).mean()\n",
    "\n",
    "def smiles_mean_cosine(z_pred: torch.Tensor, z_true: torch.Tensor):\n",
    "    return F.cosine_similarity(z_pred, z_true, dim=-1).mean()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 15) Eval (NO SMILES MSE)\n",
    "# =========================================================\n",
    "def compute_recall_precision_at_k(scores: torch.Tensor, y_true: torch.Tensor, k: int = 20):\n",
    "    B, M = scores.shape\n",
    "    kk = min(k, M)\n",
    "    _, topk_idx = torch.topk(scores, k=kk, dim=1)\n",
    "\n",
    "    recalls, precisions = [], []\n",
    "    for i in range(B):\n",
    "        true_labels = y_true[i]\n",
    "        num_pos_ = true_labels.sum().item()\n",
    "        if num_pos_ == 0:\n",
    "            continue\n",
    "        topk = topk_idx[i]\n",
    "        num_pos_in_topk = true_labels[topk].sum().item()\n",
    "        recalls.append(num_pos_in_topk / max(num_pos_, 1e-6))\n",
    "        precisions.append(num_pos_in_topk / max(kk, 1))\n",
    "\n",
    "    if len(recalls) == 0:\n",
    "        return 0.0, 0.0\n",
    "    return float(sum(recalls) / len(recalls)), float(sum(precisions) / len(precisions))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_fp_targets_and_smiles(model, loader, device, target_sub_ids, k_list=(5,10), tau_smiles_eval=0.07):\n",
    "    model.eval()\n",
    "\n",
    "    gene_emb = model.gene_emb_subset()[target_sub_ids].to(device)  # (M_TGT, d)\n",
    "    g_norm = gene_emb / (gene_emb.norm(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "    recall_sums = {k: 0.0 for k in k_list}\n",
    "    prec_sums   = {k: 0.0 for k in k_list}\n",
    "    counts_     = {k: 0   for k in k_list}\n",
    "\n",
    "    smiles_supcon_sum = 0.0\n",
    "    smiles_cos_sum = 0.0\n",
    "    n_smiles = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids   = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        values      = batch[\"values\"].to(device, non_blocking=True)\n",
    "        attn        = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        y_targets   = batch[\"y_targets\"].to(device, non_blocking=True)\n",
    "        smiles_true = batch[\"smiles_emb\"].to(device, non_blocking=True)\n",
    "        drug_ids    = batch[\"drug_id\"].to(device, non_blocking=True)\n",
    "        cell_id     = batch[\"cell_id\"].to(device, non_blocking=True)\n",
    "\n",
    "        v_pred, z_pred = model(input_ids, values, attn, cell_line_id=cell_id, return_smiles=True)\n",
    "\n",
    "        v_norm = v_pred / (v_pred.norm(dim=1, keepdim=True) + 1e-8)\n",
    "        scores = v_norm @ g_norm.T\n",
    "\n",
    "        for k in k_list:\n",
    "            r, p = compute_recall_precision_at_k(scores, y_targets, k=k)\n",
    "            recall_sums[k] += r\n",
    "            prec_sums[k]   += p\n",
    "            counts_[k]     += 1\n",
    "\n",
    "        bs = smiles_true.size(0)\n",
    "        supcon = supervised_contrastive_loss_smiles(z_pred, smiles_true, drug_ids, tau=tau_smiles_eval)\n",
    "        smiles_supcon_sum += supcon.item() * bs\n",
    "\n",
    "        cos = smiles_mean_cosine(z_pred, smiles_true)\n",
    "        smiles_cos_sum += cos.item() * bs\n",
    "\n",
    "        n_smiles += bs\n",
    "\n",
    "    out = {}\n",
    "    for k in k_list:\n",
    "        out[f\"Recall@{k}\"] = recall_sums[k] / max(counts_[k], 1)\n",
    "        out[f\"Precision@{k}\"] = prec_sums[k] / max(counts_[k], 1)\n",
    "\n",
    "    out[\"SMILES_SupCon\"] = smiles_supcon_sum / max(n_smiles, 1)\n",
    "    out[\"SMILES_COS\"]    = smiles_cos_sum / max(n_smiles, 1)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 16) Train loop (NO SMILES MSE)\n",
    "# =========================================================\n",
    "def infinite_loader(loader):\n",
    "    while True:\n",
    "        for b in loader:\n",
    "            yield b\n",
    "\n",
    "USE_AMP = (device.type == \"cuda\")\n",
    "scaler = GradScaler(enabled=USE_AMP)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "def train_one_epoch_fixed_steps(\n",
    "    model,\n",
    "    train_loader,\n",
    "    device,\n",
    "    steps_per_epoch,\n",
    "    optimizer,\n",
    "    scaler,\n",
    "    target_sub_ids,\n",
    "    pos_weight,\n",
    "    log_every=50,\n",
    "    grad_clip=1.0,\n",
    "):\n",
    "    model.train()\n",
    "    it = infinite_loader(train_loader)\n",
    "\n",
    "    running_total = 0.0\n",
    "    running_tgt   = 0.0\n",
    "    running_sm    = 0.0\n",
    "    running_rank_last = 0.0\n",
    "    n = 0\n",
    "\n",
    "    pbar = tqdm(range(1, steps_per_epoch + 1), desc=\"Train\", leave=True, dynamic_ncols=True)\n",
    "\n",
    "    for step in pbar:\n",
    "        batch = next(it)\n",
    "\n",
    "        input_ids   = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        values      = batch[\"values\"].to(device, non_blocking=True)\n",
    "        attn        = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        y_targets   = batch[\"y_targets\"].to(device, non_blocking=True)\n",
    "        smiles_true = batch[\"smiles_emb\"].to(device, non_blocking=True)\n",
    "        drug_ids    = batch[\"drug_id\"].to(device, non_blocking=True)\n",
    "        cell_id     = batch[\"cell_id\"].to(device, non_blocking=True)\n",
    "\n",
    "        bs = input_ids.size(0)\n",
    "        n += bs\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if USE_AMP:\n",
    "            with autocast(enabled=True):\n",
    "                v_pred, z_pred = model(input_ids, values, attn, cell_line_id=cell_id, return_smiles=True)\n",
    "\n",
    "                gene_emb = model.gene_emb_subset()[target_sub_ids]  # (M_TGT, d)\n",
    "\n",
    "                loss_targets, loss_cos_t, loss_bce_t, loss_rank_t = combined_target_loss_neg_sampling_tied(\n",
    "                    pred_vec=v_pred,\n",
    "                    y_targets=y_targets,\n",
    "                    gene_emb=gene_emb,\n",
    "                    pos_weight=pos_weight,\n",
    "                    lambda_cos=lambda_cos,\n",
    "                    lambda_bce=lambda_bce,\n",
    "                    lambda_rank=lambda_rank,\n",
    "                    bce_num_neg=bce_num_neg,\n",
    "                    bce_pos_cap=bce_pos_cap,\n",
    "                    rank_num_neg=rank_num_neg,\n",
    "                    rank_num_pos=rank_num_pos,\n",
    "                    tau_rank=tau_rank,\n",
    "                    tau_bce=tau_bce,\n",
    "                )\n",
    "\n",
    "                # ‚úÖ SMILES: SupCon + cosine-align only (NO MSE)\n",
    "                loss_supcon = supervised_contrastive_loss_smiles(z_pred, smiles_true, drug_ids, tau=tau_smiles)\n",
    "                loss_align  = smiles_align_loss_cosine(z_pred, smiles_true)\n",
    "                loss_smiles = loss_supcon + alpha_align * loss_align\n",
    "\n",
    "                loss = loss_targets + lambda_smiles * loss_smiles\n",
    "\n",
    "            if not torch.isfinite(loss).all():\n",
    "                continue\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "\n",
    "            any_grad = any(p.grad is not None for p in model.parameters())\n",
    "            if not any_grad:\n",
    "                scaler.update()\n",
    "                continue\n",
    "\n",
    "            if grad_clip is not None and grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        else:\n",
    "            v_pred, z_pred = model(input_ids, values, attn, cell_line_id=cell_id, return_smiles=True)\n",
    "            gene_emb = model.gene_emb_subset()[target_sub_ids]\n",
    "\n",
    "            loss_targets, loss_cos_t, loss_bce_t, loss_rank_t = combined_target_loss_neg_sampling_tied(\n",
    "                pred_vec=v_pred,\n",
    "                y_targets=y_targets,\n",
    "                gene_emb=gene_emb,\n",
    "                pos_weight=pos_weight,\n",
    "                lambda_cos=lambda_cos,\n",
    "                lambda_bce=lambda_bce,\n",
    "                lambda_rank=lambda_rank,\n",
    "                bce_num_neg=bce_num_neg,\n",
    "                bce_pos_cap=bce_pos_cap,\n",
    "                rank_num_neg=rank_num_neg,\n",
    "                rank_num_pos=rank_num_pos,\n",
    "                tau_rank=tau_rank,\n",
    "                tau_bce=tau_bce,\n",
    "            )\n",
    "\n",
    "            loss_supcon = supervised_contrastive_loss_smiles(z_pred, smiles_true, drug_ids, tau=tau_smiles)\n",
    "            loss_align  = smiles_align_loss_cosine(z_pred, smiles_true)\n",
    "            loss_smiles = loss_supcon + alpha_align * loss_align\n",
    "            loss = loss_targets + lambda_smiles * loss_smiles\n",
    "\n",
    "            if not torch.isfinite(loss).all():\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            if grad_clip is not None and grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_total += float(loss.item()) * bs\n",
    "        running_tgt   += float(loss_targets.item()) * bs\n",
    "        running_sm    += float(loss_smiles.item()) * bs\n",
    "        running_rank_last = float(loss_rank_t.item())\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            pos_in_batch = int((y_targets.sum(dim=1) > 0).sum().item())\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{running_total/max(n,1):.4f}\",\n",
    "                \"tgt\":  f\"{running_tgt/max(n,1):.4f}\",\n",
    "                \"sm\":   f\"{running_sm/max(n,1):.4f}\",\n",
    "                \"rank(last)\": f\"{running_rank_last:.4f}\",\n",
    "                \"pos_in_batch\": f\"{pos_in_batch}/{bs}\",\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"train_total\":  running_total / max(n, 1),\n",
    "        \"train_target\": running_tgt   / max(n, 1),\n",
    "        \"train_smiles\": running_sm    / max(n, 1),\n",
    "        \"rank_last\":    running_rank_last,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 17) TRAIN\n",
    "# =========================================================\n",
    "print(\">>> TRAIN START (TARGET-ONLY + DROP FIRST TOKEN + NO SMILES MSE + COSINE-BCE + STRONG RANK)\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    logs = train_one_epoch_fixed_steps(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        device=device,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        optimizer=optimizer,\n",
    "        scaler=scaler,\n",
    "        target_sub_ids=target_sub_ids,\n",
    "        pos_weight=pos_weight,\n",
    "        log_every=50,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[Epoch {epoch}/{EPOCHS}] \"\n",
    "          f\"train_total={logs['train_total']:.4f} | \"\n",
    "          f\"train_target={logs['train_target']:.4f} | \"\n",
    "          f\"train_smiles={logs['train_smiles']:.4f} | \"\n",
    "          f\"rank_last={logs['rank_last']:.4f}\")\n",
    "\n",
    "    valid_metrics = evaluate_fp_targets_and_smiles(\n",
    "        model=model,\n",
    "        loader=val_loader,\n",
    "        device=device,\n",
    "        target_sub_ids=target_sub_ids,\n",
    "        k_list=(5, 10),\n",
    "        tau_smiles_eval=tau_smiles,\n",
    "    )\n",
    "    print(\"‚úÖ VALID metrics:\", valid_metrics)\n",
    "\n",
    "    if (epoch % SAVE_EVERY == 0) or (epoch == EPOCHS):\n",
    "        save_checkpoint(\n",
    "            save_dir=CKPT_DIR,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scaler=scaler,\n",
    "            epoch=epoch,\n",
    "            metrics={\"train\": logs, \"valid\": valid_metrics},\n",
    "            prefix=\"fp_target_only\",\n",
    "        )\n",
    "\n",
    "print(\">>> DONE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176fe638",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd59b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[targets] drugs total=379, with>=1 target=264\n",
      "[HVG] token_ids: 4000\n",
      "[subset] HVG ‚à™ TARGETS size: 4184\n",
      "[target-only] size: 278\n",
      "[vocab] VOCAB_SIZE: 4188 | N_SPECIAL: 4\n",
      "[targets] drugs with>=1 target vec: 264\n",
      "[organ] NUM_ORGANS: 16 | mapped cell_lines: 102 | UNK_ORGAN_ID: 0\n",
      "[SMILES] missing=0/379 | zero_vec=2\n",
      "[SMILES] bank: (379, 768)\n",
      "[baseline] global: (62713,) | by_cl: 50\n",
      "[split] train pairs: 10505 | val pairs: 1168\n",
      "[parquet] files: 3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Index parquet row-groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3388/3388 [13:31<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[parquet] indexed pairs: 11673\n",
      "‚úÖ token_emb loaded: 4184/4184\n",
      "[target_sub_ids]: (278,)\n",
      ">>> TRAIN START: FP(TARGET-ONLY) + ORGAN + SMILES CLIP | Variant A(log1p->delta->clip) | NO pos_emb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [2:47:20<00:00,  1.43s/it, lr=1.00e-04, tau=0.106, tot=6.4425, tgt=6.1915, clip=4.8591, align=0.3241, rank(last)=5.1235, Hit@5=0.018, TrueCos=0.676, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1/20] lr=1.00e-04 | tau=0.106 | train_total=6.4425 | train_tgt=6.1915 | train_clip=4.8591 | train_align=0.3241 | rank_last=5.1235 | Hit@5=0.018 | TrueCos=0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VALID: {'Recall@5': 0.13453268006588323, 'Precision@5': 0.03774479166666671, 'Recall@10': 0.21400091495599294, 'Precision@10': 0.03110416666666666, 'SMILES_Hit@1': 0.011770833333333333, 'SMILES_Hit@5': 0.03763020833333333, 'SMILES_Hit@10': 0.055546875, 'SMILES_TrueCos': 0.761170000632604, 'SMILES_CLIP': 4.818466658592224, 'tau': 0.10604571551084518}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [3:02:26<00:00,  1.56s/it, lr=9.93e-05, tau=0.091, tot=6.0259, tgt=5.7825, clip=4.7293, align=0.2790, rank(last)=4.9658, Hit@5=0.062, TrueCos=0.721, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 2/20] lr=9.93e-05 | tau=0.091 | train_total=6.0259 | train_tgt=5.7825 | train_clip=4.7293 | train_align=0.2790 | rank_last=4.9658 | Hit@5=0.062 | TrueCos=0.721\n",
      "‚úÖ VALID: {'Recall@5': 0.1812032657013125, 'Precision@5': 0.05100000000000001, 'Recall@10': 0.2784314728963165, 'Precision@10': 0.04054687500000003, 'SMILES_Hit@1': 0.029817708333333335, 'SMILES_Hit@5': 0.07591145833333333, 'SMILES_Hit@10': 0.11114583333333333, 'SMILES_TrueCos': 0.6834968763589859, 'SMILES_CLIP': 4.665763538678487, 'tau': 0.0910659059882164}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [3:06:25<00:00,  1.60s/it, lr=9.73e-05, tau=0.077, tot=5.7821, tgt=5.5438, clip=4.5923, align=0.3492, rank(last)=4.8268, Hit@5=0.104, TrueCos=0.651, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 3/20] lr=9.73e-05 | tau=0.077 | train_total=5.7821 | train_tgt=5.5438 | train_clip=4.5923 | train_align=0.3492 | rank_last=4.8268 | Hit@5=0.104 | TrueCos=0.651\n",
      "‚úÖ VALID: {'Recall@5': 0.2047391628192409, 'Precision@5': 0.05968229166666662, 'Recall@10': 0.31234690584299996, 'Precision@10': 0.046921874999999995, 'SMILES_Hit@1': 0.051744791666666665, 'SMILES_Hit@5': 0.115703125, 'SMILES_Hit@10': 0.15895833333333334, 'SMILES_TrueCos': 0.620230129758517, 'SMILES_CLIP': 4.5414337539672855, 'tau': 0.07650987803936005}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [2:58:05<00:00,  1.53s/it, lr=9.40e-05, tau=0.066, tot=5.6183, tgt=5.3850, clip=4.4654, align=0.4042, rank(last)=4.6868, Hit@5=0.138, TrueCos=0.596, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 4/20] lr=9.40e-05 | tau=0.066 | train_total=5.6183 | train_tgt=5.3850 | train_clip=4.4654 | train_align=0.4042 | rank_last=4.6868 | Hit@5=0.138 | TrueCos=0.596\n",
      "‚úÖ VALID: {'Recall@5': 0.22050381562881563, 'Precision@5': 0.06540104166666663, 'Recall@10': 0.3277380745701059, 'Precision@10': 0.050192708333333357, 'SMILES_Hit@1': 0.06661458333333334, 'SMILES_Hit@5': 0.14067708333333334, 'SMILES_Hit@10': 0.18861979166666668, 'SMILES_TrueCos': 0.5752571612596512, 'SMILES_CLIP': 4.431879811286926, 'tau': 0.06598414480686188}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [3:07:13<00:00,  1.60s/it, lr=8.95e-05, tau=0.057, tot=5.4982, tgt=5.2693, clip=4.3565, align=0.4428, rank(last)=4.6511, Hit@5=0.164, TrueCos=0.557, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 5/20] lr=8.95e-05 | tau=0.057 | train_total=5.4982 | train_tgt=5.2693 | train_clip=4.3565 | train_align=0.4428 | rank_last=4.6511 | Hit@5=0.164 | TrueCos=0.557\n",
      "‚úÖ VALID: {'Recall@5': 0.23745490244709003, 'Precision@5': 0.07045312500000005, 'Recall@10': 0.3467131688479345, 'Precision@10': 0.05305468750000002, 'SMILES_Hit@1': 0.07565104166666667, 'SMILES_Hit@5': 0.16158854166666667, 'SMILES_Hit@10': 0.21513020833333332, 'SMILES_TrueCos': 0.5383901741107305, 'SMILES_CLIP': 4.329053503672282, 'tau': 0.05719948932528496}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [2:23:35<00:00,  1.23s/it, lr=8.39e-05, tau=0.050, tot=5.3934, tgt=5.1692, clip=4.2465, align=0.4753, rank(last)=4.5532, Hit@5=0.189, TrueCos=0.525, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 6/20] lr=8.39e-05 | tau=0.050 | train_total=5.3934 | train_tgt=5.1692 | train_clip=4.2465 | train_align=0.4753 | rank_last=4.5532 | Hit@5=0.189 | TrueCos=0.525\n",
      "‚úÖ VALID: {'Recall@5': 0.24949122723341488, 'Precision@5': 0.07468229166666664, 'Recall@10': 0.36050634030321543, 'Precision@10': 0.055804687499999964, 'SMILES_Hit@1': 0.08911458333333333, 'SMILES_Hit@5': 0.19122395833333333, 'SMILES_Hit@10': 0.250546875, 'SMILES_TrueCos': 0.5103278501828512, 'SMILES_CLIP': 4.231426575183868, 'tau': 0.05004861205816269}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [2:14:16<00:00,  1.15s/it, lr=7.74e-05, tau=0.044, tot=5.3107, tgt=5.0908, clip=4.1492, align=0.4995, rank(last)=4.5271, Hit@5=0.210, TrueCos=0.501, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 7/20] lr=7.74e-05 | tau=0.044 | train_total=5.3107 | train_tgt=5.0908 | train_clip=4.1492 | train_align=0.4995 | rank_last=4.5271 | Hit@5=0.210 | TrueCos=0.501\n",
      "‚úÖ VALID: {'Recall@5': 0.25910566350605413, 'Precision@5': 0.07683333333333335, 'Recall@10': 0.3705589896214895, 'Precision@10': 0.056846354166666654, 'SMILES_Hit@1': 0.09348958333333333, 'SMILES_Hit@5': 0.19731770833333334, 'SMILES_Hit@10': 0.2579427083333333, 'SMILES_TrueCos': 0.49005644301573437, 'SMILES_CLIP': 4.156260814666748, 'tau': 0.04421854019165039}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [3:03:08<00:00,  1.57s/it, lr=7.01e-05, tau=0.040, tot=5.2454, tgt=5.0293, clip=4.0621, align=0.5202, rank(last)=4.4614, Hit@5=0.227, TrueCos=0.480, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 8/20] lr=7.01e-05 | tau=0.040 | train_total=5.2454 | train_tgt=5.0293 | train_clip=4.0621 | train_align=0.5202 | rank_last=4.4614 | Hit@5=0.227 | TrueCos=0.480\n",
      "‚úÖ VALID: {'Recall@5': 0.2628372523020961, 'Precision@5': 0.07845312499999996, 'Recall@10': 0.37603251551689054, 'Precision@10': 0.058479166666666686, 'SMILES_Hit@1': 0.10135416666666666, 'SMILES_Hit@5': 0.21856770833333333, 'SMILES_Hit@10': 0.28325520833333334, 'SMILES_TrueCos': 0.47121061543623605, 'SMILES_CLIP': 4.096646081606547, 'tau': 0.03951994702219963}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [3:37:14<00:00,  1.86s/it, lr=6.23e-05, tau=0.036, tot=5.1898, tgt=4.9772, clip=3.9836, align=0.5375, rank(last)=4.5038, Hit@5=0.242, TrueCos=0.463, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 9/20] lr=6.23e-05 | tau=0.036 | train_total=5.1898 | train_tgt=4.9772 | train_clip=3.9836 | train_align=0.5375 | rank_last=4.5038 | Hit@5=0.242 | TrueCos=0.463\n",
      "‚úÖ VALID: {'Recall@5': 0.2748946131270349, 'Precision@5': 0.0824583333333334, 'Recall@10': 0.3857195520769744, 'Precision@10': 0.059971354166666636, 'SMILES_Hit@1': 0.11109375, 'SMILES_Hit@5': 0.23278645833333333, 'SMILES_Hit@10': 0.29932291666666666, 'SMILES_TrueCos': 0.4531615019838015, 'SMILES_CLIP': 4.0104965694745385, 'tau': 0.03574628755450249}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [3:58:35<00:00,  2.05s/it, lr=5.42e-05, tau=0.033, tot=5.1428, tgt=4.9334, clip=3.9123, align=0.5507, rank(last)=4.4189, Hit@5=0.256, TrueCos=0.449, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 10/20] lr=5.42e-05 | tau=0.033 | train_total=5.1428 | train_tgt=4.9334 | train_clip=3.9123 | train_align=0.5507 | rank_last=4.4189 | Hit@5=0.256 | TrueCos=0.449\n",
      "‚úÖ VALID: {'Recall@5': 0.27848910081527267, 'Precision@5': 0.08344791666666675, 'Recall@10': 0.3921030132338726, 'Precision@10': 0.061320312499999995, 'SMILES_Hit@1': 0.11471354166666667, 'SMILES_Hit@5': 0.243046875, 'SMILES_Hit@10': 0.311875, 'SMILES_TrueCos': 0.4423800575733185, 'SMILES_CLIP': 3.955232696533203, 'tau': 0.032634906470775604}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [3:29:00<00:00,  1.79s/it, lr=4.59e-05, tau=0.030, tot=5.1065, tgt=4.8998, clip=3.8539, align=0.5616, rank(last)=4.4987, Hit@5=0.266, TrueCos=0.438, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 11/20] lr=4.59e-05 | tau=0.030 | train_total=5.1065 | train_tgt=4.8998 | train_clip=3.8539 | train_align=0.5616 | rank_last=4.4987 | Hit@5=0.266 | TrueCos=0.438\n",
      "‚úÖ VALID: {'Recall@5': 0.2823159555288459, 'Precision@5': 0.08485937500000001, 'Recall@10': 0.3932350697624135, 'Precision@10': 0.06134374999999996, 'SMILES_Hit@1': 0.11958333333333333, 'SMILES_Hit@5': 0.24731770833333333, 'SMILES_Hit@10': 0.316015625, 'SMILES_TrueCos': 0.4323416962226232, 'SMILES_CLIP': 3.9188380829493203, 'tau': 0.03016114979982376}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [3:23:54<00:00,  1.75s/it, lr=3.78e-05, tau=0.028, tot=5.0765, tgt=4.8721, clip=3.8035, align=0.5707, rank(last)=4.3034, Hit@5=0.274, TrueCos=0.429, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 12/20] lr=3.78e-05 | tau=0.028 | train_total=5.0765 | train_tgt=4.8721 | train_clip=3.8035 | train_align=0.5707 | rank_last=4.3034 | Hit@5=0.274 | TrueCos=0.429\n",
      "‚úÖ VALID: {'Recall@5': 0.28495215519434286, 'Precision@5': 0.08613541666666664, 'Recall@10': 0.4008907776251527, 'Precision@10': 0.06268229166666665, 'SMILES_Hit@1': 0.12125, 'SMILES_Hit@5': 0.2575, 'SMILES_Hit@10': 0.325234375, 'SMILES_TrueCos': 0.42219115207592645, 'SMILES_CLIP': 3.8668323413530987, 'tau': 0.02826393023133278}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [2:16:37<00:00,  1.17s/it, lr=3.00e-05, tau=0.027, tot=5.0510, tgt=4.8485, clip=3.7606, align=0.5776, rank(last)=4.3273, Hit@5=0.281, TrueCos=0.422, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 13/20] lr=3.00e-05 | tau=0.027 | train_total=5.0510 | train_tgt=4.8485 | train_clip=3.7606 | train_align=0.5776 | rank_last=4.3273 | Hit@5=0.281 | TrueCos=0.422\n",
      "‚úÖ VALID: {'Recall@5': 0.28949838233236663, 'Precision@5': 0.08839062500000006, 'Recall@10': 0.40374855960012207, 'Precision@10': 0.0638854166666667, 'SMILES_Hit@1': 0.12619791666666666, 'SMILES_Hit@5': 0.2640104166666667, 'SMILES_Hit@10': 0.33171875, 'SMILES_TrueCos': 0.4184157766898473, 'SMILES_CLIP': 3.8417517272631327, 'tau': 0.026800617575645447}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [2:07:05<00:00,  1.09s/it, lr=2.27e-05, tau=0.026, tot=5.0314, tgt=4.8305, clip=3.7266, align=0.5827, rank(last)=4.2995, Hit@5=0.287, TrueCos=0.417, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 14/20] lr=2.27e-05 | tau=0.026 | train_total=5.0314 | train_tgt=4.8305 | train_clip=3.7266 | train_align=0.5827 | rank_last=4.2995 | Hit@5=0.287 | TrueCos=0.417\n",
      "‚úÖ VALID: {'Recall@5': 0.2882152030550469, 'Precision@5': 0.0881875, 'Recall@10': 0.4061875874414938, 'Precision@10': 0.06410677083333335, 'SMILES_Hit@1': 0.12630208333333334, 'SMILES_Hit@5': 0.26114583333333335, 'SMILES_Hit@10': 0.33466145833333333, 'SMILES_TrueCos': 0.41290168126424154, 'SMILES_CLIP': 3.8126182357470193, 'tau': 0.025704631581902504}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [2:20:48<00:00,  1.21s/it, lr=1.62e-05, tau=0.025, tot=5.0146, tgt=4.8151, clip=3.6979, align=0.5870, rank(last)=4.3114, Hit@5=0.291, TrueCos=0.413, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 15/20] lr=1.62e-05 | tau=0.025 | train_total=5.0146 | train_tgt=4.8151 | train_clip=3.6979 | train_align=0.5870 | rank_last=4.3114 | Hit@5=0.291 | TrueCos=0.413\n",
      "‚úÖ VALID: {'Recall@5': 0.2933345646685491, 'Precision@5': 0.08872916666666665, 'Recall@10': 0.40821422371031785, 'Precision@10': 0.06409375000000002, 'SMILES_Hit@1': 0.127265625, 'SMILES_Hit@5': 0.267734375, 'SMILES_Hit@10': 0.340703125, 'SMILES_TrueCos': 0.4091666708389918, 'SMILES_CLIP': 3.799344880580902, 'tau': 0.024924729019403458}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7000/7000 [2:27:37<00:00,  1.27s/it, lr=1.06e-05, tau=0.024, tot=5.0031, tgt=4.8044, clip=3.6800, align=0.5901, rank(last)=4.2728, Hit@5=0.295, TrueCos=0.410, upd=1750/1750]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 16/20] lr=1.06e-05 | tau=0.024 | train_total=5.0031 | train_tgt=4.8044 | train_clip=3.6800 | train_align=0.5901 | rank_last=4.2728 | Hit@5=0.295 | TrueCos=0.410\n",
      "‚úÖ VALID: {'Recall@5': 0.29402793040293046, 'Precision@5': 0.0888958333333333, 'Recall@10': 0.41049326382529516, 'Precision@10': 0.06436718749999999, 'SMILES_Hit@1': 0.12734375, 'SMILES_Hit@5': 0.2702864583333333, 'SMILES_Hit@10': 0.34197916666666667, 'SMILES_TrueCos': 0.406905122200648, 'SMILES_CLIP': 3.781051870981852, 'tau': 0.02439727634191513}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  29%|‚ñà‚ñà‚ñâ       | 2032/7000 [1:01:00<2:29:09,  1.80s/it, lr=9.22e-06, tau=0.024, tot=4.9950, tgt=4.7968, clip=3.6670, align=0.5911, rank(last)=4.3602, Hit@5=0.297, TrueCos=0.409, upd=500/1750]\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000000865c7a3a000a1736'\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000000865c7a35000a1735'\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs000000009655b40e000a1734'\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs00000000865c7a34000a1733'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1506\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>> TRAIN START: FP(TARGET-ONLY) + ORGAN + SMILES CLIP | Variant A(log1p->delta->clip) | NO pos_emb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m-> 1506\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch_fixed_steps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTEPS_PER_EPOCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_sub_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_sub_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m        \u001b[49m\u001b[43msmiles_bank_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmiles_bank_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_GRAD_NORM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mACCUM_STEPS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m   1523\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | tau=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtau\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHit@5=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_hit5\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | TrueCos=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_truecos\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1531\u001b[0m     )\n\u001b[1;32m   1533\u001b[0m     valid \u001b[38;5;241m=\u001b[39m evaluate_fp(\n\u001b[1;32m   1534\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1535\u001b[0m         loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         hitk\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m   1541\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[1], line 1354\u001b[0m, in \u001b[0;36mtrain_one_epoch_fixed_steps\u001b[0;34m(model, train_loader, device, steps_per_epoch, optimizer, scheduler, scaler, target_sub_ids, pos_weight, smiles_bank_t, log_every, grad_clip, accum_steps)\u001b[0m\n\u001b[1;32m   1351\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, steps_per_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m-> 1354\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1357\u001b[0m     values    \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 1297\u001b[0m, in \u001b[0;36minfinite_loader\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minfinite_loader\u001b[39m(loader):\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1297\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m   1298\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m b\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1482\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1482\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1434\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1433\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1434\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1435\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1436\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1275\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1275\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1278\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/data/aiffel/miniconda3/envs/babayakga/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# FP (TARGET-ONLY) + ORGAN TOKEN + SMILES CLIP  (READY SCRIPT)\n",
    "#\n",
    "# ‚úÖ Variant A scaling: raw counts -> log1p -> delta -> clip (+ optional asinh)\n",
    "# ‚úÖ Drop first \"service\" element from genes/expr\n",
    "# ‚úÖ Stable gene order after selection: sort by gene_id (deterministic)\n",
    "# ‚úÖ ORGAN token injected at fixed position [CLS][ORGAN] (NO cell-line embedding)\n",
    "# ‚úÖ Positional embeddings REMOVED (we agreed to remove pos emb)\n",
    "# ‚úÖ Targets: cosine vector loss + cosine-BCE (neg sampling) + InfoNCE rank\n",
    "# ‚úÖ SMILES: CLIP loss (+ optional cosine align), NO SupCon, NO MSE\n",
    "# ‚úÖ Batch is CLIP-safe: unique drug in batch (1 cell per drug)\n",
    "# ‚úÖ Skip missing/zero SMILES vectors\n",
    "# ‚úÖ OneCycleLR fixed for grad accumulation (total_steps = real updates)\n",
    "# ‚úÖ AMP + grad accumulation\n",
    "#\n",
    "# NOTE:\n",
    "# - This script trains BOTH target head and SMILES CLIP jointly.\n",
    "# - If you want SMILES-only warmup, set lambda_targets=0 for first N epochs.\n",
    "# =========================================================\n",
    "\n",
    "import os, glob, ast, random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import scanpy as sc\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 0) PATHS / HYPERPARAMS\n",
    "# =========================================================\n",
    "PARQUET_DIR    = \"/data/aiffel/data/Tahoe-100M/data\"\n",
    "GENE_META_PATH = \"/data/aiffel/data/Tahoe-100M/metadata/gene_metadata.parquet\"\n",
    "DRUG_META_PATH = \"/data/aiffel/data/Tahoe-100M/metadata/drug_metadata.parquet\"\n",
    "COUNTS_CSV     = \"/data/aiffel/babayakga/making_data/aiffel/babayakga/making_data/tahoe_counts_per_drug_cell_line.csv\"\n",
    "DMSO_PATH      = \"/data/aiffel/babayakga/outputs/dmso.h5ad\"\n",
    "CELL_LINE_META_PATH = \"/data/aiffel/data/Tahoe-100M/metadata/cell_line_metadata.parquet\"\n",
    "\n",
    "SMILES_EMB_PATH       = \"/data/aiffel/babayakga/smiles_emb/drug_smiles_emb_all1.pt\"\n",
    "PRETRAINED_GENE_NPY   = \"/data/aiffel/babayakga/pretraining/checkpoints_with_cell/gene_embeddings.npy\"  # optional\n",
    "\n",
    "CONTROL_DRUG = \"DMSO_TF\"\n",
    "SEED = 42\n",
    "\n",
    "# sequence\n",
    "MAX_SEQ_LEN = 256\n",
    "HVG_K = 4000\n",
    "\n",
    "# training\n",
    "BATCH_SIZE  = 128\n",
    "ACCUM_STEPS = 4\n",
    "STEPS_PER_EPOCH = 7000\n",
    "VAL_STEPS       = 300\n",
    "EPOCHS          = 20\n",
    "\n",
    "LR           = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# targets loss weights (from our conclusions)\n",
    "lambda_cos  = 1.0\n",
    "lambda_bce  = 0.05\n",
    "lambda_rank = 1.0\n",
    "\n",
    "# bce/rank knobs\n",
    "bce_num_neg  = 2048\n",
    "bce_pos_cap  = None\n",
    "tau_bce      = 0.15\n",
    "\n",
    "rank_num_neg = 1024\n",
    "rank_num_pos = 4\n",
    "tau_rank     = 0.15\n",
    "\n",
    "# SMILES (CLIP)\n",
    "lambda_smiles = 0.05\n",
    "alpha_align   = 0.5\n",
    "TAU_INIT      = 0.10  # initial tau (learnable temperature)\n",
    "\n",
    "# overall mixing\n",
    "lambda_targets = 1.0   # set 0.0 for SMILES-only warmup if you want\n",
    "\n",
    "# data sampling\n",
    "NUM_WORKERS = 4\n",
    "MIN_TRAIN_CELLS_PER_PAIR = 1000\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# Variant A scaling\n",
    "USE_LOG1P_EXPR   = True\n",
    "USE_ASINH_DELTA  = False\n",
    "DELTA_CLIP_ABS   = 5.0\n",
    "\n",
    "# misc\n",
    "DROP_FIRST_GENE_TOKEN = True\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 1) gene_metadata\n",
    "# =========================================================\n",
    "gene_md = pd.read_parquet(GENE_META_PATH).copy()\n",
    "gene_md[\"gene_symbol\"] = gene_md[\"gene_symbol\"].astype(str)\n",
    "gene_md[\"ensembl_id\"]  = gene_md[\"ensembl_id\"].astype(str)\n",
    "gene_md[\"token_id\"]    = gene_md[\"token_id\"].astype(int)\n",
    "gene_md = gene_md.sort_values(\"token_id\").reset_index(drop=True)\n",
    "\n",
    "N_GENES = int(gene_md[\"token_id\"].max()) + 1\n",
    "symbol_to_ensg_lower = dict(zip(gene_md[\"gene_symbol\"].str.lower(), gene_md[\"ensembl_id\"]))\n",
    "ensg_to_token_id = dict(zip(gene_md[\"ensembl_id\"].values, gene_md[\"token_id\"].values))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2) drug_metadata -> targets\n",
    "# =========================================================\n",
    "def parse_targets(x):\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return [str(t).strip() for t in x if str(t).strip()]\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "            try:\n",
    "                out = ast.literal_eval(s)\n",
    "                if isinstance(out, (list, tuple)):\n",
    "                    return [str(t).strip() for t in out if str(t).strip()]\n",
    "            except Exception:\n",
    "                pass\n",
    "        for sep in [\";\", \",\"]:\n",
    "            if sep in s:\n",
    "                return [t.strip() for t in s.split(sep) if t.strip()]\n",
    "        return [s] if s else []\n",
    "    return [str(x).strip()]\n",
    "\n",
    "drug_meta_df = pd.read_parquet(DRUG_META_PATH).copy()\n",
    "drug_meta_df[\"drug\"] = drug_meta_df[\"drug\"].astype(str)\n",
    "\n",
    "drug_to_target_tokenids = {}\n",
    "all_target_tokenids = set()\n",
    "\n",
    "for _, row in drug_meta_df.iterrows():\n",
    "    drug = str(row[\"drug\"])\n",
    "    targets = parse_targets(row.get(\"targets\", None))\n",
    "\n",
    "    tids = []\n",
    "    for t in targets:\n",
    "        t = str(t).strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        if t.startswith(\"ENSG\"):\n",
    "            ensg = t\n",
    "        else:\n",
    "            ensg = symbol_to_ensg_lower.get(t.lower(), None)\n",
    "        if ensg is None:\n",
    "            continue\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        tid = int(tid)\n",
    "        if 0 <= tid < N_GENES:\n",
    "            tids.append(tid)\n",
    "\n",
    "    tids = sorted(set(tids))\n",
    "    drug_to_target_tokenids[drug] = tids\n",
    "    all_target_tokenids.update(tids)\n",
    "\n",
    "drug_has_targets = {d: (len(tids) > 0) for d, tids in drug_to_target_tokenids.items()}\n",
    "print(f\"[targets] drugs total={len(drug_to_target_tokenids)}, with>=1 target={sum(drug_has_targets.values())}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3) HVG from DMSO\n",
    "# =========================================================\n",
    "def compute_hvg_token_ids_from_dmso(dmso_h5ad_path: str, control_drug: str, HVG_K: int, ensg_to_token_id: dict):\n",
    "    ad = sc.read_h5ad(dmso_h5ad_path)\n",
    "    obs = ad.obs\n",
    "    m = (obs[\"drug\"].astype(str).values == str(control_drug))\n",
    "    idx = np.where(m)[0]\n",
    "    if idx.size == 0:\n",
    "        raise ValueError(f\"No DMSO cells found: control_drug={control_drug}\")\n",
    "\n",
    "    X = ad.X.tocsr() if sparse.issparse(ad.X) else sparse.csr_matrix(ad.X)\n",
    "    Xc = X[idx]\n",
    "\n",
    "    mean = np.asarray(Xc.mean(axis=0)).ravel()\n",
    "    mean2 = np.asarray(Xc.multiply(Xc).mean(axis=0)).ravel()\n",
    "    var = (mean2 - mean**2).astype(np.float32)\n",
    "\n",
    "    ensgs = ad.var_names.astype(str).tolist()\n",
    "    token_ids, vars_ = [], []\n",
    "    for j, ensg in enumerate(ensgs):\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        token_ids.append(int(tid))\n",
    "        vars_.append(float(var[j]))\n",
    "\n",
    "    token_ids = np.asarray(token_ids, dtype=np.int64)\n",
    "    vars_ = np.asarray(vars_, dtype=np.float32)\n",
    "    if token_ids.size == 0:\n",
    "        raise ValueError(\"ENSG mapping failed.\")\n",
    "\n",
    "    k = min(int(HVG_K), token_ids.size)\n",
    "    top = np.argpartition(-vars_, k-1)[:k]\n",
    "    return set(token_ids[top].tolist())\n",
    "\n",
    "hvg_token_ids = compute_hvg_token_ids_from_dmso(DMSO_PATH, CONTROL_DRUG, HVG_K, ensg_to_token_id)\n",
    "print(\"[HVG] token_ids:\", len(hvg_token_ids))\n",
    "\n",
    "# INPUT subset = HVG ‚à™ TARGETS\n",
    "subset_token_ids = sorted(set(hvg_token_ids) | set(all_target_tokenids))\n",
    "M_SUB = len(subset_token_ids)\n",
    "print(\"[subset] HVG ‚à™ TARGETS size:\", M_SUB)\n",
    "\n",
    "# OUTPUT target-only = TARGETS only\n",
    "target_token_ids = sorted(set(all_target_tokenids))\n",
    "M_TGT = len(target_token_ids)\n",
    "print(\"[target-only] size:\", M_TGT)\n",
    "\n",
    "old_tid_to_subid = {tid: i for i, tid in enumerate(subset_token_ids)}\n",
    "old_tid_to_tgtid = {tid: i for i, tid in enumerate(target_token_ids)}\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4) vocab + LUT (include ORGAN token)\n",
    "# =========================================================\n",
    "SPECIAL_TOKENS = [\"[PAD]\", \"[CLS]\", \"[ORGAN]\", \"[MASK]\"]\n",
    "local_token_to_id = {tok: i for i, tok in enumerate(SPECIAL_TOKENS)}\n",
    "N_SPECIAL = len(SPECIAL_TOKENS)\n",
    "\n",
    "VOCAB_SIZE = N_SPECIAL + M_SUB\n",
    "PAD_ID   = local_token_to_id[\"[PAD]\"]\n",
    "CLS_ID   = local_token_to_id[\"[CLS]\"]\n",
    "ORGAN_TOK_ID = local_token_to_id[\"[ORGAN]\"]\n",
    "\n",
    "old_tid_to_vocab_lut = np.full((N_GENES,), -1, dtype=np.int64)\n",
    "for sid, old_tid in enumerate(subset_token_ids):\n",
    "    if 0 <= old_tid < N_GENES:\n",
    "        old_tid_to_vocab_lut[old_tid] = N_SPECIAL + sid\n",
    "\n",
    "subset_token_ids_np = np.asarray(subset_token_ids, dtype=np.int64)\n",
    "print(\"[vocab] VOCAB_SIZE:\", VOCAB_SIZE, \"| N_SPECIAL:\", N_SPECIAL)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5) y_targets (drug -> TARGET-ONLY multi-hot)\n",
    "# =========================================================\n",
    "drug_to_target_vec_tgt = {}\n",
    "for d, tids in drug_to_target_tokenids.items():\n",
    "    vec = np.zeros(M_TGT, dtype=np.float32)\n",
    "    for tid in tids:\n",
    "        j = old_tid_to_tgtid.get(int(tid), None)\n",
    "        if j is not None:\n",
    "            vec[j] = 1.0\n",
    "    drug_to_target_vec_tgt[d] = vec\n",
    "\n",
    "print(\"[targets] drugs with>=1 target vec:\", sum(float(v.sum()) > 0 for v in drug_to_target_vec_tgt.values()))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6) organ mapping: cell_line_id -> organ_id   (FIX: UNK=0, organs start at 1)\n",
    "# =========================================================\n",
    "cl_meta = pd.read_parquet(CELL_LINE_META_PATH).copy()\n",
    "cl_meta[\"Cell_ID_Cellosaur\"] = cl_meta[\"Cell_ID_Cellosaur\"].astype(str)\n",
    "cl_meta[\"Organ\"] = cl_meta[\"Organ\"].astype(str)\n",
    "\n",
    "cl_meta_small = cl_meta[[\"Cell_ID_Cellosaur\", \"Organ\"]].dropna().drop_duplicates()\n",
    "organs = sorted(cl_meta_small[\"Organ\"].unique().tolist())\n",
    "\n",
    "UNK_ORGAN_ID = 0\n",
    "organ2id = {o: i+1 for i, o in enumerate(organs)}  # shift by 1\n",
    "NUM_ORGANS = len(organs) + 1\n",
    "\n",
    "cellline2organid = {\n",
    "    str(cvcl): int(organ2id.get(str(org), UNK_ORGAN_ID))\n",
    "    for cvcl, org in cl_meta_small.values\n",
    "}\n",
    "print(\"[organ] NUM_ORGANS:\", NUM_ORGANS, \"| mapped cell_lines:\", len(cellline2organid), \"| UNK_ORGAN_ID:\", UNK_ORGAN_ID)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7) SMILES embeddings + bank (for retrieval)\n",
    "# =========================================================\n",
    "obj = torch.load(SMILES_EMB_PATH, map_location=\"cpu\")\n",
    "assert isinstance(obj, dict) and \"drug\" in obj and \"emb\" in obj\n",
    "\n",
    "drug_list_saved = [str(d) for d in obj[\"drug\"]]\n",
    "emb_matrix = obj[\"emb\"].to(dtype=torch.float32).cpu().numpy()\n",
    "SMILES_DIM = int(emb_matrix.shape[1])\n",
    "drug_to_smiles_np_raw = {d: emb_matrix[i].astype(np.float32, copy=False) for i, d in enumerate(drug_list_saved)}\n",
    "\n",
    "drug_names_all = sorted(set(drug_meta_df[\"drug\"].astype(str).tolist()))\n",
    "drug2id = {d: i for i, d in enumerate(drug_names_all)}\n",
    "\n",
    "drug_to_smiles_np = {}\n",
    "missing = 0\n",
    "zeroed  = 0\n",
    "for d in drug_names_all:\n",
    "    v = drug_to_smiles_np_raw.get(d, None)\n",
    "    if v is None:\n",
    "        drug_to_smiles_np[d] = np.zeros((SMILES_DIM,), dtype=np.float32)\n",
    "        missing += 1\n",
    "    else:\n",
    "        vv = v.astype(np.float32, copy=False)\n",
    "        if np.abs(vv).sum() == 0.0:\n",
    "            zeroed += 1\n",
    "        drug_to_smiles_np[d] = vv\n",
    "print(f\"[SMILES] missing={missing}/{len(drug_names_all)} | zero_vec={zeroed}\")\n",
    "\n",
    "smiles_bank_np = np.stack([drug_to_smiles_np[d] for d in drug_names_all], axis=0).astype(np.float32)\n",
    "print(\"[SMILES] bank:\", smiles_bank_np.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8) DMSO baselines (Variant A: log1p BEFORE mean)\n",
    "# =========================================================\n",
    "def build_dmso_baselines_gene_space(dmso_h5ad_path: str, control_drug: str, N_GENES: int, ensg_to_token_id: dict, use_log1p: bool):\n",
    "    adata = sc.read_h5ad(dmso_h5ad_path)\n",
    "    obs = adata.obs\n",
    "    X = adata.X.tocsr() if sparse.issparse(adata.X) else sparse.csr_matrix(adata.X)\n",
    "\n",
    "    m = (obs[\"drug\"].astype(str).values == str(control_drug))\n",
    "    idx = np.where(m)[0]\n",
    "    if idx.size == 0:\n",
    "        raise ValueError(\"No DMSO cells.\")\n",
    "\n",
    "    ensgs = adata.var_names.astype(str).tolist()\n",
    "    token_ids, cols = [], []\n",
    "    for j, ensg in enumerate(ensgs):\n",
    "        tid = ensg_to_token_id.get(ensg, None)\n",
    "        if tid is None:\n",
    "            continue\n",
    "        token_ids.append(int(tid)); cols.append(j)\n",
    "\n",
    "    token_ids = np.asarray(token_ids, dtype=np.int64)\n",
    "    cols = np.asarray(cols, dtype=np.int64)\n",
    "\n",
    "    Xc = X[idx][:, cols]\n",
    "    if use_log1p:\n",
    "        Xc = Xc.copy()\n",
    "        Xc.data = np.log1p(np.clip(Xc.data, a_min=0.0, a_max=None))\n",
    "\n",
    "    mean_global_sub = np.asarray(Xc.mean(axis=0)).ravel().astype(np.float32)\n",
    "    baseline_global = np.zeros(N_GENES, dtype=np.float32)\n",
    "    baseline_global[token_ids] = mean_global_sub\n",
    "\n",
    "    baseline_by_cl = {}\n",
    "    cl_values = obs[\"cell_line_id\"].astype(str).values\n",
    "    for cl in np.unique(cl_values):\n",
    "        cl_idx = np.where(m & (cl_values == cl))[0]\n",
    "        if cl_idx.size == 0:\n",
    "            continue\n",
    "        Xcl = X[cl_idx][:, cols]\n",
    "        if use_log1p:\n",
    "            Xcl = Xcl.copy()\n",
    "            Xcl.data = np.log1p(np.clip(Xcl.data, a_min=0.0, a_max=None))\n",
    "        mean_cl_sub = np.asarray(Xcl.mean(axis=0)).ravel().astype(np.float32)\n",
    "\n",
    "        v = np.zeros(N_GENES, dtype=np.float32)\n",
    "        v[token_ids] = mean_cl_sub\n",
    "        baseline_by_cl[str(cl)] = v\n",
    "\n",
    "    return baseline_global, baseline_by_cl\n",
    "\n",
    "baseline_global, baseline_by_cl = build_dmso_baselines_gene_space(\n",
    "    DMSO_PATH, CONTROL_DRUG, N_GENES, ensg_to_token_id, use_log1p=USE_LOG1P_EXPR\n",
    ")\n",
    "print(\"[baseline] global:\", baseline_global.shape, \"| by_cl:\", len(baseline_by_cl))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9) split (drug, cell_line) pairs + weights (filter: has targets)\n",
    "# =========================================================\n",
    "DRUG_COL, CELL_COL, N_COL = \"drug\", \"cell_line_id\", \"n_cells\"\n",
    "\n",
    "counts = pd.read_csv(COUNTS_CSV)\n",
    "counts[DRUG_COL] = counts[DRUG_COL].astype(str)\n",
    "counts[CELL_COL] = counts[CELL_COL].astype(str)\n",
    "counts[N_COL]    = counts[N_COL].astype(int)\n",
    "\n",
    "pairs_df = counts[counts[N_COL] >= MIN_TRAIN_CELLS_PER_PAIR][[DRUG_COL, CELL_COL]].drop_duplicates().copy()\n",
    "pairs_df = pairs_df[pairs_df[DRUG_COL] != str(CONTROL_DRUG)].copy()\n",
    "pairs_df = pairs_df[pairs_df[DRUG_COL].map(lambda d: drug_has_targets.get(str(d), False))].copy()\n",
    "pairs_df = pairs_df[pairs_df[DRUG_COL].isin(set(drug2id.keys()))].copy()\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    pairs_df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=pairs_df[DRUG_COL],\n",
    ")\n",
    "\n",
    "train_pairs = list(zip(train_df[DRUG_COL], train_df[CELL_COL]))\n",
    "val_pairs   = list(zip(val_df[DRUG_COL],   val_df[CELL_COL]))\n",
    "print(\"[split] train pairs:\", len(train_pairs), \"| val pairs:\", len(val_pairs))\n",
    "\n",
    "def make_pair_weights_from_counts(counts_df, pairs, mode=\"inv_sqrt\", eps=1.0):\n",
    "    pair2n = {(str(d), str(c)): int(n) for d, c, n in counts_df[[DRUG_COL, CELL_COL, N_COL]].values}\n",
    "    w = []\n",
    "    for p in pairs:\n",
    "        n = pair2n.get((str(p[0]), str(p[1])), 0)\n",
    "        if mode == \"inv\":\n",
    "            ww = 1.0 / (n + eps)\n",
    "        elif mode == \"inv_log\":\n",
    "            ww = 1.0 / np.log1p(n + eps)\n",
    "        else:\n",
    "            ww = 1.0 / np.sqrt(n + eps)\n",
    "        w.append(float(ww))\n",
    "    w = np.asarray(w, dtype=np.float64)\n",
    "    w = np.clip(w, 0.0, None)\n",
    "    w = w / (w.sum() + 1e-12)\n",
    "    return w\n",
    "\n",
    "w_train = make_pair_weights_from_counts(counts, train_pairs, mode=\"inv_sqrt\")\n",
    "w_val   = make_pair_weights_from_counts(counts, val_pairs,   mode=\"inv_sqrt\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10) parquet row-group indexing\n",
    "# =========================================================\n",
    "PARQUET_FILES = sorted(glob.glob(os.path.join(PARQUET_DIR, \"**\", \"*.parquet\"), recursive=True))\n",
    "print(\"[parquet] files:\", len(PARQUET_FILES))\n",
    "\n",
    "def build_pair_to_locations(parquet_files, valid_pairs_set, drug_col=\"drug\", cell_col=\"cell_line_id\"):\n",
    "    out = defaultdict(list)\n",
    "    for f in tqdm(parquet_files, desc=\"Index parquet row-groups\", dynamic_ncols=True):\n",
    "        pf = pq.ParquetFile(f)\n",
    "        for rg in range(pf.num_row_groups):\n",
    "            tbl = pf.read_row_group(rg, columns=[drug_col, cell_col])\n",
    "            df = tbl.to_pandas()\n",
    "            pairs_here = set(zip(df[drug_col].astype(str), df[cell_col].astype(str)))\n",
    "            inter = pairs_here.intersection(valid_pairs_set)\n",
    "            for p in inter:\n",
    "                out[p].append((f, rg))\n",
    "    return out\n",
    "\n",
    "valid_pairs_set = set(train_pairs) | set(val_pairs)\n",
    "pair_to_locations = build_pair_to_locations(PARQUET_FILES, valid_pairs_set, drug_col=DRUG_COL, cell_col=CELL_COL)\n",
    "print(\"[parquet] indexed pairs:\", len(pair_to_locations))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 11) Dataset (unique drug batch + Variant A + stable ordering + organ_id)\n",
    "# =========================================================\n",
    "class TahoeFPParquetDataset_UniqueDrug(torch.utils.data.IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pair_to_locations,\n",
    "        pairs,\n",
    "        baseline_global,\n",
    "        baseline_by_cellline,\n",
    "        drug_to_target_vec_target_only,   # (M_TGT,)\n",
    "        drug2id,\n",
    "        drug_to_smiles_np,\n",
    "        cellline2organid,\n",
    "        unk_organ_id,\n",
    "        n_genes_full,\n",
    "        steps,\n",
    "        max_seq_len=256,\n",
    "        batch_size=128,\n",
    "        control_drug=\"DMSO_TF\",\n",
    "        pad_id=0,\n",
    "        cls_id=1,\n",
    "        organtok_id=2,\n",
    "        pair_weights=None,\n",
    "        seed=42,\n",
    "        drug_col=\"drug\",\n",
    "        cell_col=\"cell_line_id\",\n",
    "        genes_col=\"genes\",\n",
    "        expr_col=\"expressions\",\n",
    "        cap_per_pair_in_rg=None,\n",
    "        max_tries_per_pair=20,\n",
    "        invalid_global_gene_tids=(1, 2),\n",
    "        subset_token_ids_np=None,\n",
    "        old_tid_to_vocab_lut=None,\n",
    "        m_tgt: int = 0,\n",
    "        drop_first_gene_token: bool = True,\n",
    "        use_log1p_expr: bool = True,\n",
    "        use_asinh_delta: bool = False,\n",
    "        delta_clip_abs: float = 5.0,\n",
    "        stable_sort_selected_by_gene_id: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pair_to_locations = pair_to_locations\n",
    "        self.pairs = list(pairs)\n",
    "        self.baseline_global = np.asarray(baseline_global, dtype=np.float32)\n",
    "        self.baseline_by_cellline = baseline_by_cellline or {}\n",
    "        self.drug_to_target_vec_target_only = drug_to_target_vec_target_only\n",
    "        self.drug2id = drug2id\n",
    "        self.drug_to_smiles_np = drug_to_smiles_np\n",
    "        self.cellline2organid = cellline2organid or {}\n",
    "        self.unk_organ_id = int(unk_organ_id)\n",
    "\n",
    "        self.n_genes_full = int(n_genes_full)\n",
    "        self.steps = int(steps)\n",
    "        self.max_seq_len = int(max_seq_len)\n",
    "        self.batch_size = int(batch_size)\n",
    "\n",
    "        self.control_drug = str(control_drug)\n",
    "        self.pad_id = int(pad_id)\n",
    "        self.cls_id = int(cls_id)\n",
    "        self.organtok_id = int(organtok_id)\n",
    "\n",
    "        self.drug_col = drug_col\n",
    "        self.cell_col = cell_col\n",
    "        self.genes_col = genes_col\n",
    "        self.expr_col = expr_col\n",
    "\n",
    "        self.cap_per_pair_in_rg = cap_per_pair_in_rg\n",
    "        self.max_tries_per_pair = int(max_tries_per_pair)\n",
    "        self.seed = int(seed)\n",
    "\n",
    "        any_vec = next(iter(self.drug_to_smiles_np.values()))\n",
    "        self.smiles_dim = int(any_vec.shape[-1])\n",
    "\n",
    "        self.invalid_global_gene_tids = np.asarray(list(set(int(x) for x in invalid_global_gene_tids)), dtype=np.int64)\n",
    "\n",
    "        self.m_tgt = int(m_tgt); assert self.m_tgt > 0\n",
    "        self.drop_first_gene_token = bool(drop_first_gene_token)\n",
    "\n",
    "        self.subset_token_ids_np = subset_token_ids_np\n",
    "        self.old_tid_to_vocab_lut = old_tid_to_vocab_lut\n",
    "\n",
    "        self.use_log1p_expr = bool(use_log1p_expr)\n",
    "        self.use_asinh_delta = bool(use_asinh_delta)\n",
    "        self.delta_clip_abs = float(delta_clip_abs)\n",
    "        self.stable_sort_selected_by_gene_id = bool(stable_sort_selected_by_gene_id)\n",
    "\n",
    "        if pair_weights is None:\n",
    "            self.pair_weights = None\n",
    "        else:\n",
    "            w = np.asarray(pair_weights, dtype=np.float64)\n",
    "            assert len(w) == len(self.pairs)\n",
    "            w = np.clip(w, 0.0, None)\n",
    "            w = w / (w.sum() + 1e-12)\n",
    "            self.pair_weights = w\n",
    "\n",
    "        self._pf_cache = {}\n",
    "\n",
    "    def _get_pf(self, file_path):\n",
    "        pf = self._pf_cache.get(file_path, None)\n",
    "        if pf is None:\n",
    "            pf = pq.ParquetFile(file_path)\n",
    "            self._pf_cache[file_path] = pf\n",
    "        return pf\n",
    "\n",
    "    def _read_row_group_df(self, file_path, rg_id, columns):\n",
    "        pf = self._get_pf(file_path)\n",
    "        return pf.read_row_group(rg_id, columns=columns).to_pandas()\n",
    "\n",
    "    def _scale_delta(self, delta: np.ndarray) -> np.ndarray:\n",
    "        if self.delta_clip_abs and self.delta_clip_abs > 0:\n",
    "            delta = np.clip(delta, -self.delta_clip_abs, self.delta_clip_abs)\n",
    "        if self.use_asinh_delta:\n",
    "            delta = np.arcsinh(delta)\n",
    "        return delta.astype(np.float32, copy=False)\n",
    "\n",
    "    def _prepare_sparse_sorted_drop0(self, genes, expr):\n",
    "        if genes is None or expr is None:\n",
    "            return np.asarray([], dtype=np.int64), np.asarray([], dtype=np.float32)\n",
    "\n",
    "        idx = np.asarray(genes, dtype=np.int64)\n",
    "        val = np.asarray(expr, dtype=np.float32)\n",
    "        L = min(idx.size, val.size)\n",
    "        idx = idx[:L]; val = val[:L]\n",
    "\n",
    "        if self.drop_first_gene_token and L >= 1:\n",
    "            idx = idx[1:]\n",
    "            val = val[1:]\n",
    "\n",
    "        if idx.size == 0:\n",
    "            return idx, val\n",
    "\n",
    "        # Variant A: log1p before delta\n",
    "        if self.use_log1p_expr:\n",
    "            val = np.log1p(np.clip(val, a_min=0.0, a_max=None))\n",
    "\n",
    "        if self.invalid_global_gene_tids.size > 0:\n",
    "            m_bad = np.isin(idx, self.invalid_global_gene_tids, assume_unique=False)\n",
    "            if m_bad.any():\n",
    "                keep = ~m_bad\n",
    "                idx = idx[keep]; val = val[keep]\n",
    "                if idx.size == 0:\n",
    "                    return idx, val\n",
    "\n",
    "        m = (idx >= 0) & (idx < self.n_genes_full)\n",
    "        idx = idx[m]; val = val[m]\n",
    "        if idx.size == 0:\n",
    "            return idx, val\n",
    "\n",
    "        order = np.argsort(idx)\n",
    "        return idx[order], val[order]\n",
    "\n",
    "    def _fill_one_row(self, row_genes, row_expr, baseline_vec, input_ids_row, values_row, attn_row):\n",
    "        idx_sorted, val_sorted = self._prepare_sparse_sorted_drop0(row_genes, row_expr)\n",
    "        if idx_sorted.size == 0:\n",
    "            return False\n",
    "\n",
    "        delta = val_sorted - baseline_vec[idx_sorted]\n",
    "        delta = self._scale_delta(delta)\n",
    "\n",
    "        mask_sub = np.isin(idx_sorted, self.subset_token_ids_np, assume_unique=False)\n",
    "        if not mask_sub.any():\n",
    "            return False\n",
    "\n",
    "        idx_sub = idx_sorted[mask_sub]\n",
    "        del_sub = delta[mask_sub]\n",
    "        if idx_sub.size == 0:\n",
    "            return False\n",
    "\n",
    "        k = min(self.max_seq_len, idx_sub.size)\n",
    "        top = np.argpartition(-np.abs(del_sub), k - 1)[:k]\n",
    "        sel_tid = idx_sub[top]\n",
    "        sel_del = del_sub[top]\n",
    "\n",
    "        # stable order: sort by gene_id (deterministic)\n",
    "        if self.stable_sort_selected_by_gene_id:\n",
    "            o2 = np.argsort(sel_tid)\n",
    "            sel_tid = sel_tid[o2]\n",
    "            sel_del = sel_del[o2]\n",
    "\n",
    "        sel_vid = self.old_tid_to_vocab_lut[sel_tid]\n",
    "        ok = sel_vid != -1\n",
    "        if not ok.any():\n",
    "            return False\n",
    "\n",
    "        sel_vid = sel_vid[ok]\n",
    "        sel_del = sel_del[ok]\n",
    "\n",
    "        L = min(self.max_seq_len, sel_vid.size)\n",
    "        if L <= 0:\n",
    "            return False\n",
    "\n",
    "        # layout: [CLS][ORGAN] + genes...\n",
    "        input_ids_row[2:2+L] = sel_vid[:L]\n",
    "        values_row[2:2+L]    = sel_del[:L]\n",
    "        attn_row[2:2+L]      = 1\n",
    "        return True\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        base_seed = self.seed if worker_info is None else (self.seed + worker_info.id)\n",
    "        rng = np.random.default_rng(base_seed)\n",
    "\n",
    "        pairs = self.pairs\n",
    "        weights = self.pair_weights\n",
    "        n_pairs = len(pairs)\n",
    "\n",
    "        cols = [self.drug_col, self.cell_col, self.genes_col, self.expr_col]\n",
    "        seq_len = 2 + self.max_seq_len\n",
    "\n",
    "        cnt = 0\n",
    "        while True:\n",
    "            chosen = []\n",
    "            seen_drugs = set()\n",
    "\n",
    "            tries = 0\n",
    "            while len(chosen) < self.batch_size and tries < 80:\n",
    "                tries += 1\n",
    "                draw = min(max(self.batch_size * 8, 512), max(n_pairs, 1))\n",
    "                if weights is None:\n",
    "                    cand_idx = rng.integers(0, n_pairs, size=draw)\n",
    "                else:\n",
    "                    cand_idx = rng.choice(n_pairs, size=draw, replace=True, p=weights)\n",
    "\n",
    "                for ii in cand_idx:\n",
    "                    drug_name, cell_line = pairs[int(ii)]\n",
    "                    drug_name = str(drug_name); cell_line = str(cell_line)\n",
    "\n",
    "                    if drug_name == self.control_drug:\n",
    "                        continue\n",
    "                    if drug_name in seen_drugs:\n",
    "                        continue\n",
    "\n",
    "                    y_vec = self.drug_to_target_vec_target_only.get(drug_name, None)\n",
    "                    if y_vec is None or float(y_vec.sum()) <= 0.0:\n",
    "                        continue\n",
    "\n",
    "                    sm = self.drug_to_smiles_np.get(drug_name, None)\n",
    "                    if sm is None or (not np.isfinite(sm).all()) or (np.abs(sm).sum() == 0.0):\n",
    "                        continue\n",
    "\n",
    "                    if not self.pair_to_locations.get((drug_name, cell_line), []):\n",
    "                        continue\n",
    "\n",
    "                    chosen.append((drug_name, cell_line))\n",
    "                    seen_drugs.add(drug_name)\n",
    "                    if len(chosen) >= self.batch_size:\n",
    "                        break\n",
    "\n",
    "            if len(chosen) < self.batch_size:\n",
    "                continue\n",
    "\n",
    "            input_ids = np.full((self.batch_size, seq_len), self.pad_id, dtype=np.int64)\n",
    "            values    = np.zeros((self.batch_size, seq_len), dtype=np.float32)\n",
    "            attn      = np.zeros((self.batch_size, seq_len), dtype=np.int64)\n",
    "\n",
    "            input_ids[:, 0] = self.cls_id\n",
    "            input_ids[:, 1] = self.organtok_id\n",
    "            attn[:, 0:2] = 1\n",
    "\n",
    "            y_batch       = np.zeros((self.batch_size, self.m_tgt), dtype=np.float32)\n",
    "            smiles_batch  = np.zeros((self.batch_size, self.smiles_dim), dtype=np.float32)\n",
    "            drug_id_batch = np.zeros((self.batch_size,), dtype=np.int64)\n",
    "            organ_id_batch = np.zeros((self.batch_size,), dtype=np.int64)\n",
    "\n",
    "            row_ptr = 0\n",
    "            built_any = False\n",
    "\n",
    "            for (drug_name, cell_line) in chosen:\n",
    "                locs = self.pair_to_locations.get((drug_name, cell_line), [])\n",
    "                if not locs:\n",
    "                    continue\n",
    "\n",
    "                baseline = self.baseline_by_cellline.get(cell_line, self.baseline_global)\n",
    "                did = int(self.drug2id.get(drug_name, 0))\n",
    "                oid = int(self.cellline2organid.get(cell_line, self.unk_organ_id))\n",
    "\n",
    "                y_vec = self.drug_to_target_vec_target_only[drug_name]\n",
    "                sm_vec = self.drug_to_smiles_np[drug_name]\n",
    "\n",
    "                ok_row = False\n",
    "                for _ in range(self.max_tries_per_pair):\n",
    "                    fpath, rg_id = locs[rng.integers(0, len(locs))]\n",
    "                    df = self._read_row_group_df(fpath, rg_id, columns=cols)\n",
    "\n",
    "                    df = df[(df[self.drug_col].astype(str) == drug_name) &\n",
    "                            (df[self.cell_col].astype(str) == cell_line)]\n",
    "                    if len(df) == 0:\n",
    "                        continue\n",
    "\n",
    "                    r = df.sample(1, random_state=None).itertuples(index=False).__next__()\n",
    "\n",
    "                    ok_row = self._fill_one_row(\n",
    "                        getattr(r, self.genes_col),\n",
    "                        getattr(r, self.expr_col),\n",
    "                        baseline,\n",
    "                        input_ids[row_ptr], values[row_ptr], attn[row_ptr]\n",
    "                    )\n",
    "                    if ok_row:\n",
    "                        y_batch[row_ptr] = y_vec\n",
    "                        smiles_batch[row_ptr] = sm_vec\n",
    "                        drug_id_batch[row_ptr] = did\n",
    "                        organ_id_batch[row_ptr] = oid\n",
    "                        row_ptr += 1\n",
    "                        built_any = True\n",
    "                        break  # ‚úÖ break only if ok_row\n",
    "\n",
    "                if row_ptr >= self.batch_size:\n",
    "                    break\n",
    "\n",
    "            if not built_any:\n",
    "                continue\n",
    "\n",
    "            if row_ptr < self.batch_size:\n",
    "                fill = self.batch_size - row_ptr\n",
    "                input_ids[row_ptr:]      = input_ids[:fill]\n",
    "                values[row_ptr:]         = values[:fill]\n",
    "                attn[row_ptr:]           = attn[:fill]\n",
    "                y_batch[row_ptr:]        = y_batch[:fill]\n",
    "                smiles_batch[row_ptr:]   = smiles_batch[:fill]\n",
    "                drug_id_batch[row_ptr:]  = drug_id_batch[:fill]\n",
    "                organ_id_batch[row_ptr:] = organ_id_batch[:fill]\n",
    "\n",
    "            yield {\n",
    "                \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                \"values\": torch.tensor(values, dtype=torch.float32),\n",
    "                \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
    "                \"y_targets\": torch.tensor(y_batch, dtype=torch.float32),\n",
    "                \"smiles_emb\": torch.tensor(smiles_batch, dtype=torch.float32),\n",
    "                \"drug_id\": torch.tensor(drug_id_batch, dtype=torch.long),\n",
    "                \"organ_id\": torch.tensor(organ_id_batch, dtype=torch.long),\n",
    "            }\n",
    "\n",
    "            cnt += 1\n",
    "            if cnt >= self.steps:\n",
    "                return\n",
    "\n",
    "\n",
    "train_ds = TahoeFPParquetDataset_UniqueDrug(\n",
    "    pair_to_locations=pair_to_locations,\n",
    "    pairs=train_pairs,\n",
    "    baseline_global=baseline_global,\n",
    "    baseline_by_cellline=baseline_by_cl,\n",
    "    drug_to_target_vec_target_only=drug_to_target_vec_tgt,\n",
    "    drug2id=drug2id,\n",
    "    drug_to_smiles_np=drug_to_smiles_np,\n",
    "    cellline2organid=cellline2organid,\n",
    "    unk_organ_id=UNK_ORGAN_ID,\n",
    "    n_genes_full=N_GENES,\n",
    "    steps=STEPS_PER_EPOCH,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    pad_id=PAD_ID,\n",
    "    cls_id=CLS_ID,\n",
    "    organtok_id=ORGAN_TOK_ID,\n",
    "    pair_weights=w_train,\n",
    "    seed=SEED,\n",
    "    subset_token_ids_np=subset_token_ids_np,\n",
    "    old_tid_to_vocab_lut=old_tid_to_vocab_lut,\n",
    "    m_tgt=M_TGT,\n",
    "    drop_first_gene_token=DROP_FIRST_GENE_TOKEN,\n",
    "    use_log1p_expr=USE_LOG1P_EXPR,\n",
    "    use_asinh_delta=USE_ASINH_DELTA,\n",
    "    delta_clip_abs=DELTA_CLIP_ABS,\n",
    "    stable_sort_selected_by_gene_id=True,\n",
    ")\n",
    "\n",
    "val_ds = TahoeFPParquetDataset_UniqueDrug(\n",
    "    pair_to_locations=pair_to_locations,\n",
    "    pairs=val_pairs,\n",
    "    baseline_global=baseline_global,\n",
    "    baseline_by_cellline=baseline_by_cl,\n",
    "    drug_to_target_vec_target_only=drug_to_target_vec_tgt,\n",
    "    drug2id=drug2id,\n",
    "    drug_to_smiles_np=drug_to_smiles_np,\n",
    "    cellline2organid=cellline2organid,\n",
    "    unk_organ_id=UNK_ORGAN_ID,\n",
    "    n_genes_full=N_GENES,\n",
    "    steps=VAL_STEPS,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    control_drug=CONTROL_DRUG,\n",
    "    pad_id=PAD_ID,\n",
    "    cls_id=CLS_ID,\n",
    "    organtok_id=ORGAN_TOK_ID,\n",
    "    pair_weights=w_val,\n",
    "    seed=SEED + 123,\n",
    "    subset_token_ids_np=subset_token_ids_np,\n",
    "    old_tid_to_vocab_lut=old_tid_to_vocab_lut,\n",
    "    m_tgt=M_TGT,\n",
    "    drop_first_gene_token=DROP_FIRST_GENE_TOKEN,\n",
    "    use_log1p_expr=USE_LOG1P_EXPR,\n",
    "    use_asinh_delta=USE_ASINH_DELTA,\n",
    "    delta_clip_abs=DELTA_CLIP_ABS,\n",
    "    stable_sort_selected_by_gene_id=True,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=None,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=(NUM_WORKERS > 0),\n",
    "    prefetch_factor=2 if NUM_WORKERS > 0 else None,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=None,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 12) Model: ORGAN embedding + NO positional emb + learnable CLIP temperature\n",
    "# =========================================================\n",
    "class FPEncoderWithOrgan(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, num_layers, pad_id,\n",
    "                 max_len: int, num_organs: int, organ_pos: int = 1, use_pos_emb: bool = False):\n",
    "        super().__init__()\n",
    "        self.token_emb  = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.value_proj = nn.Linear(1, d_model)\n",
    "\n",
    "        # ‚úÖ we agreed to remove positional embeddings\n",
    "        self.use_pos_emb = bool(use_pos_emb)\n",
    "        if self.use_pos_emb:\n",
    "            self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        self.organ_emb = nn.Embedding(num_organs, d_model)\n",
    "        self.organ_pos = int(organ_pos)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=4*d_model,\n",
    "            dropout=0.1, batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, input_ids, values, attention_mask, organ_id):\n",
    "        B, L = input_ids.shape\n",
    "        dev = input_ids.device\n",
    "\n",
    "        x = self.token_emb(input_ids) + self.value_proj(values.unsqueeze(-1))\n",
    "\n",
    "        if self.use_pos_emb:\n",
    "            pos = torch.arange(L, device=dev).unsqueeze(0).expand(B, L)\n",
    "            x = x + self.pos_emb(pos)\n",
    "\n",
    "        if organ_id is not None:\n",
    "            x[:, self.organ_pos, :] = x[:, self.organ_pos, :] + self.organ_emb(organ_id.to(dev)).to(x.dtype)\n",
    "\n",
    "        key_padding_mask = (attention_mask == 0)\n",
    "        h = self.encoder(x, src_key_padding_mask=key_padding_mask)\n",
    "        return h[:, 0, :]\n",
    "\n",
    "\n",
    "class FPModelTied_OrganCLIP(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, num_layers, pad_id, smiles_dim,\n",
    "                 max_len: int, num_organs: int, n_special: int, tau_init: float = 0.10):\n",
    "        super().__init__()\n",
    "        self.n_special = int(n_special)\n",
    "\n",
    "        self.encoder = FPEncoderWithOrgan(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            num_layers=num_layers,\n",
    "            pad_id=pad_id,\n",
    "            max_len=max_len,\n",
    "            num_organs=num_organs,\n",
    "            organ_pos=1,          # [CLS][ORGAN]\n",
    "            use_pos_emb=False,    # ‚úÖ removed\n",
    "        )\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.smiles_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.BatchNorm1d(4*d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(4*d_model, smiles_dim),\n",
    "        )\n",
    "\n",
    "        # CLIP logit scale: logits = (z1 @ z2.T) * exp(logit_scale)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1.0 / float(tau_init)))\n",
    "\n",
    "    def gene_emb_subset(self):\n",
    "        return self.encoder.token_emb.weight[self.n_special:, :]  # (M_SUB, d)\n",
    "\n",
    "    def get_tau(self):\n",
    "        # tau = 1/exp(scale)\n",
    "        return (1.0 / self.logit_scale.exp()).clamp(0.01, 0.5)\n",
    "\n",
    "    def forward(self, input_ids, values, attention_mask, organ_id, return_smiles=False):\n",
    "        h_cls = self.encoder(input_ids, values, attention_mask, organ_id=organ_id)\n",
    "        v_pred = self.proj(h_cls)\n",
    "        z_pred = self.smiles_head(h_cls)\n",
    "        if return_smiles:\n",
    "            return v_pred, z_pred\n",
    "        return v_pred\n",
    "\n",
    "\n",
    "D_MODEL = 256\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 4\n",
    "\n",
    "model = FPModelTied_OrganCLIP(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    num_layers=N_LAYERS,\n",
    "    pad_id=PAD_ID,\n",
    "    smiles_dim=SMILES_DIM,\n",
    "    max_len=(2 + MAX_SEQ_LEN),\n",
    "    num_organs=NUM_ORGANS,\n",
    "    n_special=N_SPECIAL,\n",
    "    tau_init=TAU_INIT,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 13) (optional) Load pretrained gene embeddings into subset token emb\n",
    "# =========================================================\n",
    "def load_pretrained_subset_into_token_emb(token_emb: nn.Embedding, npy_path: str, device):\n",
    "    if (npy_path is None) or (not os.path.exists(npy_path)):\n",
    "        print(\"‚ö†Ô∏è PRETRAINED_GENE_NPY not found. Skip loading.\")\n",
    "        return\n",
    "    W = np.load(npy_path)  # (N_GENES, d)\n",
    "    Wt = torch.tensor(W, dtype=torch.float32, device=device)\n",
    "    d = token_emb.weight.shape[1]\n",
    "    if Wt.shape[1] != d:\n",
    "        raise ValueError(f\"d mismatch: npy={Wt.shape[1]} vs token_emb={d}\")\n",
    "\n",
    "    loaded = 0\n",
    "    with torch.no_grad():\n",
    "        for sid, old_tid in enumerate(subset_token_ids):\n",
    "            vid = N_SPECIAL + sid\n",
    "            if 0 <= old_tid < Wt.shape[0]:\n",
    "                token_emb.weight[vid].copy_(Wt[int(old_tid)])\n",
    "                loaded += 1\n",
    "    print(f\"‚úÖ token_emb loaded: {loaded}/{len(subset_token_ids)}\")\n",
    "\n",
    "load_pretrained_subset_into_token_emb(model.encoder.token_emb, PRETRAINED_GENE_NPY, device=device)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 14) target_sub_ids (TARGET-ONLY indices inside SUBSET)\n",
    "# =========================================================\n",
    "target_sub_ids = torch.tensor([old_tid_to_subid[tid] for tid in target_token_ids],\n",
    "                              dtype=torch.long, device=device)\n",
    "print(\"[target_sub_ids]:\", tuple(target_sub_ids.shape))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 15) pos_weight (TARGET-ONLY)  (simple: ones)\n",
    "# =========================================================\n",
    "pos_weight = torch.ones((M_TGT,), dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 16) Losses (Targets + SMILES CLIP)\n",
    "# =========================================================\n",
    "def info_nce_ranking_loss_multi_pos(\n",
    "    v_pred: torch.Tensor,\n",
    "    gene_emb: torch.Tensor,\n",
    "    y_targets: torch.Tensor,\n",
    "    num_neg: int = 256,\n",
    "    num_pos: int = 8,\n",
    "    tau: float = 0.1,\n",
    "):\n",
    "    device_ = v_pred.device\n",
    "    B, _ = v_pred.shape\n",
    "    losses = []\n",
    "\n",
    "    v_pred = F.normalize(v_pred, dim=1)\n",
    "    gene_emb = F.normalize(gene_emb, dim=1)\n",
    "\n",
    "    for i in range(B):\n",
    "        pos_idx = (y_targets[i] > 0.5).nonzero(as_tuple=True)[0]\n",
    "        if pos_idx.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        neg_idx_all = (y_targets[i] < 0.5).nonzero(as_tuple=True)[0]\n",
    "        if neg_idx_all.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        if num_pos and pos_idx.numel() > num_pos:\n",
    "            pos_idx = pos_idx[torch.randperm(pos_idx.numel(), device=device_)[:num_pos]]\n",
    "\n",
    "        if neg_idx_all.numel() > num_neg:\n",
    "            neg_idx = neg_idx_all[torch.randperm(neg_idx_all.numel(), device=device_)[:num_neg]]\n",
    "        else:\n",
    "            neg_idx = neg_idx_all\n",
    "\n",
    "        pos_emb = gene_emb[pos_idx]\n",
    "        neg_emb = gene_emb[neg_idx]\n",
    "        cand_emb = torch.cat([pos_emb, neg_emb], dim=0)\n",
    "\n",
    "        v = v_pred[i].unsqueeze(0)\n",
    "        scores = (v @ cand_emb.T).squeeze(0) / tau\n",
    "\n",
    "        P = pos_emb.size(0)\n",
    "        logits = scores.unsqueeze(0).repeat(P, 1)\n",
    "        targets = torch.arange(P, device=device_, dtype=torch.long)\n",
    "        losses.append(F.cross_entropy(logits, targets))\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=device_)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "def bce_with_neg_sampling_cosine(\n",
    "    pred_vec: torch.Tensor,\n",
    "    y_targets: torch.Tensor,\n",
    "    gene_emb: torch.Tensor,\n",
    "    pos_weight_full: torch.Tensor,\n",
    "    num_neg: int = 2048,\n",
    "    pos_cap: int | None = None,\n",
    "    tau_bce: float = 0.10,\n",
    "):\n",
    "    device_ = pred_vec.device\n",
    "    B, _ = pred_vec.shape\n",
    "    losses = []\n",
    "\n",
    "    gene_emb = F.normalize(gene_emb, dim=1)\n",
    "\n",
    "    for i in range(B):\n",
    "        yi = y_targets[i]\n",
    "        pos_idx = (yi > 0.5).nonzero(as_tuple=True)[0]\n",
    "        if pos_idx.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        if (pos_cap is not None) and (pos_idx.numel() > pos_cap):\n",
    "            pos_idx = pos_idx[torch.randperm(pos_idx.numel(), device=device_)[:pos_cap]]\n",
    "\n",
    "        neg_idx_all = (yi < 0.5).nonzero(as_tuple=True)[0]\n",
    "        if neg_idx_all.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        k = min(int(num_neg), neg_idx_all.numel())\n",
    "        neg_idx = neg_idx_all[torch.randperm(neg_idx_all.numel(), device=device_)[:k]]\n",
    "\n",
    "        idx = torch.cat([pos_idx, neg_idx], dim=0)\n",
    "\n",
    "        v = F.normalize(pred_vec[i], dim=0)\n",
    "        logits = (v @ gene_emb[idx].T) / tau_bce\n",
    "\n",
    "        y_sub = yi[idx]\n",
    "        pw_sub = pos_weight_full[idx]\n",
    "\n",
    "        losses.append(F.binary_cross_entropy_with_logits(logits, y_sub, pos_weight=pw_sub, reduction=\"mean\"))\n",
    "\n",
    "    if len(losses) == 0:\n",
    "        return torch.tensor(0.0, device=device_)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "\n",
    "def combined_target_loss_neg_sampling_tied(\n",
    "    pred_vec: torch.Tensor,\n",
    "    y_targets: torch.Tensor,\n",
    "    gene_emb: torch.Tensor,\n",
    "    pos_weight: torch.Tensor,\n",
    "    lambda_cos: float = 1.0,\n",
    "    lambda_bce: float = 0.1,\n",
    "    lambda_rank: float = 0.5,\n",
    "    bce_num_neg: int = 2048,\n",
    "    bce_pos_cap: int | None = None,\n",
    "    rank_num_neg: int = 256,\n",
    "    rank_num_pos: int = 8,\n",
    "    tau_rank: float = 0.1,\n",
    "    tau_bce: float = 0.10,\n",
    "):\n",
    "    device_ = pred_vec.device\n",
    "\n",
    "    gene_emb_norm = F.normalize(gene_emb, dim=1)\n",
    "    pred_norm = F.normalize(pred_vec, dim=1)\n",
    "\n",
    "    true_vec = y_targets @ gene_emb_norm\n",
    "    num_t = y_targets.sum(dim=1, keepdim=True)\n",
    "    mask = (num_t > 0).squeeze(1)\n",
    "\n",
    "    if mask.any():\n",
    "        true_vec_pos = true_vec[mask] / (num_t[mask] + 1e-6)\n",
    "        true_vec_pos = F.normalize(true_vec_pos, dim=1)\n",
    "        pred_pos = pred_norm[mask]\n",
    "        loss_cos = 1.0 - (pred_pos * true_vec_pos).sum(dim=1).mean()\n",
    "    else:\n",
    "        loss_cos = torch.tensor(0.0, device=device_)\n",
    "\n",
    "    loss_bce = bce_with_neg_sampling_cosine(\n",
    "        pred_vec=pred_vec,\n",
    "        y_targets=y_targets,\n",
    "        gene_emb=gene_emb,\n",
    "        pos_weight_full=pos_weight,\n",
    "        num_neg=bce_num_neg,\n",
    "        pos_cap=bce_pos_cap,\n",
    "        tau_bce=tau_bce,\n",
    "    )\n",
    "\n",
    "    loss_rank = info_nce_ranking_loss_multi_pos(\n",
    "        v_pred=pred_vec,\n",
    "        gene_emb=gene_emb,\n",
    "        y_targets=y_targets,\n",
    "        num_neg=rank_num_neg,\n",
    "        num_pos=rank_num_pos,\n",
    "        tau=tau_rank,\n",
    "    )\n",
    "\n",
    "    loss = lambda_cos * loss_cos + lambda_bce * loss_bce + lambda_rank * loss_rank\n",
    "    return loss, loss_cos.detach(), loss_bce.detach(), loss_rank.detach()\n",
    "\n",
    "\n",
    "# --- SMILES CLIP loss (+ optional cosine align) ---\n",
    "def clip_loss(z_pred: torch.Tensor, z_true: torch.Tensor, tau: torch.Tensor):\n",
    "    z1 = F.normalize(z_pred, dim=1)\n",
    "    z2 = F.normalize(z_true, dim=1)\n",
    "    logits = (z1 @ z2.T) / tau\n",
    "    labels = torch.arange(z_pred.size(0), device=z_pred.device, dtype=torch.long)\n",
    "    return 0.5 * (F.cross_entropy(logits, labels) + F.cross_entropy(logits.T, labels))\n",
    "\n",
    "def smiles_align_loss_cosine(z_pred: torch.Tensor, z_true: torch.Tensor):\n",
    "    z1 = F.normalize(z_pred, dim=1)\n",
    "    z2 = F.normalize(z_true, dim=1)\n",
    "    return 1.0 - (z1 * z2).sum(dim=1).mean()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 17) Eval\n",
    "# =========================================================\n",
    "def compute_recall_precision_at_k(scores: torch.Tensor, y_true: torch.Tensor, k: int = 20):\n",
    "    B, M = scores.shape\n",
    "    kk = min(k, M)\n",
    "    _, topk_idx = torch.topk(scores, k=kk, dim=1)\n",
    "\n",
    "    recalls, precisions = [], []\n",
    "    for i in range(B):\n",
    "        true_labels = y_true[i]\n",
    "        num_pos_ = true_labels.sum().item()\n",
    "        if num_pos_ == 0:\n",
    "            continue\n",
    "        topk = topk_idx[i]\n",
    "        num_pos_in_topk = true_labels[topk].sum().item()\n",
    "        recalls.append(num_pos_in_topk / max(num_pos_, 1e-6))\n",
    "        precisions.append(num_pos_in_topk / max(kk, 1))\n",
    "\n",
    "    if len(recalls) == 0:\n",
    "        return 0.0, 0.0\n",
    "    return float(sum(recalls) / len(recalls)), float(sum(precisions) / len(precisions))\n",
    "\n",
    "@torch.no_grad()\n",
    "def smiles_retrieval_hitk(z_pred: torch.Tensor, drug_id: torch.Tensor, smiles_bank_t: torch.Tensor, k_list=(1,5,10)):\n",
    "    z = F.normalize(z_pred.float(), dim=1)\n",
    "    b = F.normalize(smiles_bank_t.float(), dim=1)\n",
    "    logits = z @ b.T\n",
    "    out = {}\n",
    "    for k in k_list:\n",
    "        topk = torch.topk(logits, k=min(k, logits.size(1)), dim=1).indices\n",
    "        hit = (topk == drug_id.view(-1,1)).any(dim=1).float().mean().item()\n",
    "        out[f\"Hit@{k}\"] = float(hit)\n",
    "    true_vec = b[drug_id]\n",
    "    out[\"TrueCos\"] = float((z * true_vec).sum(dim=1).mean().item())\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_fp(model, loader, device, target_sub_ids, smiles_bank_t, k_list=(5,10), hitk=(1,5,10)):\n",
    "    model.eval()\n",
    "\n",
    "    gene_emb = model.gene_emb_subset()[target_sub_ids].to(device)\n",
    "    g_norm = F.normalize(gene_emb, dim=1)\n",
    "\n",
    "    recall_sums = {k: 0.0 for k in k_list}\n",
    "    prec_sums   = {k: 0.0 for k in k_list}\n",
    "    counts_     = {k: 0   for k in k_list}\n",
    "\n",
    "    hit_sums = {f\"Hit@{k}\": 0.0 for k in hitk}\n",
    "    hit_sums[\"TrueCos\"] = 0.0\n",
    "    clip_sum = 0.0\n",
    "    tau_sum  = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        values    = batch[\"values\"].to(device, non_blocking=True)\n",
    "        attn      = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        y_targets = batch[\"y_targets\"].to(device, non_blocking=True)\n",
    "        z_true    = batch[\"smiles_emb\"].to(device, non_blocking=True)\n",
    "        drug_id   = batch[\"drug_id\"].to(device, non_blocking=True)\n",
    "        organ_id  = batch[\"organ_id\"].to(device, non_blocking=True)\n",
    "\n",
    "        v_pred, z_pred = model(input_ids, values, attn, organ_id=organ_id, return_smiles=True)\n",
    "        v_norm = F.normalize(v_pred, dim=1)\n",
    "        scores = v_norm @ g_norm.T\n",
    "\n",
    "        for k in k_list:\n",
    "            r, p = compute_recall_precision_at_k(scores, y_targets, k=k)\n",
    "            recall_sums[k] += r\n",
    "            prec_sums[k]   += p\n",
    "            counts_[k]     += 1\n",
    "\n",
    "        bs = input_ids.size(0)\n",
    "        m = smiles_retrieval_hitk(z_pred, drug_id, smiles_bank_t, k_list=hitk)\n",
    "        for k in hitk:\n",
    "            hit_sums[f\"Hit@{k}\"] += m[f\"Hit@{k}\"] * bs\n",
    "        hit_sums[\"TrueCos\"] += m[\"TrueCos\"] * bs\n",
    "\n",
    "        tau = model.get_tau()\n",
    "        clip_sum += float(clip_loss(z_pred, z_true, tau=tau).item()) * bs\n",
    "        tau_sum  += float(tau.item()) * bs\n",
    "\n",
    "        n += bs\n",
    "\n",
    "    out = {}\n",
    "    for k in k_list:\n",
    "        out[f\"Recall@{k}\"] = recall_sums[k] / max(counts_[k], 1)\n",
    "        out[f\"Precision@{k}\"] = prec_sums[k] / max(counts_[k], 1)\n",
    "\n",
    "    for k in hitk:\n",
    "        out[f\"SMILES_Hit@{k}\"] = hit_sums[f\"Hit@{k}\"] / max(n, 1)\n",
    "    out[\"SMILES_TrueCos\"] = hit_sums[\"TrueCos\"] / max(n, 1)\n",
    "    out[\"SMILES_CLIP\"] = clip_sum / max(n, 1)\n",
    "    out[\"tau\"] = tau_sum / max(n, 1)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 18) Train loop (OneCycleLR fixed + grad accumulation + AMP)\n",
    "# =========================================================\n",
    "def infinite_loader(loader):\n",
    "    while True:\n",
    "        for b in loader:\n",
    "            yield b\n",
    "\n",
    "USE_AMP = (device.type == \"cuda\")\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=USE_AMP)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "updates_per_epoch = max(1, STEPS_PER_EPOCH // max(1, ACCUM_STEPS))\n",
    "total_updates = EPOCHS * updates_per_epoch\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=LR,\n",
    "    total_steps=total_updates,\n",
    "    pct_start=0.05,\n",
    "    anneal_strategy=\"cos\",\n",
    "    div_factor=10.0,\n",
    "    final_div_factor=100.0,\n",
    ")\n",
    "\n",
    "smiles_bank_t = torch.tensor(smiles_bank_np, dtype=torch.float32, device=device)\n",
    "\n",
    "def train_one_epoch_fixed_steps(\n",
    "    model,\n",
    "    train_loader,\n",
    "    device,\n",
    "    steps_per_epoch,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    scaler,\n",
    "    target_sub_ids,\n",
    "    pos_weight,\n",
    "    smiles_bank_t,\n",
    "    log_every=50,\n",
    "    grad_clip=1.0,\n",
    "    accum_steps=1,\n",
    "):\n",
    "    model.train()\n",
    "    it = infinite_loader(train_loader)\n",
    "\n",
    "    run_total = 0.0\n",
    "    run_tgt   = 0.0\n",
    "    run_clip  = 0.0\n",
    "    run_align = 0.0\n",
    "    run_rank_last = 0.0\n",
    "\n",
    "    run_hit5 = 0.0\n",
    "    run_truecos = 0.0\n",
    "    n = 0\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    update_count = 0\n",
    "\n",
    "    pbar = tqdm(range(1, steps_per_epoch + 1), desc=\"Train\", leave=True, dynamic_ncols=True)\n",
    "\n",
    "    for step in pbar:\n",
    "        batch = next(it)\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        values    = batch[\"values\"].to(device, non_blocking=True)\n",
    "        attn      = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        y_targets = batch[\"y_targets\"].to(device, non_blocking=True)\n",
    "        z_true    = batch[\"smiles_emb\"].to(device, non_blocking=True)\n",
    "        drug_id   = batch[\"drug_id\"].to(device, non_blocking=True)\n",
    "        organ_id  = batch[\"organ_id\"].to(device, non_blocking=True)\n",
    "\n",
    "        bs = input_ids.size(0)\n",
    "        n += bs\n",
    "\n",
    "        if USE_AMP:\n",
    "            with torch.amp.autocast(\"cuda\", enabled=True):\n",
    "                v_pred, z_pred = model(input_ids, values, attn, organ_id=organ_id, return_smiles=True)\n",
    "\n",
    "                # --- Targets ---\n",
    "                gene_emb = model.gene_emb_subset()[target_sub_ids]  # (M_TGT, d)\n",
    "                loss_targets, loss_cos_t, loss_bce_t, loss_rank_t = combined_target_loss_neg_sampling_tied(\n",
    "                    pred_vec=v_pred,\n",
    "                    y_targets=y_targets,\n",
    "                    gene_emb=gene_emb,\n",
    "                    pos_weight=pos_weight,\n",
    "                    lambda_cos=lambda_cos,\n",
    "                    lambda_bce=lambda_bce,\n",
    "                    lambda_rank=lambda_rank,\n",
    "                    bce_num_neg=bce_num_neg,\n",
    "                    bce_pos_cap=bce_pos_cap,\n",
    "                    rank_num_neg=rank_num_neg,\n",
    "                    rank_num_pos=rank_num_pos,\n",
    "                    tau_rank=tau_rank,\n",
    "                    tau_bce=tau_bce,\n",
    "                )\n",
    "\n",
    "                # --- SMILES CLIP ---\n",
    "                tau = model.get_tau()\n",
    "                loss_c = clip_loss(z_pred, z_true, tau=tau)\n",
    "                loss_a = smiles_align_loss_cosine(z_pred, z_true)\n",
    "                loss_smiles = loss_c + alpha_align * loss_a\n",
    "\n",
    "                loss = (lambda_targets * loss_targets + lambda_smiles * loss_smiles) / float(accum_steps)\n",
    "\n",
    "            if not torch.isfinite(loss).all():\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                continue\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            do_update = (step % accum_steps) == 0\n",
    "            if do_update:\n",
    "                scaler.unscale_(optimizer)\n",
    "                if grad_clip is not None and grad_clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                scheduler.step()\n",
    "                update_count += 1\n",
    "\n",
    "        else:\n",
    "            v_pred, z_pred = model(input_ids, values, attn, organ_id=organ_id, return_smiles=True)\n",
    "\n",
    "            gene_emb = model.gene_emb_subset()[target_sub_ids]\n",
    "            loss_targets, loss_cos_t, loss_bce_t, loss_rank_t = combined_target_loss_neg_sampling_tied(\n",
    "                pred_vec=v_pred,\n",
    "                y_targets=y_targets,\n",
    "                gene_emb=gene_emb,\n",
    "                pos_weight=pos_weight,\n",
    "                lambda_cos=lambda_cos,\n",
    "                lambda_bce=lambda_bce,\n",
    "                lambda_rank=lambda_rank,\n",
    "                bce_num_neg=bce_num_neg,\n",
    "                bce_pos_cap=bce_pos_cap,\n",
    "                rank_num_neg=rank_num_neg,\n",
    "                rank_num_pos=rank_num_pos,\n",
    "                tau_rank=tau_rank,\n",
    "                tau_bce=tau_bce,\n",
    "            )\n",
    "\n",
    "            tau = model.get_tau()\n",
    "            loss_c = clip_loss(z_pred, z_true, tau=tau)\n",
    "            loss_a = smiles_align_loss_cosine(z_pred, z_true)\n",
    "            loss_smiles = loss_c + alpha_align * loss_a\n",
    "\n",
    "            loss = (lambda_targets * loss_targets + lambda_smiles * loss_smiles) / float(accum_steps)\n",
    "\n",
    "            if not torch.isfinite(loss).all():\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            do_update = (step % accum_steps) == 0\n",
    "            if do_update:\n",
    "                if grad_clip is not None and grad_clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                scheduler.step()\n",
    "                update_count += 1\n",
    "\n",
    "        # logging metrics (cheap)\n",
    "        with torch.no_grad():\n",
    "            m = smiles_retrieval_hitk(z_pred, drug_id, smiles_bank_t, k_list=(5,))\n",
    "            run_hit5 += float(m[\"Hit@5\"]) * bs\n",
    "            run_truecos += float(m[\"TrueCos\"]) * bs\n",
    "\n",
    "        run_total += float((lambda_targets * loss_targets + lambda_smiles * loss_smiles).item()) * bs\n",
    "        run_tgt   += float(loss_targets.item()) * bs\n",
    "        run_clip  += float(loss_c.item()) * bs\n",
    "        run_align += float(loss_a.item()) * bs\n",
    "        run_rank_last = float(loss_rank_t.item())\n",
    "\n",
    "        if step % log_every == 0:\n",
    "            lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "            pbar.set_postfix({\n",
    "                \"lr\": f\"{lr_now:.2e}\",\n",
    "                \"tau\": f\"{float(model.get_tau().item()):.3f}\",\n",
    "                \"tot\": f\"{run_total/max(n,1):.4f}\",\n",
    "                \"tgt\": f\"{run_tgt/max(n,1):.4f}\",\n",
    "                \"clip\": f\"{run_clip/max(n,1):.4f}\",\n",
    "                \"align\": f\"{run_align/max(n,1):.4f}\",\n",
    "                \"rank(last)\": f\"{run_rank_last:.4f}\",\n",
    "                \"Hit@5\": f\"{run_hit5/max(n,1):.3f}\",\n",
    "                \"TrueCos\": f\"{run_truecos/max(n,1):.3f}\",\n",
    "                \"upd\": f\"{update_count}/{updates_per_epoch}\",\n",
    "            })\n",
    "\n",
    "    return {\n",
    "        \"train_total\": run_total / max(n,1),\n",
    "        \"train_tgt\": run_tgt / max(n,1),\n",
    "        \"train_clip\": run_clip / max(n,1),\n",
    "        \"train_align\": run_align / max(n,1),\n",
    "        \"train_hit5\": run_hit5 / max(n,1),\n",
    "        \"train_truecos\": run_truecos / max(n,1),\n",
    "        \"rank_last\": run_rank_last,\n",
    "        \"tau\": float(model.get_tau().item()),\n",
    "        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "        \"updates\": update_count,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 19) TRAIN\n",
    "# =========================================================\n",
    "print(\">>> TRAIN START: FP(TARGET-ONLY) + ORGAN + SMILES CLIP | Variant A(log1p->delta->clip) | NO pos_emb\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    logs = train_one_epoch_fixed_steps(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        device=device,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        scaler=scaler,\n",
    "        target_sub_ids=target_sub_ids,\n",
    "        pos_weight=pos_weight,\n",
    "        smiles_bank_t=smiles_bank_t,\n",
    "        log_every=50,\n",
    "        grad_clip=MAX_GRAD_NORM,\n",
    "        accum_steps=max(1, int(ACCUM_STEPS)),\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"\\n[Epoch {epoch}/{EPOCHS}] \"\n",
    "        f\"lr={logs['lr']:.2e} | tau={logs['tau']:.3f} | \"\n",
    "        f\"train_total={logs['train_total']:.4f} | \"\n",
    "        f\"train_tgt={logs['train_tgt']:.4f} | \"\n",
    "        f\"train_clip={logs['train_clip']:.4f} | \"\n",
    "        f\"train_align={logs['train_align']:.4f} | \"\n",
    "        f\"rank_last={logs['rank_last']:.4f} | \"\n",
    "        f\"Hit@5={logs['train_hit5']:.3f} | TrueCos={logs['train_truecos']:.3f}\"\n",
    "    )\n",
    "\n",
    "    valid = evaluate_fp(\n",
    "        model=model,\n",
    "        loader=val_loader,\n",
    "        device=device,\n",
    "        target_sub_ids=target_sub_ids,\n",
    "        smiles_bank_t=smiles_bank_t,\n",
    "        k_list=(5,10),\n",
    "        hitk=(1,5,10),\n",
    "    )\n",
    "    print(\"‚úÖ VALID:\", valid)\n",
    "\n",
    "print(\">>> DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3182e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved FULL checkpoint (model+opt+sched+scaler+RNG+LUT): /data/aiffel/babayakga/checkpoints/f_p_final/fp_smalltargets.pt\n",
      "   - LUT saved: True\n",
      "   - optimizer saved: True\n",
      "   - scheduler saved: True\n",
      "   - scaler saved: True\n",
      "   - cuda rng saved: True\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# =========================\n",
    "# CONFIG: –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å\n",
    "# =========================\n",
    "CKPT_DIR  = \"/data/aiffel/babayakga/checkpoints/f_p_final\"          # –ø–æ–º–µ–Ω—è–π –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏\n",
    "CKPT_NAME = \"fp_smalltargets.pt\"  # –∏–º—è —Ñ–∞–π–ª–∞\n",
    "ckpt_path = os.path.join(CKPT_DIR, CKPT_NAME)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# 1) RNG states (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º)\n",
    "# =========================\n",
    "def get_rng_state_bundle():\n",
    "    out = {}\n",
    "    # python random\n",
    "    try:\n",
    "        out[\"python_random_state\"] = random.getstate()\n",
    "    except Exception as e:\n",
    "        out[\"python_random_state\"] = None\n",
    "        out[\"python_random_state_err\"] = repr(e)\n",
    "\n",
    "    # numpy\n",
    "    try:\n",
    "        out[\"numpy_random_state\"] = np.random.get_state()\n",
    "    except Exception as e:\n",
    "        out[\"numpy_random_state\"] = None\n",
    "        out[\"numpy_random_state_err\"] = repr(e)\n",
    "\n",
    "    # torch cpu\n",
    "    try:\n",
    "        out[\"torch_rng_state\"] = torch.get_rng_state()\n",
    "    except Exception as e:\n",
    "        out[\"torch_rng_state\"] = None\n",
    "        out[\"torch_rng_state_err\"] = repr(e)\n",
    "\n",
    "    # torch cuda (–≤—Å–µ –¥–µ–≤–∞–π—Å—ã)\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            out[\"torch_cuda_rng_state_all\"] = torch.cuda.get_rng_state_all()\n",
    "        else:\n",
    "            out[\"torch_cuda_rng_state_all\"] = None\n",
    "    except Exception as e:\n",
    "        out[\"torch_cuda_rng_state_all\"] = None\n",
    "        out[\"torch_cuda_rng_state_all_err\"] = repr(e)\n",
    "\n",
    "    return out\n",
    "\n",
    "rng_bundle = get_rng_state_bundle()\n",
    "\n",
    "# =========================\n",
    "# 2) EXTRA (–≤—à–∏–≤–∞–µ–º LUT –ø—Ä—è–º–æ –≤ .pt)\n",
    "# =========================\n",
    "EXTRA = {\n",
    "    \"SPECIAL_TOKENS\": SPECIAL_TOKENS,\n",
    "    \"N_SPECIAL\": int(N_SPECIAL),\n",
    "    \"VOCAB_SIZE\": int(VOCAB_SIZE),\n",
    "    \"PAD_ID\": int(PAD_ID),\n",
    "    \"CLS_ID\": int(CLS_ID),\n",
    "    \"ORGAN_TOK_ID\": int(ORGAN_TOK_ID),\n",
    "\n",
    "    \"subset_token_ids\": list(map(int, subset_token_ids)),\n",
    "    \"target_token_ids\": list(map(int, target_token_ids)),\n",
    "\n",
    "    \"UNK_ORGAN_ID\": int(UNK_ORGAN_ID),\n",
    "    \"organ2id\": organ2id,\n",
    "    \"NUM_ORGANS\": int(NUM_ORGANS),\n",
    "\n",
    "    \"D_MODEL\": int(D_MODEL),\n",
    "    \"N_HEADS\": int(N_HEADS),\n",
    "    \"N_LAYERS\": int(N_LAYERS),\n",
    "    \"MAX_SEQ_LEN\": int(MAX_SEQ_LEN),\n",
    "    \"SMILES_DIM\": int(SMILES_DIM),\n",
    "\n",
    "    \"SEED\": int(SEED),\n",
    "    \"CONTROL_DRUG\": str(CONTROL_DRUG),\n",
    "    \"HVG_K\": int(HVG_K),\n",
    "\n",
    "    \"USE_LOG1P_EXPR\": bool(USE_LOG1P_EXPR),\n",
    "    \"USE_ASINH_DELTA\": bool(USE_ASINH_DELTA),\n",
    "    \"DELTA_CLIP_ABS\": float(DELTA_CLIP_ABS),\n",
    "    \"DROP_FIRST_GENE_TOKEN\": bool(DROP_FIRST_GENE_TOKEN),\n",
    "}\n",
    "\n",
    "# LUT –æ–±—ã—á–Ω–æ –Ω–µ–±–æ–ª—å—à–æ–π ‚Äî –≤—à–∏–≤–∞–µ–º –≤–Ω—É—Ç—Ä—å .pt\n",
    "# (–¥–µ–ª–∞–µ–º CPU tensor int64 –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç–∏)\n",
    "if \"old_tid_to_vocab_lut\" in globals() and isinstance(old_tid_to_vocab_lut, np.ndarray):\n",
    "    lut_tensor = torch.from_numpy(old_tid_to_vocab_lut.astype(np.int64, copy=False)).cpu()\n",
    "elif \"old_tid_to_vocab_lut\" in globals() and torch.is_tensor(old_tid_to_vocab_lut):\n",
    "    lut_tensor = old_tid_to_vocab_lut.detach().to(dtype=torch.int64, device=\"cpu\")\n",
    "else:\n",
    "    lut_tensor = None\n",
    "\n",
    "# =========================\n",
    "# 3) PAYLOAD\n",
    "# =========================\n",
    "payload = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_class\": model.__class__.__name__,\n",
    "    \"model_state\": model.state_dict(),\n",
    "\n",
    "    \"optimizer_state\": optimizer.state_dict() if \"optimizer\" in globals() and optimizer is not None else None,\n",
    "    \"scheduler_state\": scheduler.state_dict() if \"scheduler\" in globals() and scheduler is not None else None,\n",
    "    \"scaler_state\": scaler.state_dict() if \"scaler\" in globals() and scaler is not None else None,\n",
    "\n",
    "    # –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ –ø–∞–º—è—Ç–∏\n",
    "    \"metrics\": {\"valid\": valid} if \"valid\" in globals() else {},\n",
    "\n",
    "    \"extra\": EXTRA,\n",
    "\n",
    "    # ‚úÖ LUT inside checkpoint\n",
    "    \"old_tid_to_vocab_lut\": lut_tensor,\n",
    "\n",
    "    # ‚úÖ RNG states\n",
    "    \"rng_state\": rng_bundle,\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 4) ATOMIC SAVE\n",
    "# =========================\n",
    "tmp_path = ckpt_path + \".tmp\"\n",
    "torch.save(payload, tmp_path)\n",
    "os.replace(tmp_path, ckpt_path)\n",
    "\n",
    "print(f\"‚úÖ Saved FULL checkpoint (model+opt+sched+scaler+RNG+LUT): {ckpt_path}\")\n",
    "print(f\"   - LUT saved: {lut_tensor is not None}\")\n",
    "print(f\"   - optimizer saved: {payload['optimizer_state'] is not None}\")\n",
    "print(f\"   - scheduler saved: {payload['scheduler_state'] is not None}\")\n",
    "print(f\"   - scaler saved: {payload['scaler_state'] is not None}\")\n",
    "print(f\"   - cuda rng saved: {payload['rng_state'].get('torch_cuda_rng_state_all') is not None}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ff6ee59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ loaded: /data/aiffel/babayakga/checkpoints/f_p_final/fp_smalltargets.pt\n",
      "top-level keys: ['extra', 'metrics', 'model_class', 'model_state', 'old_tid_to_vocab_lut', 'optimizer_state', 'rng_state', 'scaler_state', 'scheduler_state', 'timestamp']\n",
      "\n",
      "--- EXTRA (summary) ---\n",
      "VOCAB_SIZE: 4188\n",
      "N_SPECIAL: 4\n",
      "PAD_ID: 0\n",
      "CLS_ID: 1\n",
      "ORGAN_TOK_ID: 2\n",
      "NUM_ORGANS: 16\n",
      "UNK_ORGAN_ID: 0\n",
      "D_MODEL: 256\n",
      "N_HEADS: 8\n",
      "N_LAYERS: 4\n",
      "MAX_SEQ_LEN: 256\n",
      "SMILES_DIM: 768\n",
      "SEED: 42\n",
      "CONTROL_DRUG: DMSO_TF\n",
      "HVG_K: 4000\n",
      "USE_LOG1P_EXPR: True\n",
      "USE_ASINH_DELTA: False\n",
      "DELTA_CLIP_ABS: 5.0\n",
      "DROP_FIRST_GENE_TOKEN: True\n",
      "len(subset_token_ids) = 4184\n",
      "len(target_token_ids) = 278\n",
      "\n",
      "--- LUT ---\n",
      "‚úÖ LUT: <class 'torch.Tensor'> shape= (62713,) dtype= torch.int64 device= cpu\n",
      "bad LUT entries (!= -1 but < N_SPECIAL): 0\n",
      "\n",
      "‚úÖ RNG restored from checkpoint\n",
      "\n",
      "model_class saved in ckpt: FPModelTied_OrganCLIP\n",
      "\n",
      "--- load_state_dict ---\n",
      "missing keys: []\n",
      "unexpected keys: []\n",
      "‚úÖ model weights loaded\n",
      "‚úÖ optimizer restored\n",
      "‚ö†Ô∏è scheduler_state exists, but scheduler recreation is user-specific. Skipping by default.\n",
      "‚úÖ scaler restored\n",
      "\n",
      "--- forward OK ---\n",
      "v_pred: (2, 256) torch.float32\n",
      "z_pred: (2, 768) torch.float32\n",
      "tau: 0.024286000058054924\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD + VERIFY checkpoint (PyTorch 2.6+ compatible)\n",
    "# - fixes UnpicklingError by using weights_only=False (trusted ckpt)\n",
    "# - prints EXTRA + LUT sanity\n",
    "# - optionally restores RNG\n",
    "# - rebuilds model skeleton + loads weights\n",
    "# - optionally restores optimizer/scaler\n",
    "# - runs 1 dummy forward pass\n",
    "\n",
    "\n",
    "CKPT_PATH = \"/data/aiffel/babayakga/checkpoints/f_p_final/fp_smalltargets.pt\"  # <- your path\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 0) helper: pretty print\n",
    "# -------------------------\n",
    "def _pp(d, keys):\n",
    "    for k in keys:\n",
    "        print(f\"{k}: {d.get(k, None)}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1) load checkpoint\n",
    "#    ‚úÖ PyTorch 2.6 changed default: weights_only=True\n",
    "#    Your ckpt contains numpy RNG state -> needs trusted load.\n",
    "# -------------------------\n",
    "ckpt = torch.load(CKPT_PATH, map_location=\"cpu\", weights_only=False)\n",
    "print(\"‚úÖ loaded:\", CKPT_PATH)\n",
    "print(\"top-level keys:\", sorted(list(ckpt.keys())))\n",
    "\n",
    "required = [\"model_state\", \"extra\", \"rng_state\"]\n",
    "for k in required:\n",
    "    assert k in ckpt, f\"Missing key in checkpoint: {k}\"\n",
    "\n",
    "extra = ckpt[\"extra\"]\n",
    "\n",
    "print(\"\\n--- EXTRA (summary) ---\")\n",
    "_pp(extra, [\n",
    "    \"VOCAB_SIZE\", \"N_SPECIAL\", \"PAD_ID\", \"CLS_ID\",\n",
    "    \"ORGAN_TOK_ID\", \"NUM_ORGANS\", \"UNK_ORGAN_ID\",\n",
    "    \"D_MODEL\", \"N_HEADS\", \"N_LAYERS\", \"MAX_SEQ_LEN\",\n",
    "    \"SMILES_DIM\", \"SEED\", \"CONTROL_DRUG\", \"HVG_K\",\n",
    "    \"USE_LOG1P_EXPR\", \"USE_ASINH_DELTA\", \"DELTA_CLIP_ABS\",\n",
    "    \"DROP_FIRST_GENE_TOKEN\",\n",
    "])\n",
    "print(\"len(subset_token_ids) =\", len(extra.get(\"subset_token_ids\", [])))\n",
    "print(\"len(target_token_ids) =\", len(extra.get(\"target_token_ids\", [])))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# LUT sanity\n",
    "# -------------------------\n",
    "lut = ckpt.get(\"old_tid_to_vocab_lut\", None)\n",
    "print(\"\\n--- LUT ---\")\n",
    "if lut is None:\n",
    "    print(\"‚ùå LUT is None (not saved)\")\n",
    "else:\n",
    "    print(\"‚úÖ LUT:\", type(lut), \"shape=\", tuple(lut.shape), \"dtype=\", lut.dtype, \"device=\", lut.device)\n",
    "    lut_np = lut.cpu().numpy()\n",
    "    n_special = int(extra[\"N_SPECIAL\"])\n",
    "    bad = np.logical_and(lut_np != -1, lut_np < n_special).sum()\n",
    "    print(\"bad LUT entries (!= -1 but < N_SPECIAL):\", int(bad))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2) restore RNG (optional, for exact resume)\n",
    "# -------------------------\n",
    "def restore_rng(rng_state: dict):\n",
    "    # python\n",
    "    if rng_state.get(\"python_random_state\", None) is not None:\n",
    "        random.setstate(rng_state[\"python_random_state\"])\n",
    "    # numpy\n",
    "    if rng_state.get(\"numpy_random_state\", None) is not None:\n",
    "        np.random.set_state(rng_state[\"numpy_random_state\"])\n",
    "    # torch cpu\n",
    "    if rng_state.get(\"torch_rng_state\", None) is not None:\n",
    "        torch.set_rng_state(rng_state[\"torch_rng_state\"])\n",
    "    # torch cuda\n",
    "    if torch.cuda.is_available() and rng_state.get(\"torch_cuda_rng_state_all\", None) is not None:\n",
    "        torch.cuda.set_rng_state_all(rng_state[\"torch_cuda_rng_state_all\"])\n",
    "\n",
    "restore_rng(ckpt[\"rng_state\"])\n",
    "print(\"\\n‚úÖ RNG restored from checkpoint\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3) rebuild model from EXTRA and load weights\n",
    "# IMPORTANT:\n",
    "#   You must have your model class definition available in runtime.\n",
    "#   Example: FPModelTied_OrganCLIP must be defined ABOVE this cell/script.\n",
    "# -------------------------\n",
    "model_class_name = ckpt.get(\"model_class\", \"UNKNOWN\")\n",
    "print(\"\\nmodel_class saved in ckpt:\", model_class_name)\n",
    "\n",
    "# === CHANGE THIS if your saved class differs ===\n",
    "ModelClass = FPModelTied_OrganCLIP  # <-- must exist in your runtime\n",
    "\n",
    "VOCAB_SIZE  = int(extra[\"VOCAB_SIZE\"])\n",
    "PAD_ID      = int(extra[\"PAD_ID\"])\n",
    "N_SPECIAL   = int(extra[\"N_SPECIAL\"])\n",
    "NUM_ORGANS  = int(extra[\"NUM_ORGANS\"])\n",
    "D_MODEL     = int(extra[\"D_MODEL\"])\n",
    "N_HEADS     = int(extra[\"N_HEADS\"])\n",
    "N_LAYERS    = int(extra[\"N_LAYERS\"])\n",
    "MAX_SEQ_LEN = int(extra[\"MAX_SEQ_LEN\"])\n",
    "SMILES_DIM  = int(extra[\"SMILES_DIM\"])\n",
    "\n",
    "model1 = ModelClass(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    num_layers=N_LAYERS,\n",
    "    pad_id=PAD_ID,\n",
    "    smiles_dim=SMILES_DIM,\n",
    "    max_len=(2 + MAX_SEQ_LEN),\n",
    "    num_organs=NUM_ORGANS,\n",
    "    n_special=N_SPECIAL,\n",
    " \n",
    ").to(device)\n",
    "\n",
    "missing, unexpected = model1.load_state_dict(ckpt[\"model_state\"], strict=False)\n",
    "print(\"\\n--- load_state_dict ---\")\n",
    "print(\"missing keys:\", missing)\n",
    "print(\"unexpected keys:\", unexpected)\n",
    "assert len(unexpected) == 0, \"Unexpected keys => model definition mismatch\"\n",
    "print(\"‚úÖ model weights loaded\")\n",
    "\n",
    "model1.eval()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4) restore optimizer/scheduler/scaler (optional)\n",
    "# -------------------------\n",
    "optimizer = None\n",
    "scheduler = None\n",
    "scaler = None\n",
    "\n",
    "if ckpt.get(\"optimizer_state\", None) is not None:\n",
    "    optimizer = torch.optim.AdamW(model1.parameters(), lr=1e-4)  # lr can be dummy\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "    print(\"‚úÖ optimizer restored\")\n",
    "\n",
    "if ckpt.get(\"scheduler_state\", None) is not None and optimizer is not None:\n",
    "    print(\"‚ö†Ô∏è scheduler_state exists, but scheduler recreation is user-specific. Skipping by default.\")\n",
    "\n",
    "if ckpt.get(\"scaler_state\", None) is not None and device.type == \"cuda\":\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=True)\n",
    "    scaler.load_state_dict(ckpt[\"scaler_state\"])\n",
    "    print(\"‚úÖ scaler restored\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5) quick forward sanity test (no dataloader needed)\n",
    "# -------------------------\n",
    "B = 2\n",
    "L = 2 + MAX_SEQ_LEN\n",
    "\n",
    "dummy_input_ids = torch.full((B, L), int(extra[\"PAD_ID\"]), dtype=torch.long, device=device)\n",
    "dummy_values    = torch.zeros((B, L), dtype=torch.float32, device=device)\n",
    "dummy_attn      = torch.zeros((B, L), dtype=torch.long, device=device)\n",
    "\n",
    "dummy_input_ids[:, 0] = int(extra[\"CLS_ID\"])\n",
    "dummy_input_ids[:, 1] = int(extra[\"ORGAN_TOK_ID\"])\n",
    "dummy_attn[:, :2] = 1\n",
    "\n",
    "dummy_organ = torch.zeros((B,), dtype=torch.long, device=device)  # UNK organ id usually 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    v_pred, z_pred = model1(dummy_input_ids, dummy_values, dummy_attn, organ_id=dummy_organ, return_smiles=True)\n",
    "\n",
    "print(\"\\n--- forward OK ---\")\n",
    "print(\"v_pred:\", tuple(v_pred.shape), v_pred.dtype)\n",
    "print(\"z_pred:\", tuple(z_pred.shape), z_pred.dtype)\n",
    "print(\"tau:\", float(model1.get_tau().item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28248d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ VALID: {'Recall@5': 0.2924789224068324, 'Precision@5': 0.08897396000723044, 'mAP@5': 0.19173023119471813, 'NDCG@5': 0.22381376816503082, 'Coverage@5_avg_targets': 2.095833333333333, 'Coverage@5_median_targets': 1.14, 'Coverage@5_frac_targets_le_k': 0.9517708333333333, 'Coverage@5_avg_recall_ceiling': 0.9826236150662104, 'Recall@5_over_ceiling': 0.2976510211258507, 'Recall@10': 0.4075360565384229, 'Precision@10': 0.06435156346609196, 'mAP@10': 0.21013024507517306, 'NDCG@10': 0.2646626206972481, 'Coverage@10_avg_targets': 2.095833333333333, 'Coverage@10_median_targets': 1.14, 'Coverage@10_frac_targets_le_k': 0.9886458333333333, 'Coverage@10_avg_recall_ceiling': 0.9976251810789108, 'Recall@10_over_ceiling': 0.4085061847553619, 'Recall@20': 0.5443653134504954, 'Precision@20': 0.0442695319528381, 'mAP@20': 0.2221575258799324, 'NDCG@20': 0.30403822676472675, 'Coverage@20_avg_targets': 2.095833333333333, 'Coverage@20_median_targets': 1.14, 'Coverage@20_frac_targets_le_k': 1.0, 'Coverage@20_avg_recall_ceiling': 1.0, 'Recall@20_over_ceiling': 0.5443653134504954, 'SMILES_Hit@1': 0.12861979166666668, 'SMILES_Hit@5': 0.268203125, 'SMILES_Hit@10': 0.34294270833333335, 'SMILES_TrueCos': 0.4066869940360387, 'SMILES_MRR': 0.20479313254356385, 'SMILES_median_rank': 32.126666666666665, 'SMILES_mean_rank': 72.740703125, 'SMILES_CLIP': 3.7804848623275755, 'tau': 0.024286000058054924, 'n_samples': 38400.0}\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple, Iterable, List\n",
    "# =========================================================\n",
    "# Ranking metrics for targets: mAP@K, NDCG@K, Coverage ceiling\n",
    "# =========================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def average_precision_at_k(scores_1d: torch.Tensor, y_true_1d: torch.Tensor, k: int) -> float:\n",
    "    \"\"\"\n",
    "    AP@K for ONE sample (binary relevance).\n",
    "    scores_1d: (M,)\n",
    "    y_true_1d: (M,) in {0,1}\n",
    "    \"\"\"\n",
    "    M = scores_1d.numel()\n",
    "    kk = min(int(k), int(M))\n",
    "    if kk <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    pos_total = float(y_true_1d.sum().item())\n",
    "    if pos_total <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    topk = torch.topk(scores_1d, k=kk, dim=0).indices\n",
    "    rel = y_true_1d[topk].float()  # (kk,)\n",
    "\n",
    "    # precision@i only when rel[i]=1\n",
    "    cumsum_rel = torch.cumsum(rel, dim=0)\n",
    "    ranks = torch.arange(1, kk + 1, device=scores_1d.device, dtype=torch.float32)\n",
    "    precision_i = cumsum_rel / ranks\n",
    "\n",
    "    ap = (precision_i * rel).sum() / max(1.0, min(pos_total, float(kk)))\n",
    "    return float(ap.item())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def mean_average_precision_at_k(scores: torch.Tensor, y_true: torch.Tensor, k: int) -> float:\n",
    "    \"\"\"\n",
    "    mAP@K over batch (binary relevance).\n",
    "    scores: (B, M)\n",
    "    y_true: (B, M) in {0,1}\n",
    "    \"\"\"\n",
    "    B = scores.size(0)\n",
    "    aps = []\n",
    "    for i in range(B):\n",
    "        aps.append(average_precision_at_k(scores[i], y_true[i], k))\n",
    "    return float(sum(aps) / max(1, len(aps)))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ndcg_at_k(scores_1d: torch.Tensor, y_true_1d: torch.Tensor, k: int) -> float:\n",
    "    \"\"\"\n",
    "    NDCG@K for ONE sample (binary relevance).\n",
    "    DCG = sum_{i=1..K} rel_i / log2(i+1)\n",
    "    IDCG computed from sorted relevances (all ones first).\n",
    "    \"\"\"\n",
    "    M = scores_1d.numel()\n",
    "    kk = min(int(k), int(M))\n",
    "    if kk <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    pos_total = int(y_true_1d.sum().item())\n",
    "    if pos_total <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    topk = torch.topk(scores_1d, k=kk, dim=0).indices\n",
    "    rel = y_true_1d[topk].float()  # (kk,)\n",
    "\n",
    "    denom = torch.log2(torch.arange(2, kk + 2, device=scores_1d.device, dtype=torch.float32))\n",
    "    dcg = (rel / denom).sum()\n",
    "\n",
    "    ideal_k = min(pos_total, kk)\n",
    "    ideal_rel = torch.ones((ideal_k,), device=scores_1d.device, dtype=torch.float32)\n",
    "    idcg = (ideal_rel / denom[:ideal_k]).sum()\n",
    "\n",
    "    return float((dcg / (idcg + 1e-12)).item())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def mean_ndcg_at_k(scores: torch.Tensor, y_true: torch.Tensor, k: int) -> float:\n",
    "    B = scores.size(0)\n",
    "    vals = []\n",
    "    for i in range(B):\n",
    "        vals.append(ndcg_at_k(scores[i], y_true[i], k))\n",
    "    return float(sum(vals) / max(1, len(vals)))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def recall_precision_at_k(scores: torch.Tensor, y_true: torch.Tensor, k: int) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Same spirit as your compute_recall_precision_at_k, but vectorized-ish and robust.\n",
    "    \"\"\"\n",
    "    B, M = scores.shape\n",
    "    kk = min(int(k), int(M))\n",
    "    if kk <= 0:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    topk = torch.topk(scores, k=kk, dim=1).indices  # (B, kk)\n",
    "    rel = torch.gather(y_true, 1, topk).float()     # (B, kk)\n",
    "    pos_total = y_true.sum(dim=1).float()           # (B,)\n",
    "\n",
    "    mask = pos_total > 0\n",
    "    if not mask.any():\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    num_pos_in_topk = rel.sum(dim=1)  # (B,)\n",
    "    recall = (num_pos_in_topk[mask] / (pos_total[mask] + 1e-12)).mean()\n",
    "    precision = (num_pos_in_topk[mask] / float(kk)).mean()\n",
    "    return float(recall.item()), float(precision.item())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def coverage_ceiling_recall_at_k(y_true: torch.Tensor, k: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Coverage vs #targets:\n",
    "    - avg #targets\n",
    "    - fraction of samples with <=k targets\n",
    "    - avg ceiling recall@k = min(k, #targets) / #targets\n",
    "    - median #targets\n",
    "    \"\"\"\n",
    "    k = int(k)\n",
    "    num_t = y_true.sum(dim=1).float()  # (B,)\n",
    "    mask = num_t > 0\n",
    "    if not mask.any():\n",
    "        return {\n",
    "            \"avg_targets\": 0.0,\n",
    "            \"median_targets\": 0.0,\n",
    "            \"frac_targets_le_k\": 0.0,\n",
    "            \"avg_recall_ceiling\": 0.0,\n",
    "        }\n",
    "\n",
    "    nt = num_t[mask]\n",
    "    ceiling = torch.minimum(nt, torch.tensor(float(k), device=y_true.device)) / (nt + 1e-12)\n",
    "    frac_le = (nt <= float(k)).float().mean()\n",
    "\n",
    "    # median (torch median)\n",
    "    median = nt.median()\n",
    "\n",
    "    return {\n",
    "        \"avg_targets\": float(nt.mean().item()),\n",
    "        \"median_targets\": float(median.item()),\n",
    "        \"frac_targets_le_k\": float(frac_le.item()),\n",
    "        \"avg_recall_ceiling\": float(ceiling.mean().item()),\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Retrieval metrics for SMILES: MRR, median rank, plus Hit@K, TrueCos\n",
    "# =========================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def smiles_retrieval_metrics(\n",
    "    z_pred: torch.Tensor,\n",
    "    drug_id: torch.Tensor,\n",
    "    smiles_bank_t: torch.Tensor,\n",
    "    k_list: Iterable[int] = (1, 5, 10),\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    z_pred: (B, D) predicted SMILES vector\n",
    "    drug_id: (B,) true drug index in bank\n",
    "    smiles_bank_t: (N, D)\n",
    "    Returns: Hit@K, TrueCos, MRR, median_rank, mean_rank\n",
    "    \"\"\"\n",
    "    z = F.normalize(z_pred.float(), dim=1)\n",
    "    b = F.normalize(smiles_bank_t.float(), dim=1)\n",
    "\n",
    "    logits = z @ b.T  # (B, N)\n",
    "    B, N = logits.shape\n",
    "\n",
    "    # ranks: higher logits => better\n",
    "    # rank = 1 + number of items with score > true_score (ties -> worst-ish; acceptable)\n",
    "    true_scores = logits.gather(1, drug_id.view(-1, 1))  # (B,1)\n",
    "    better = (logits > true_scores).sum(dim=1)           # (B,)\n",
    "    rank = better + 1                                   # (B,) 1..N\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    # Hit@K\n",
    "    for k in k_list:\n",
    "        k = min(int(k), N)\n",
    "        topk = torch.topk(logits, k=k, dim=1).indices\n",
    "        hit = (topk == drug_id.view(-1, 1)).any(dim=1).float().mean()\n",
    "        out[f\"Hit@{k}\"] = float(hit.item())\n",
    "\n",
    "    # TrueCos\n",
    "    true_vec = b[drug_id]\n",
    "    out[\"TrueCos\"] = float((z * true_vec).sum(dim=1).mean().item())\n",
    "\n",
    "    # MRR / ranks\n",
    "    out[\"MRR\"] = float((1.0 / rank.float()).mean().item())\n",
    "    out[\"median_rank\"] = float(rank.float().median().item())\n",
    "    out[\"mean_rank\"] = float(rank.float().mean().item())\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Drop-in replacement: evaluate_fp + new metrics\n",
    "# =========================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_fp_with_ranking_and_retrieval(\n",
    "    model,\n",
    "    loader,\n",
    "    device,\n",
    "    target_sub_ids,\n",
    "    smiles_bank_t,\n",
    "    k_list_targets: Iterable[int] = (5, 10, 20),\n",
    "    k_list_smiles: Iterable[int] = (1, 5, 10),\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Adds:\n",
    "      - Targets: mAP@K, NDCG@K, Recall@K, Precision@K + Coverage ceiling stats\n",
    "      - SMILES: Hit@K, TrueCos, MRR, median rank, mean rank, CLIP loss, tau\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    gene_emb = model.gene_emb_subset()[target_sub_ids].to(device)  # (M_TGT, d)\n",
    "    g_norm = F.normalize(gene_emb, dim=1)\n",
    "\n",
    "    # accumulators\n",
    "    out_sum = defaultdict(float)\n",
    "    n_batches = 0\n",
    "    n_samples = 0\n",
    "\n",
    "    clip_sum = 0.0\n",
    "    tau_sum = 0.0\n",
    "\n",
    "    # coverage stats accum (per K)\n",
    "    cov_sums = {k: defaultdict(float) for k in k_list_targets}\n",
    "    cov_counts = {k: 0 for k in k_list_targets}\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        values    = batch[\"values\"].to(device, non_blocking=True)\n",
    "        attn      = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        y_targets = batch[\"y_targets\"].to(device, non_blocking=True)  # (B, M_TGT)\n",
    "        z_true    = batch[\"smiles_emb\"].to(device, non_blocking=True)\n",
    "        drug_id   = batch[\"drug_id\"].to(device, non_blocking=True)\n",
    "        organ_id  = batch[\"organ_id\"].to(device, non_blocking=True)\n",
    "\n",
    "        v_pred, z_pred = model(input_ids, values, attn, organ_id=organ_id, return_smiles=True)\n",
    "        v_norm = F.normalize(v_pred, dim=1)\n",
    "        scores = v_norm @ g_norm.T  # (B, M_TGT)\n",
    "\n",
    "        B = scores.size(0)\n",
    "        n_batches += 1\n",
    "        n_samples += B\n",
    "\n",
    "        # ---- Targets metrics ----\n",
    "        for k in k_list_targets:\n",
    "            r, p = recall_precision_at_k(scores, y_targets, k=k)\n",
    "            ap = mean_average_precision_at_k(scores, y_targets, k=k)\n",
    "            nd = mean_ndcg_at_k(scores, y_targets, k=k)\n",
    "\n",
    "            out_sum[f\"Recall@{k}\"] += r\n",
    "            out_sum[f\"Precision@{k}\"] += p\n",
    "            out_sum[f\"mAP@{k}\"] += ap\n",
    "            out_sum[f\"NDCG@{k}\"] += nd\n",
    "\n",
    "            cov = coverage_ceiling_recall_at_k(y_targets, k=k)\n",
    "            for kk, vv in cov.items():\n",
    "                cov_sums[k][kk] += float(vv)\n",
    "            cov_counts[k] += 1\n",
    "\n",
    "        # ---- SMILES retrieval metrics ----\n",
    "        m = smiles_retrieval_metrics(z_pred, drug_id, smiles_bank_t, k_list=k_list_smiles)\n",
    "        for key, val in m.items():\n",
    "            out_sum[f\"SMILES_{key}\"] += float(val) * B  # weight by batch size\n",
    "\n",
    "        # ---- CLIP loss / tau ----\n",
    "        tau = model.get_tau()\n",
    "        clip_sum += float(clip_loss(z_pred, z_true, tau=tau).item()) * B\n",
    "        tau_sum  += float(tau.item()) * B\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    # Average batch-averaged target metrics\n",
    "    for k in k_list_targets:\n",
    "        out[f\"Recall@{k}\"] = out_sum[f\"Recall@{k}\"] / max(1, n_batches)\n",
    "        out[f\"Precision@{k}\"] = out_sum[f\"Precision@{k}\"] / max(1, n_batches)\n",
    "        out[f\"mAP@{k}\"] = out_sum[f\"mAP@{k}\"] / max(1, n_batches)\n",
    "        out[f\"NDCG@{k}\"] = out_sum[f\"NDCG@{k}\"] / max(1, n_batches)\n",
    "\n",
    "        # Coverage ceiling stats (averaged over batches)\n",
    "        cc = cov_counts[k]\n",
    "        if cc > 0:\n",
    "            out[f\"Coverage@{k}_avg_targets\"] = cov_sums[k][\"avg_targets\"] / cc\n",
    "            out[f\"Coverage@{k}_median_targets\"] = cov_sums[k][\"median_targets\"] / cc\n",
    "            out[f\"Coverage@{k}_frac_targets_le_k\"] = cov_sums[k][\"frac_targets_le_k\"] / cc\n",
    "            out[f\"Coverage@{k}_avg_recall_ceiling\"] = cov_sums[k][\"avg_recall_ceiling\"] / cc\n",
    "            # optional: \"normalized recall\" = Recall@K / ceiling (if you want)\n",
    "            ceil = out[f\"Coverage@{k}_avg_recall_ceiling\"]\n",
    "            out[f\"Recall@{k}_over_ceiling\"] = out[f\"Recall@{k}\"] / max(ceil, 1e-9)\n",
    "\n",
    "    # SMILES metrics averaged over samples (we weighted by B already)\n",
    "    for key in [\"Hit@1\", \"Hit@5\", \"Hit@10\", \"TrueCos\", \"MRR\", \"median_rank\", \"mean_rank\"]:\n",
    "        sk = f\"SMILES_{key}\"\n",
    "        if sk in out_sum:\n",
    "            out[sk] = out_sum[sk] / max(1, n_samples)\n",
    "\n",
    "    out[\"SMILES_CLIP\"] = clip_sum / max(1, n_samples)\n",
    "    out[\"tau\"] = tau_sum / max(1, n_samples)\n",
    "    out[\"n_samples\"] = float(n_samples)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "valid = evaluate_fp_with_ranking_and_retrieval(\n",
    "    model=model,\n",
    "    loader=val_loader,\n",
    "    device=device,\n",
    "    target_sub_ids=target_sub_ids,\n",
    "    smiles_bank_t=smiles_bank_t,\n",
    "    k_list_targets=(5, 10, 20),\n",
    "    k_list_smiles=(1, 5, 10),\n",
    ")\n",
    "print(\"‚úÖ VALID:\", valid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babayakga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
