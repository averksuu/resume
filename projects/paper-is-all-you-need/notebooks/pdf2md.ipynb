{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "734149d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Docling\n",
    "from docling_core.types.doc import ImageRefMode, PictureItem\n",
    "from docling_core.types.doc.document import DocTagsDocument, DoclingDocument\n",
    "\n",
    "# MLX-VLM\n",
    "from mlx_vlm import load, stream_generate\n",
    "from mlx_vlm.prompt_utils import apply_chat_template\n",
    "from mlx_vlm.utils import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c94e7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 설정\n",
    "MODEL_PATH = \"ibm-granite/granite-docling-258M-mlx\"   # Docling 모델 경로\n",
    "PROMPT = \"Convert this page to docling.\"             # 페이지 변환 프롬프트\n",
    "PDF_PATH = \"paper3.pdf\"                               # 입력 PDF 파일\n",
    "OUT_IMG_DIR = Path(\"figures\")                         # 이미지 저장 폴더\n",
    "OUT_IMG_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a928d959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 196844.59it/s]\n",
      "Fetching 13 files: 100%|██████████| 13/13 [00:00<00:00, 168289.98it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "model, processor = load(MODEL_PATH)   # 모델과 프로세서 로드\n",
    "config = load_config(MODEL_PATH)      # 설정값 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17568c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting PDF: paper3.pdf\n",
      "Total pages converted: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Converting PDF: {PDF_PATH}\")\n",
    "pages = convert_from_path(PDF_PATH, dpi=200)   # PDF to Image\n",
    "print(f\"Total pages converted: {len(pages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47738e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doctags(tokens: str) -> str:\n",
    "    soup = BeautifulSoup(tokens, \"html.parser\")\n",
    "\n",
    "    for tag in [\"page_header\", \"page_footer\", \"footnote\"]:\n",
    "        for node in soup.find_all(tag):\n",
    "            node.decompose()\n",
    "\n",
    "    section_headers = soup.find_all(\"section_header_level_1\")\n",
    "\n",
    "    if section_headers:\n",
    "        first_header = section_headers[0]\n",
    "        abstract_header = None\n",
    "        for node in section_headers[1:]:\n",
    "            if node.get_text(strip=True).lower() == \"abstract\":\n",
    "                abstract_header = node\n",
    "                break\n",
    "\n",
    "        if abstract_header:\n",
    "            current = first_header.find_next_sibling()\n",
    "            while current and current != abstract_header:\n",
    "                nxt = current.find_next_sibling()\n",
    "                current.decompose()\n",
    "                current = nxt\n",
    "\n",
    "        for node in section_headers:\n",
    "            if node.get_text(strip=True).lower() == \"references\":\n",
    "                current = node\n",
    "                while current:\n",
    "                    nxt = current.find_next_sibling()\n",
    "                    current.decompose()\n",
    "                    current = nxt\n",
    "                break\n",
    "\n",
    "    return str(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9aff6d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Page 1/5 ---\n",
      "Page 1 done.\n",
      "\n",
      "--- Page 2/5 ---\n",
      "Page 2 done.\n",
      "\n",
      "--- Page 3/5 ---\n",
      "Page 3 done.\n",
      "\n",
      "--- Page 4/5 ---\n",
      "Page 4 done.\n",
      "\n",
      "--- Page 5/5 ---\n",
      "Page 5 done.\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "all_markdown_pages = []\n",
    "\n",
    "for i, page in enumerate(pages):\n",
    "    print(f\"\\n--- Page {i+1}/{len(pages)} ---\")\n",
    "\n",
    "    # DocTags 추출\n",
    "    formatted_prompt = apply_chat_template(processor, config, PROMPT, num_images=1)\n",
    "    output = \"\"\n",
    "    for token in stream_generate(\n",
    "        model, processor, formatted_prompt, [page], max_tokens=4096, verbose=False\n",
    "    ):\n",
    "        output += token.text\n",
    "        if \"</doctag>\" in token.text:\n",
    "            break\n",
    "\n",
    "    # 원본/클린 텍스트 저장\n",
    "    raw_txt_path = Path(f\"./page_{i+1}_raw.txt\")\n",
    "    clean_txt_path = Path(f\"./page_{i+1}_cleaned.txt\")\n",
    "    raw_txt_path.write_text(output, encoding=\"utf-8\")\n",
    "    cleaned_output = clean_doctags(output)\n",
    "    clean_txt_path.write_text(cleaned_output, encoding=\"utf-8\")\n",
    "\n",
    "    # DoclingDocument 로드\n",
    "    doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([cleaned_output], [page])\n",
    "    doc = DoclingDocument.load_from_doctags(doctags_doc, document_name=f\"Page {i+1}\")\n",
    "    docs.append(doc)\n",
    "\n",
    "    # Figure 추출\n",
    "    fig_paths = []\n",
    "    for idx, (element, _) in enumerate(doc.iterate_items(), start=1):\n",
    "        if isinstance(element, PictureItem):\n",
    "            out_path = OUT_IMG_DIR / f\"page{i+1}_figure{idx}.png\"\n",
    "            element.get_image(doc).save(out_path, format=\"PNG\")\n",
    "            fig_paths.append(f\"./figures/page{i+1}_figure{idx}.png\")\n",
    "\n",
    "    # Markdown 변환 + 이미지 삽입\n",
    "    md_text = doc.export_to_markdown().replace(\"<!-- image -->\", \"\")\n",
    "    output_lines, figure_index = [], 1\n",
    "    for line in md_text.splitlines():\n",
    "        # \"Figure N:\" 또는 \"FIGURE N:\" 만 매칭\n",
    "        if re.match(r\"^\\s*Figure\\s+\\d+:\", line, re.IGNORECASE):\n",
    "            if figure_index <= len(fig_paths):\n",
    "                img_path = fig_paths[figure_index - 1]\n",
    "                output_lines.append(f\"![Figure]({img_path})\")\n",
    "                figure_index += 1\n",
    "        output_lines.append(line)\n",
    "\n",
    "    all_markdown_pages.append(\"\\n\".join(output_lines))\n",
    "\n",
    "    print(f\"Page {i+1} done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef81b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs\n",
      "\n",
      "## Abstract\n",
      "\n",
      "Multi-modal language-vision models trained on hundreds of millions of image-text pairs (e.g. CLIP, DALL-E) gained a recent surge, showing remarkable capability to perform zero- or few-shot learning and transfer even in absence of per-sample labels on target image data. Despite this trend, to date there has been no publicly available datasets of sufficient scale for training such models from scratch. To address this issue, in a community effort we build and release for public LAION-400M, a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices that allow efficient similarity search. 1\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Multi-modal language-vision models demonstrated recently strong transfer capability to novel datasets in absense of per-sample labels [1, 2, 3]. This capability requires sufficiently large model and data scale during pre-training. Increasing data scale alone can often improve model performance [4]. When increasing model and compute budget scale in addition, scaling laws suggest further increase in generalization and transfer performance if not bottlenecked by the data scale [5, 6, 7, 8]. There is a plethora of recent works that have built massive datasets in order to optimally scale up various models [9, 1, 2, 3]. However, these massive datasets have rarely been released for various reasons. Gao et. al. recently released The Pile, an openly-available 800GB text dataset [10], in an attempt to loosely mimic the dataset used for GPT-3. The largest publicly known image-text paired datasets range from 400 million to around a billion, but none of them has been released.\n",
      "\n",
      "![Figure](./figures/page2_figure2.png)\n",
      "Figure 1: Sample images retrieved from the queries \"blue cat\" or \"cat with blue eyes\" in the web demo\n",
      "\n",
      "\n",
      "\n",
      "To address this issue, we build and release LAION-400M, a dataset with CLIP-filtered 400 million image-text pairs, their CLIP embeddings and kNN indices. We describe the procedure to create the dataset and demonstrate successful training of DALL-E architecture. Having sufficiently large scale, the dataset opens venues for research on multi-modal language-vision models to broad community.\n",
      "\n",
      "## 2 Dataset and Methods\n",
      "\n",
      "Overview of LAION-400M. We officially release the following packages under LAION-400M project:\n",
      "\n",
      "- · 400 million pairs of image URL and the corresponding metadata\n",
      "- · 400 million pairs of CLIP image embedding and the corresponding text\n",
      "- · Several sets of kNN indices that enable quick search in the dataset\n",
      "- · img2dataset library that enables efficient crawling and processing of hundreds of millions of images and their metadata from a list of URLs with minimal resources\n",
      "- · Web demo of image-text search on LAION-400M (Fig. 1, 2\n",
      "\n",
      "As for the pairs of image URL and metadata, we provide parquet files that consist of the following attributes for each pair: sample ID, URL, type of Creative Commons license (if applicable), NSFW tag (detected with CLIP), cosine similarity score between the text and image embedding and height and width of the image. We found less than 1% of images were detected as NSFW, which can be filtered out by an user with NSFW tag.\n",
      "\n",
      "![Figure](./figures/page3_figure2.png)\n",
      "Figure 2: Acquisition workflow\n",
      "\n",
      "\n",
      "\n",
      "Acquisition. The acquisition follows the flowchart of Fig. 2 and can be split into two major components:\n",
      "\n",
      "- · Distributed processing of petabyte-scale Common Crawl dataset, which produces a collection of matching URLs and captions.\n",
      "- · Single node post-processing of the data, which is much lighter and can be run in a few days, producing the final dataset.\n",
      "\n",
      "## 2.1 Distributed processing of Common Crawl\n",
      "\n",
      "To create image-text pairs, we parse through WAT files from Common Crawl and parse out all HTML IMG tags containing an alt-text attribute. We download the raw images from the parsed URLs with asynchronous requests using Trio and Asks libraries.\n",
      "\n",
      "## 2.1.1 Filtering out unsuitable image-text pairs\n",
      "\n",
      "After downloading the WAT files from Common Crawl, we apply the following filtering conditions:\n",
      "\n",
      "- · All samples with less than 5 character alt-text length or less than 5 KB image size are dropped.\n",
      "- · Duplicate removal is performed with bloom filter based on URL and alt-text.\n",
      "- · We use CLIP to compute embeddings of the image and alt-text. Then we compute the cosine similarity of both embeddings and drop all samples with cosine similarity below 0.3. This threshold was selected based on human inspections.\n",
      "- · We use the CLIP embeddings of images and texts to filter out illegal contents.\n",
      "\n",
      "Table 1: Image size distribution of LAION-400M\n",
      "\n",
      "| Number of unique samples            | 413M   |\n",
      "|-------------------------------------|--------|\n",
      "| Number with height or width ≥ 1024  | 26M    |\n",
      "| Number with height and width ≥ 1024 | 9.6M   |\n",
      "| Number with height and width ≥ 512  | 67M    |\n",
      "| Number with height or width ≥ 512   | 112M   |\n",
      "| Number with height and width ≥ 256  | 211M   |\n",
      "| Number with height or width ≥ 256   | 268M   |\n",
      "\n",
      "![Figure](./figures/page4_figure4.png)\n",
      "Figure 3: DALL-E Experiments. (Left) Generated samples from a DALL-E model trained with 7.2M randomly picked LAION-400M samples on 1 RTX 2070 Super (8 GB VRAM) for 1 epoch (Right) DALL-E runs with Conceptual Captions 3M (green), Conceptual Captions 12M (orange) and a 3M subset of LAION-400M (grey)\n",
      "\n",
      "\n",
      "\n",
      "## 2.1.2 img2dataset\n",
      "\n",
      "We developed img2dataset library to comfortably download from a given set of URLs, resize and store the images and captions in the webdataset format. 3 This allows to download 100 million images from our list of URLs in 20 hours with a single node (1Gbps connection speed, 32GB of RAM, an i7 CPU with 16 cores), which allows anyone to obtain the whole dataset or a smaller subset.\n",
      "\n",
      "## 3 Analysis &amp;amp; Results\n",
      "\n",
      "Web demo and similarity search. A web demo was created to allow an user to search images and texts based on a query image or text using the CLIP embeddings of the input and our precomputed kNN indices. It demonstrates the diversity of images and captions that can be found in LAION-400M as well as high semantic relevance (Fig. 1).\n",
      "\n",
      "Tab. 1 shows the distribution of image sizes of LAION-400M. Given the abundance of high-resolution images, one can produce subsets of images for training various customized models, and also choose image resolution that is suitable for purpose of particular training.\n",
      "\n",
      "Training DALL-E model. We ran DALLE-pytorch [11], an open-source replication of DALL-E [2], to assess the dataset's capability to train a text-to-image model. The VQGAN [12] pretrained on ImageNet is used to encode image tokens. For generation, we use CLIP ViT-B/16 [1] to rank the top 8 of 128 total samples per caption. Despite only seeing a subset of approximately 7.2 million images for a single epoch, we observe fast convergence across a variety of categories. Samples generated from the model show sufficient quality and provide evidence for successful training progress (Fig. 3).\n",
      "\n",
      "## 4 Conclusion\n",
      "\n",
      "By releasing an openly available dataset that contains 400 million image-text pairs, we have closed the gap to proprietary large scale datasets that were necessary to train state-of-the-art language-vision models such as DALL-E and CLIP. As proof of concept, we demonstrated that a subset of our dataset can be used to train a DALL-E model, producing samples of sufficient quality. The dataset opens the road for large-scale training and research of language-vision models, that were previously restricted to those having access to proprietary large datasets, to the broad community.\n"
     ]
    }
   ],
   "source": [
    "# 모든 페이지 마크다운 합치기 (조건: 마지막 문장이 . ! ? 로 안끝나면 이어붙임)\n",
    "merged_pages = []\n",
    "\n",
    "for i, page_md in enumerate(all_markdown_pages):\n",
    "    if merged_pages:\n",
    "        prev = merged_pages[-1].rstrip()\n",
    "        # 이전 페이지의 마지막 줄\n",
    "        last_line = prev.splitlines()[-1].strip() if prev.splitlines() else \"\"\n",
    "        if last_line and not last_line.endswith((\".\", \"!\", \"?\")):\n",
    "            # 끝이 문장 부호가 아니면 다음 페이지와 바로 이어붙임\n",
    "            merged_pages[-1] = prev + \" \" + page_md.lstrip()\n",
    "        else:\n",
    "            merged_pages.append(page_md)\n",
    "    else:\n",
    "        merged_pages.append(page_md)\n",
    "\n",
    "full_markdown = \"\\n\\n\".join(merged_pages)\n",
    "\n",
    "print(full_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9940c1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.md\n",
      "figures\n"
     ]
    }
   ],
   "source": [
    "# 최종 마크다운과 이미지 디렉터리 경로 출력\n",
    "MD_PATH = Path(\"./output.md\")\n",
    "with open(MD_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(full_markdown)\n",
    "\n",
    "print(MD_PATH)\n",
    "print(OUT_IMG_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
